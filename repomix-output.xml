This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed, content has been compressed (code blocks are separated by ‚ãÆ---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*
- Files matching these patterns are excluded: coverage/**, dist/**, build/**, .git/**, node_modules/**, pnpm-lock.yaml, pnpm-workspace.yaml, **/*.png, **/*.jpg, **/*.jpeg, **/*.gif, **/*.svg, **/*.ico, **/*.mp4, **/*.mp3, **/*.wav, **/*.pdf, **/*.zip, **/*.gz, **/*.tar, **/*.7z, **/*.exe, **/*.bin, **/*.sqlite, **/*.db, docs/api/index.html, docs/archive/**, **/test/fixtures/**, **/package-lock.json, **/yarn.lock, **/pnpm-lock.yaml
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ‚ãÆ---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  commands/
    upgrade-deps.md
.github/
  agents/
    aws-media-downloader.agent.md
  scripts/
    generate-sidebar.sh
    sync-wiki.sh
  workflows/
    check-dep0180.yml
    check-ts-morph-update.yml
    dependency-check.yml
    integration-tests.yml
    sync-wiki.yml
    unit-tests.yml
    update-agents.yml
    update-yt-dlp.yml
.husky/
  commit-msg
  pre-push
apps/
  .gitkeep
bin/
  build-dependencies.sh
  ci-local-full.sh
  ci-local.sh
  document-api.sh
  document-source.sh
  extract-fixtures.sh
  extract-production-fixtures.sh
  process-fixtures.js
  sync-examples.sh
  test-hook.sh
  test-integration.sh
  test-list.sh
  test-registerDevice.sh
  update-agents-prs.sh
  update-youtube-cookies.sh
  update-yt-dlp.sh
  validate-doc-sync.sh
  validate-docs.sh
  validate-graphrag.sh
config/
  esbuild.config.ts
  jest.all.config.mjs
  jest.config.mjs
  jest.integration.config.mjs
docs/
  api/
    openapi.yaml
  plans/
    ai-optimization-roadmap.md
    scheduled-video-handling.md
  wiki/
    Authentication/
      Better-Auth-Architecture.md
      ElectroDB-Adapter-Design.md
    AWS/
      CloudWatch-Logging.md
      Lambda-Environment-Variables.md
      SDK-Encapsulation-Policy.md
      X-Ray-Integration.md
    Bash/
      Bash-Error-Handling.md
      Directory-Resolution.md
      Script-Patterns.md
      User-Output-Formatting.md
      Variable-Naming.md
    Conventions/
      Code-Comments.md
      Code-Formatting.md
      ESLint-vs-MCP-Validation.md
      Git-Workflow.md
      Import-Organization.md
      Naming-Conventions.md
    Infrastructure/
      Environment-Variables.md
      File-Organization.md
      OpenTofu-Patterns.md
      Resource-Naming.md
      Script-Registry.md
    Integration/
      LocalStack-Testing.md
    iOS/
      Apple-Sign-In-ID-Token-Migration.md
    MCP/
      Convention-Tools.md
    Meta/
      AI-Tool-Context-Files.md
      Convention-Capture-System.md
      Documentation-Patterns.md
      Emerging-Conventions.md
      GitHub-Wiki-Sync.md
      pnpm-Migration.md
      Serverless-Architecture-Assessment.md
      Working-with-AI-Assistants.md
    Methodologies/
      Convention-Over-Configuration.md
      Dependabot-Resolution.md
      Library-Migration-Checklist.md
      Production-Debugging.md
    Testing/
      Coverage-Philosophy.md
      Dependency-Graph-Analysis.md
      ElectroDB-Testing-Patterns.md
      Fixture-Extraction.md
      Integration-Testing.md
      Jest-ESM-Mocking-Strategy.md
      Lazy-Initialization-Pattern.md
      Local-CI-Testing.md
      Mock-Type-Annotations.md
    TypeScript/
      Lambda-Function-Patterns.md
      Module-Best-Practices.md
      Type-Definitions.md
      TypeScript-Error-Handling.md
    Getting-Started.md
    Home.md
  conventions-tracking.md
  doc-code-mapping.json
  doc-code-mapping.schema.json
  MCP-GUIDE.md
eslint-local-rules/
  rules/
    authenticated-handler-enforcement.cjs
    cascade-delete-order.cjs
    env-validation.cjs
    no-direct-aws-sdk-import.cjs
    response-helpers.cjs
    use-electrodb-mock-helper.cjs
  test/
    cascade-delete-order.test.cjs
    env-validation.test.cjs
    no-direct-aws-sdk-import.test.cjs
    response-helpers.test.cjs
    use-electrodb-mock-helper.test.cjs
  index.cjs
graphrag/
  extract.ts
  knowledge-graph.json
  metadata.json
  metadata.schema.json
  query.ts
layers/
  yt-dlp/
    bin/
      yt-dlp_linux
    VERSION
packages/
  .gitkeep
scripts/
  generateDependencyGraph.ts
  validateConfig.ts
  visualize-bundles.ts
src/
  entities/
    Accounts.ts
    Collections.ts
    Devices.ts
    FileDownloads.ts
    Files.ts
    Sessions.ts
    UserDevices.ts
    UserFiles.ts
    Users.ts
    VerificationTokens.ts
  lambdas/
    ApiGatewayAuthorizer/
      src/
        index.ts
      test/
        index.test.ts
    CloudfrontMiddleware/
      src/
        index.ts
      test/
        index.test.ts
      types/
        index.ts
    FileCoordinator/
      src/
        index.ts
      test/
        index.test.ts
    ListFiles/
      src/
        index.ts
      test/
        index.test.ts
    LogClientEvent/
      src/
        index.ts
      test/
        index.test.ts
    LoginUser/
      src/
        index.ts
      test/
        index.test.ts
    PruneDevices/
      src/
        index.ts
      test/
        index.test.ts
      types/
        index.ts
    RefreshToken/
      src/
        index.ts
      test/
        index.test.ts
    RegisterDevice/
      src/
        index.ts
      test/
        index.test.ts
    RegisterUser/
      src/
        index.ts
      test/
        index.test.ts
    S3ObjectCreated/
      src/
        index.ts
      test/
        index.test.ts
    SendPushNotification/
      src/
        index.ts
      test/
        index.test.ts
    StartFileUpload/
      bin/
        yt-dlp_linux
      src/
        file-helpers.ts
        index.ts
      test/
        index.test.ts
    UserDelete/
      src/
        index.ts
      test/
        index.test.ts
    UserSubscribe/
      src/
        index.ts
      test/
        index.test.ts
    WebhookFeedly/
      src/
        index.ts
      test/
        index.test.ts
  lib/
    vendor/
      AWS/
        ApiGateway.ts
        clients.ts
        CloudWatch.ts
        DynamoDB.ts
        Lambda.ts
        S3.ts
        SNS.ts
        SQS.ts
        XRay.ts
      BetterAuth/
        config.ts
        electrodb-adapter.ts
      ElectroDB/
        entity.ts
        service.ts
      Powertools/
        idempotency.ts
        index.ts
        parser.ts
      YouTube.test.ts
      YouTube.ts
  mcp/
    handlers/
      conventions.ts
      coverage.ts
      data-loader.test.ts
      data-loader.ts
      electrodb.ts
      impact.ts
      infrastructure.ts
      lambda.ts
      naming.ts
      test-scaffold.ts
      validation.ts
    parsers/
      convention-parser.test.ts
      convention-parser.ts
    validation/
      rules/
        authenticated-handler-enforcement.test.ts
        authenticated-handler-enforcement.ts
        aws-sdk-encapsulation.test.ts
        aws-sdk-encapsulation.ts
        batch-retry.test.ts
        batch-retry.ts
        cascade-safety.test.ts
        cascade-safety.ts
        config-enforcement.test.ts
        config-enforcement.ts
        doc-sync.ts
        electrodb-mocking.test.ts
        electrodb-mocking.ts
        env-validation.test.ts
        env-validation.ts
        import-order.test.ts
        import-order.ts
        mock-formatting.test.ts
        mock-formatting.ts
        naming-conventions.ts
        response-enum.test.ts
        response-enum.ts
        response-helpers.test.ts
        response-helpers.ts
        scan-pagination.test.ts
        scan-pagination.ts
        types-location.test.ts
        types-location.ts
      index.test.ts
      index.ts
      types.ts
    README.md
    server.ts
  pipeline/
    infrastructure.environment.test.ts
  templates/
    github-issues/
      cookie-expiration.md
      user-deletion-failure.md
      video-download-failure.md
  types/
    vendor/
      IFTTT/
        Feedly/
          Webhook.d.ts
    better-auth.d.ts
    domain-models.d.ts
    enums.ts
    global.d.ts
    infrastructure-types.d.ts
    lambda-wrappers.ts
    notification-types.d.ts
    persistence-types.d.ts
    util.ts
    video.ts
    youtube.ts
  util/
    test/
      better-auth-helpers.test.ts
      env-validation.test.ts
      github-helpers.test.ts
      lambda-helpers.test.ts
      pagination.test.ts
      retry.test.ts
      template-helpers.test.ts
      video-error-classifier.test.ts
    apigateway-helpers.ts
    better-auth-helpers.ts
    constants-runtime.ts
    constants.ts
    constraints.ts
    device-helpers.ts
    env-validation.ts
    errors.ts
    github-helpers.ts
    jest-setup.ts
    lambda-helpers.ts
    lambda-invoke-helpers.ts
    logging.ts
    pagination.ts
    retry.ts
    template-helpers.ts
    transformers.ts
    user-file-helpers.ts
    video-error-classifier.ts
terraform/
  api_gateway_authorizer.tf
  api_gateway.tf
  cloudfront_middleware.tf
  cloudwatch.tf
  configuration_apns.tf
  dynamodb_idempotency.tf
  feedly_webhook.tf
  file_bucket.tf
  file_coordinator.tf
  list_files.tf
  log_client_event.tf
  login_user.tf
  main.tf
  prune_devices.tf
  register_device.tf
  register_user.tf
  send_push_notification.tf
  user_delete.tf
  user_subscribe.tf
test/
  helpers/
    better-auth-mock.ts
    better-auth-test-data.ts
    electrodb-mock.ts
  integration/
    helpers/
      dynamodb-helpers.ts
      electrodb-localstack.ts
      lambda-context.ts
      mock-youtube.ts
      s3-helpers.ts
      test-data.ts
    lib/
      vendor/
        AWS/
          DynamoDB.ts
          S3.ts
    workflows/
      auth.flow.integration.test.ts
      betterAuth.entities.integration.test.ts
      deviceRegistration.integration.test.ts
      fileCoordinator.workflow.integration.test.ts
      fileRetry.integration.test.ts
      listFiles.workflow.integration.test.ts
      sendPushNotification.workflow.integration.test.ts
      startFileUpload.workflow.integration.test.ts
      userDelete.cascade.integration.test.ts
      webhookFeedly.workflow.integration.test.ts
    README.md
    setup.ts
tsp/
  examples/
    list-files-response.json
    login-user-request.json
    login-user-response.json
    README.md
    register-device-request.json
    register-device-response.json
    register-user-request.json
    register-user-response.json
    webhook-feedly-request.json
    webhook-feedly-response.json
  models/
    models.tsp
  operations/
    operations.tsp
  EXAMPLES.md
  main.tsp
  README.md
  tspconfig.yaml
.codecov.yml
.dependency-cruiser.cjs
.editorconfig
.gitignore
.mcp.json
.npmrc
.nvmrc
.sops.yaml
AGENTS.md
CLAUDE.md
docker-compose.localstack.yml
dprint.json
eslint.config.mjs
GEMINI.md
LICENSE
package.json
README.md
repomix.config.json
secrets.enc.yaml
tsconfig.json
tsconfig.test.json
tsdoc.json
typedoc.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="docs/plans/ai-optimization-roadmap.md">
# AI-Ready Repository Optimization Plan

## Phase 1: Context Packing (Repomix)
**Objective:** Create a single, compressed artifact of the codebase to maximize "Context Caching" efficiency for both Gemini 1.5 Pro and Claude 3.5 Sonnet.

1.  **Install Repomix:** Add `repomix` to `devDependencies` to ensure version consistency across the team.
2.  **Configuration (`repomix.config.json`):**
    *   **Format:** XML (Structured, strict parsing for agents) or Markdown (Human-readable). *Decision: XML for higher agent precision.*
    *   **Optimization:** Enable `compress: true` (removes comments/whitespace) and `removeEmptyLines`.
    *   **Filtering:** Configure rigorous `.gitignore` usage and custom exclusion patterns (e.g., `coverage/`, `dist/`, `**/*.spec.ts` if irrelevant to architecture).
3.  **Scripts:** Add convenience scripts to `package.json`:
    *   `pnpm run pack:context`: Generates the full context file.
    *   `pnpm run pack:light`: Generates a "lite" version (only source interfaces, no implementation details) for high-level planning.

## Phase 2: Universal Agent Entry Point (`llms.txt`)
**Objective:** Create a standardized "map" for web-based agents or crawlers to instantly understand the project structure without cloning.

1.  **Generate `docs/llms.txt`:**
    *   **Project Summary:** High-level purpose (AWS Media Downloader).
    *   **Architecture Stack:** OpenTofu, TypeScript, AWS Lambda, ElectroDB.
    *   **Key Files:** Direct pointers to `AGENTS.md`, `README.md`, and core entity definitions.
2.  **Generate `docs/llms-full.txt`:** A comprehensive, single-file concatenation of all high-value documentation (Wiki, Architecture constraints).

## Phase 3: "Semantic Memory" (LanceDB Integration)
**Objective:** Enable agents to answer "Where is X?" or "How do I Y?" questions using semantic search rather than brute-force file scanning.

1.  **Scaffold LanceDB:** Add `lancedb` and embedding libraries to the project.
2.  **Update MCP Server:**
    *   Create a new MCP tool: `index_codebase`.
    *   Create a new MCP tool: `search_codebase_semantics`.
3.  **Indexing Strategy:**
    *   Parse TypeScript files into AST chunks (Classes, Functions).
    *   Generate embeddings for each chunk.
    *   Store in a local `.lancedb` directory (gitignored).

## Phase 4: CI/CD Compliance Automation
**Objective:** Move `AGENTS.md` rules from "Passive Guidance" to "Active Enforcement."

1.  **GitHub Action (`.github/workflows/agent-compliance.yml`):**
    *   Trigger: Pull Requests.
    *   Step 1: Use `ts-morph` (existing dependency) to scan for forbidden patterns (e.g., direct `aws-sdk` imports).
    *   Step 2: Verify that `AGENTS.md` boundaries (e.g., "Never modify file X") are respected.
    *   Step 3: Fail build if violations are found.

## Phase 5: IntelliJ Integration
**Objective:** Ensure the setup works seamlessly within your preferred IDE.

1.  **`.gemini/instructions.md`:** Create this file to provide system-level instructions specifically for the Gemini CLI when running locally.
2.  **External Tool Config:** (Optional) Configure IntelliJ "External Tools" to trigger `repomix` generation on save or via shortcut.

## Phase 6: Automatic Context Loading strategies
**Objective:** Ensure agents automatically ingest the packed context without manual intervention.

1.  **For Claude (CLI/Code):**
    *   Create a shell alias or wrapper script (e.g., `claude-context`) that reads `repomix-output.xml` and pipes it into the Claude CLI start command.
    *   *Example:* `alias claude-dev="claude --system \"$(cat repomix-output.xml)\""` (Implementation depends on specific CLI tool used).
2.  **For Gemini (CLI):**
    *   Update `.gemini/instructions.md` with a high-priority rule: "At the start of the session, look for and read 'repomix-output.xml' to load the full codebase context."
3.  **For IDEs (Projects):**
    *   Document the "Project Knowledge" setup for manual IDEs (Claude Desktop, etc.) in `AGENTS.md`.

---

## Next Steps
1.  **Execute Phase 1 (Repomix):** Install and configure the context packer.
2.  **Execute Phase 2 (llms.txt):** Establish the documentation entry points.
3.  **Review Phase 3:** Assess complexity before implementation.
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2019 Jonathan Lloyd

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="repomix.config.json">
{
  "output": {
    "filePath": "repomix-output.xml",
    "format": "xml",
    "compress": true,
    "removeEmptyLines": true,
    "style": "xml"
  },
  "include": [
    "**/*"
  ],
  "ignore": {
    "customPatterns": [
      "coverage/**",
      "dist/**",
      "build/**",
      ".git/**",
      "node_modules/**",
      "pnpm-lock.yaml",
      "pnpm-workspace.yaml",
      "**/*.png",
      "**/*.jpg",
      "**/*.jpeg",
      "**/*.gif",
      "**/*.svg",
      "**/*.ico",
      "**/*.mp4",
      "**/*.mp3",
      "**/*.wav",
      "**/*.pdf",
      "**/*.zip",
      "**/*.gz",
      "**/*.tar",
      "**/*.7z",
      "**/*.exe",
      "**/*.bin",
      "**/*.sqlite",
      "**/*.db",
      "docs/api/index.html",
      "docs/archive/**",
      "**/test/fixtures/**",
      "**/package-lock.json",
      "**/yarn.lock",
      "**/pnpm-lock.yaml"
    ],
    "useGitignore": true,
    "useDefaultPatterns": true
  }
}
</file>

<file path=".github/workflows/check-dep0180.yml">
name: Check DEP0180 Status
on:
  schedule:
    - cron: '0 9 1 * *'  # Monthly on the 1st at 9am UTC
  workflow_dispatch:  # Allow manual triggering
jobs:
  check-deprecation:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup pnpm
        uses: pnpm/action-setup@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: 'pnpm'
      - name: Install dependencies
        run: pnpm install --frozen-lockfile
      - name: Test for DEP0180 warning
        id: check
        run: |
          set -e
          echo "Running generateDependencyGraph.ts WITHOUT --disable-warning=DEP0180..."
          # Run the script without the deprecation suppression flag
          # Capture both stdout and stderr
          OUTPUT=$(node --import tsx scripts/generateDependencyGraph.ts 2>&1) || true
          # Check if DEP0180 appears in the output
          if echo "$OUTPUT" | grep -q "DEP0180"; then
            echo "still_broken=true" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è DEP0180 warning still appears - ts-morph or dependencies not yet fixed"
          else
            echo "still_broken=false" >> $GITHUB_OUTPUT
            echo "‚úÖ No DEP0180 warning detected!"
          fi
          # Get current ts-morph version for the report
          TS_MORPH_VERSION=$(pnpm list ts-morph --depth=0 | grep ts-morph || echo "unknown")
          echo "ts_morph_version=${TS_MORPH_VERSION}" >> $GITHUB_OUTPUT
      - name: Comment on issue if fixed
        if: steps.check.outputs.still_broken == 'false'
        run: |
          gh issue comment 137 \
            --repo j0nathan-ll0yd/aws-cloudformation-media-downloader \
            --body "$(cat <<EOF
          üéâ **DEP0180 Warning No Longer Appears!**
          The monthly automated check found that the \`--disable-warning=DEP0180\` flag can likely be removed.
          **Current ts-morph version**: ${{ steps.check.outputs.ts_morph_version }}
          **Next steps**:
          1. Remove \`--disable-warning=DEP0180\` from \`package.json\` scripts
          2. Test locally to confirm no warnings appear
          3. Close this issue
          [Workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          EOF
          )"
        env:
          GH_TOKEN: ${{ github.token }}
      - name: Notify on failure
        if: failure()
        run: |
          gh issue create \
            --repo j0nathan-ll0yd/aws-cloudformation-media-downloader \
            --title "DEP0180 check workflow failed" \
            --body "The automated DEP0180 deprecation check workflow failed. Check [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})." \
            --label "bug" \
            --label "automation"
        env:
          GH_TOKEN: ${{ github.token }}
</file>

<file path=".github/workflows/check-ts-morph-update.yml">
name: Check ts-morph Update
on:
  pull_request:
    paths:
      - 'pnpm-lock.yaml'
jobs:
  check-ts-morph:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for diff
      - name: Check if ts-morph was updated
        id: check
        run: |
          set -e
          echo "Checking if ts-morph was updated in this PR..."
          # Get the diff of pnpm-lock.yaml against the base branch
          if git diff origin/${{ github.base_ref }} -- pnpm-lock.yaml | grep -q '"ts-morph"'; then
            echo "ts_morph_updated=true" >> $GITHUB_OUTPUT
            echo "‚úÖ ts-morph package was updated in this PR"
          else
            echo "ts_morph_updated=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è ts-morph was not updated in this PR"
          fi
      - name: Comment on DEP0180 issue
        if: steps.check.outputs.ts_morph_updated == 'true'
        run: |
          gh issue comment 137 \
            --repo j0nathan-ll0yd/aws-cloudformation-media-downloader \
            --body "$(cat <<EOF
          üì¶ **ts-morph Update Detected**
          PR #${{ github.event.pull_request.number }} updates the \`ts-morph\` package.
          **Please test if the DEP0180 flag can be removed**:
          \`\`\`bash
          # Run without the --disable-warning flag
          node --import tsx scripts/generateDependencyGraph.ts 2>&1 | grep DEP0180
          \`\`\`
          If no DEP0180 warning appears, you can remove the flag from \`package.json\` and close this issue.
          [View PR](${{ github.event.pull_request.html_url }})
          EOF
          )"
        env:
          GH_TOKEN: ${{ github.token }}
</file>

<file path=".github/workflows/update-agents.yml">
name: Update AGENTS.md
on:
  pull_request:
    types: [closed]
    branches:
      - master
  workflow_dispatch: # Allow manual trigger
jobs:
  update-agents:
    if: github.event.pull_request.merged == true || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Setup pnpm
        uses: pnpm/action-setup@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: 'pnpm'
      - name: Install dependencies
        run: pnpm install --frozen-lockfile
      - name: Update AGENTS.md with recent PRs
        run: pnpm run update:agents-prs
        env:
          CI: true
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Push changes
        run: |
          if git diff --quiet; then
            echo "No changes to push"
          else
            git push
          fi
</file>

<file path=".husky/commit-msg">
#!/usr/bin/env bash
# commit-msg hook: Block AI attribution in commit messages
# This enforces the Zero AI References in Commits convention
#
# To bypass (emergency only): git commit --no-verify

COMMIT_MSG_FILE=$1
COMMIT_MSG=$(cat "$COMMIT_MSG_FILE")

# Patterns to block (case-insensitive)
AI_PATTERNS=(
  "Generated with Claude"
  "Co-Authored-By: Claude"
  "Co-Authored-By:.*Anthropic"
  "AI-generated"
  "AI generated"
  "Generated by AI"
  "Generated by Claude"
  "Generated by GPT"
  "Generated by LLM"
  "ü§ñ"
)

for pattern in "${AI_PATTERNS[@]}"; do
  if echo "$COMMIT_MSG" | grep -iqE "$pattern"; then
    echo ""
    echo "ERROR: Commit message contains AI attribution pattern: '$pattern'"
    echo ""
    echo "This project has a zero-tolerance policy for AI references in commits."
    echo "Please remove AI attribution and write a clean commit message."
    echo ""
    echo "See: docs/wiki/Conventions/Git-Workflow.md"
    echo ""
    echo "To bypass (emergency only): git commit --no-verify"
    exit 1
  fi
done

exit 0
</file>

<file path="apps/.gitkeep">

</file>

<file path="bin/process-fixtures.js">
/**
 * Process raw CloudWatch fixtures into clean test fixtures
 * - Deduplicates similar payloads
 * - Separates incoming/outgoing fixtures
 * - Formats for test consumption
 *
 * Usage: node bin/process-fixtures.js [--input <dir>] [--output <dir>]
 */
‚ãÆ----
/**
 * Calculate structural similarity between two objects
 * Returns 0-1 score (1 = identical structure)
 */
function calculateSimilarity(obj1, obj2)
/**
 * Deduplicate fixtures by structural similarity
 * Keeps most recent fixture for each unique structure
 */
function deduplicateFixtures(fixtures, similarityThreshold = 0.9)
/**
 * Process a raw fixture file
 */
async function processFixtureFile(inputPath, outputDir)
‚ãÆ----
// Extract Lambda/service name (handles both regular Lambdas and BetterAuth)
‚ãÆ----
/**
 * Main processing function
 */
async function main()
</file>

<file path="config/esbuild.config.ts">
import {glob} from 'glob'
‚ãÆ----
// Discover Lambda entry points dynamically
‚ãÆ----
// AWS SDK v3 is available in Lambda runtime - externalize to reduce bundle size
// Note: aws-xray-sdk-core is NOT in Lambda runtime - must be bundled
‚ãÆ----
async function build()
‚ãÆ----
// Ensure build directories exist
‚ãÆ----
// Build all Lambdas in parallel for speed
‚ãÆ----
// Path is like "src/lambdas/WebhookFeedly/src/index.ts" or "./src/lambdas/WebhookFeedly/src/index.ts"
‚ãÆ----
target: 'node22', // Lambda runtime (Node 22.x, not Node 24 yet in AWS)
format: 'cjs', // CommonJS for Lambda compatibility
‚ãÆ----
metafile: isAnalyze, // Generate metafile for bundle analysis
‚ãÆ----
// Handle dynamic imports from better-auth/kysely by bundling them
‚ãÆ----
// Resolve Node.js subpath imports from package.json
‚ãÆ----
// Log level
‚ãÆ----
// Write metafile for bundle analysis
‚ãÆ----
// Get bundle size
</file>

<file path="docs/plans/scheduled-video-handling.md">
# Implementation Plan: Self-Healing Workflows for Scheduled Videos

## Overview

Transform the system from brittle fail-fast behavior to resilient self-healing workflows that intelligently handle scheduled videos, livestreams, and transient failures.

**GitHub Issue**: [#3 - Better handle videos with a scheduled video public time](https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/issues/3)

**Goal**: When YouTube RSS publishes a scheduled video URL before publication, the system should detect this, schedule a retry at the correct time, and automatically download when the video becomes available - with zero user intervention.

---

## Current State Analysis

### Files Entity (`src/entities/Files.ts`)
- **Status values**: `PendingMetadata`, `PendingDownload`, `Downloaded`, `Failed`
- **Index**: `byStatus` GSI sorts by `status` + `availableAt`
- **Missing**: No retry fields, no scheduled status

### StartFileUpload Lambda (`src/lambdas/StartFileUpload/src/index.ts`)
- Fetches video info via `fetchVideoInfo()`
- Handles `CookieExpirationError` specifically
- All other errors result in `Failed` status
- Creates GitHub issues for ALL failures (noise problem)

### FileCoordinator Lambda (`src/lambdas/FileCoordinator/src/index.ts`)
- Runs every 4 minutes (currently disabled)
- Queries `PendingDownload` files where `availableAt <= now` and `url` doesn't exist
- **Missing**: Does not query for scheduled files ready for retry

### YtDlpVideoInfo Type (`src/types/youtube.ts`)
- **Missing**: `release_timestamp`, `is_live`, `live_status` fields that yt-dlp provides for scheduled content

---

## Implementation Phases

### Phase 1: Error Classification Utility

**Files to create:**
- `src/util/video-error-classifier.ts` - Error classification logic

**Changes:**

```typescript
// src/util/video-error-classifier.ts

export type VideoErrorCategory =
  | 'scheduled'           // Scheduled video, retry at release_timestamp
  | 'livestream_upcoming' // Livestream not started, retry when starts
  | 'premiere'            // Premiere scheduled, retry at premiere time
  | 'transient'           // Network/temporary error, exponential backoff
  | 'cookie_expired'      // Cookie needs refresh, requires manual intervention
  | 'permanent'           // Deleted, geo-blocked, private - no retry

export interface VideoErrorClassification {
  category: VideoErrorCategory
  retryable: boolean
  retryAfter?: number      // Unix timestamp for retry (undefined = no retry)
  maxRetries?: number      // Override default max retries for this category
  reason: string           // Human-readable reason
}

export function classifyVideoError(
  error: Error,
  videoInfo?: Partial<ExtendedYtDlpVideoInfo>
): VideoErrorClassification

export function calculateExponentialBackoff(
  retryCount: number,
  baseDelaySeconds?: number,
  maxDelaySeconds?: number
): number
```

**Classification Logic:**

| Condition | Category | Retry Strategy |
|-----------|----------|----------------|
| `videoInfo.release_timestamp > now` | `scheduled` | Retry at `release_timestamp + 300` (5 min buffer) |
| `videoInfo.is_live === false && live_status === 'upcoming'` | `livestream_upcoming` | Retry at `release_timestamp` or exponential backoff |
| `error instanceof CookieExpirationError` | `cookie_expired` | No automatic retry, create GitHub issue |
| Network error patterns (timeout, ECONNRESET) | `transient` | Exponential backoff (15min, 30min, 1hr, 2hr, 4hr) |
| "Video unavailable" without release_timestamp | `permanent` | No retry, create GitHub issue |
| "Private video", "Deleted video", "Geo-blocked" | `permanent` | No retry, create GitHub issue |

**Test coverage:**
- Unit tests for each error category
- Edge cases: release_timestamp in past, missing videoInfo, malformed errors

---

### Phase 2: Schema Updates

**Files to modify:**
- `src/types/enums.ts` - Add `Scheduled` status
- `src/entities/Files.ts` - Add retry metadata fields
- `src/types/youtube.ts` - Extend with scheduling fields

#### 2.1 FileStatus Enum Update

```typescript
// src/types/enums.ts
export enum FileStatus {
  PendingMetadata = 'PendingMetadata',
  PendingDownload = 'PendingDownload',
  Scheduled = 'Scheduled',      // NEW: Waiting for availability
  Downloaded = 'Downloaded',
  Failed = 'Failed'
}
```

#### 2.2 Files Entity Schema Update

```typescript
// src/entities/Files.ts - New attributes
attributes: {
  // ... existing attributes ...

  // Retry metadata (NEW)
  retryAfter: {type: 'number', required: false},           // Unix timestamp for next retry
  retryCount: {type: 'number', required: false, default: 0}, // Number of retry attempts
  maxRetries: {type: 'number', required: false, default: 5}, // Max retries before permanent failure
  lastError: {type: 'string', required: false},            // Last error message for debugging
  scheduledPublishTime: {type: 'number', required: false}, // Original publish time from YouTube
  errorCategory: {type: 'string', required: false}         // Error classification category
}
```

**Index consideration:** The existing `byStatus` index already sorts by `availableAt`. For scheduled files, we'll set `availableAt` to the retry time, allowing FileCoordinator to query efficiently:
- `status = 'Scheduled' AND availableAt <= now` - Files ready for retry

#### 2.3 YtDlpVideoInfo Type Extension

```typescript
// src/types/youtube.ts
export interface YtDlpVideoInfo {
  // ... existing fields ...

  // Scheduling fields (yt-dlp provides these for scheduled content)
  release_timestamp?: number    // Unix timestamp when video becomes available
  is_live?: boolean            // Whether this is a livestream
  live_status?: 'is_live' | 'is_upcoming' | 'was_live' | 'not_live'
  premiere_timestamp?: number   // Premiere scheduled time (if applicable)
  availability?: 'public' | 'unlisted' | 'private' | 'needs_auth' | 'subscriber_only'
}
```

---

### Phase 3: StartFileUpload Intelligent Error Handling

**Files to modify:**
- `src/lambdas/StartFileUpload/src/index.ts` - Integrate error classifier

**Changes:**

1. **Attempt metadata fetch even on failure** - Use `--skip-download` flag to get video info without downloading
2. **Apply error classification** to determine retry strategy
3. **Update file with retry metadata** instead of immediately failing
4. **Only create GitHub issues for permanent failures** - Eliminates noise

**Updated Error Handling Flow:**

```typescript
// src/lambdas/StartFileUpload/src/index.ts - Error handler

try {
  // ... existing download logic ...
} catch (error) {
  assertIsError(error)

  // Attempt to fetch metadata even after failure (may have release_timestamp)
  let videoInfo: Partial<ExtendedYtDlpVideoInfo> | undefined
  try {
    videoInfo = await fetchVideoInfoSafe(fileUrl) // New function with --skip-download
  } catch {
    // Ignore - we tried our best to get scheduling info
  }

  // Classify the error
  const classification = classifyVideoError(error, videoInfo)

  // Get existing file for retry count
  const {data: existingFile} = await Files.get({fileId}).go()
  const retryCount = (existingFile?.retryCount ?? 0) + 1
  const maxRetries = classification.maxRetries ?? existingFile?.maxRetries ?? 5

  if (classification.retryable && classification.retryAfter && retryCount <= maxRetries) {
    // Schedule retry
    await Files.update({fileId})
      .set({
        status: FileStatus.Scheduled,
        retryAfter: classification.retryAfter,
        availableAt: classification.retryAfter, // For GSI query efficiency
        retryCount,
        lastError: classification.reason,
        scheduledPublishTime: videoInfo?.release_timestamp,
        errorCategory: classification.category
      })
      .go()

    await putMetrics([
      {name: 'ScheduledVideoDetected', value: 1, unit: 'Count'},
      {name: 'RetryScheduled', value: 1, unit: 'Count', dimensions: [{Name: 'Category', Value: classification.category}]}
    ])

    logInfo(`Scheduled retry for ${fileId}`, {
      retryAfter: new Date(classification.retryAfter * 1000).toISOString(),
      reason: classification.reason,
      retryCount
    })

    // NO GitHub issue for scheduled retries
    return response(200, {fileId, status: 'scheduled', retryAfter: classification.retryAfter})
  }

  // Max retries exceeded OR permanent failure
  await Files.update({fileId})
    .set({
      status: FileStatus.Failed,
      lastError: classification.reason,
      retryCount,
      errorCategory: classification.category
    })
    .go()

  await putMetrics([{name: 'LambdaExecutionFailure', value: 1, unit: 'Count'}])

  // Only create GitHub issues for permanent failures
  if (classification.category === 'permanent' || classification.category === 'cookie_expired') {
    if (error instanceof CookieExpirationError) {
      await createCookieExpirationIssue(fileId, fileUrl, error)
    } else {
      await createVideoDownloadFailureIssue(fileId, fileUrl, error, classification.reason)
    }
  }

  return errorResponse(error)
}
```

**New utility function:**

```typescript
// src/lib/vendor/YouTube.ts

/**
 * Fetch video info without attempting download
 * Used to get release_timestamp for unavailable videos
 */
export async function fetchVideoInfoSafe(uri: string): Promise<YtDlpVideoInfo | undefined> {
  try {
    // Use --skip-download to only get metadata
    const ytDlp = new YTDlpWrap(getRequiredEnv('YtdlpBinaryPath'))
    const info = await ytDlp.getVideoInfo([uri, '--skip-download', '--ignore-errors', ...commonFlags])
    return info as YtDlpVideoInfo
  } catch {
    return undefined
  }
}
```

---

### Phase 4: FileCoordinator Updates

**Files to modify:**
- `src/lambdas/FileCoordinator/src/index.ts` - Query scheduled files

**Changes:**

Update `getFileIdsToBeDownloaded()` to include both `PendingDownload` AND `Scheduled` files ready for retry:

```typescript
// src/lambdas/FileCoordinator/src/index.ts

async function getFileIdsToBeDownloaded(): Promise<string[]> {
  const now = Math.floor(Date.now() / 1000)

  // Query 1: Pending downloads
  const {data: pendingFiles} = await Files.query
    .byStatus({status: FileStatus.PendingDownload})
    .where(({availableAt}, {lte}) => lte(availableAt, now))
    .where(({url}, {notExists}) => notExists(url))
    .go()

  // Query 2: Scheduled files ready for retry
  const {data: scheduledFiles} = await Files.query
    .byStatus({status: FileStatus.Scheduled})
    .where(({availableAt}, {lte}) => lte(availableAt, now))
    .go()

  if (!pendingFiles || !scheduledFiles) {
    throw new UnexpectedError(providerFailureErrorMessage)
  }

  const allFileIds = [
    ...pendingFiles.map(f => f.fileId),
    ...scheduledFiles.map(f => f.fileId)
  ]

  logInfo(`Files to process: ${allFileIds.length}`, {
    pending: pendingFiles.length,
    scheduled: scheduledFiles.length
  })

  return allFileIds
}
```

**Concurrency consideration:** Add rate limiting for scheduled retries to avoid overwhelming yt-dlp:

```typescript
// Process in batches of 5 with 10-second delays between batches
const BATCH_SIZE = 5
const BATCH_DELAY_MS = 10000

for (let i = 0; i < allFileIds.length; i += BATCH_SIZE) {
  const batch = allFileIds.slice(i, i + BATCH_SIZE)
  await Promise.all(batch.map(fileId => initiateFileDownload(fileId)))

  if (i + BATCH_SIZE < allFileIds.length) {
    await new Promise(resolve => setTimeout(resolve, BATCH_DELAY_MS))
  }
}
```

---

### Phase 5: Infrastructure & Monitoring

**Files to modify:**
- `terraform/file_coordinator.tf` - Enable scheduling, update IAM
- `terraform/main.tf` - No changes needed (GSI already exists)

#### 5.1 CloudWatch Metrics

New metrics to track:
- `ScheduledVideoDetected` - Count of detected scheduled videos
- `RetryScheduled` - Count of scheduled retries (with Category dimension)
- `RetrySuccess` - Count of successful retries
- `RetryExhausted` - Count of files that exceeded max retries

#### 5.2 CloudWatch Alarms (Optional)

```hcl
# terraform/cloudwatch_alarms.tf

resource "aws_cloudwatch_metric_alarm" "RetryQueueBacklog" {
  alarm_name          = "MediaDownloader-RetryQueueBacklog"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 3
  metric_name         = "ScheduledVideoDetected"
  namespace           = "MediaDownloader"
  period              = 3600
  statistic           = "Sum"
  threshold           = 100
  alarm_description   = "High number of scheduled videos in retry queue"
  alarm_actions       = [aws_sns_topic.alerts.arn]
}
```

#### 5.3 Enable FileCoordinator Schedule

```hcl
# terraform/file_coordinator.tf - Change from DISABLED to ENABLED
resource "aws_cloudwatch_event_rule" "FileCoordinator" {
  name                = "FileCoordinator"
  schedule_expression = "rate(4 minutes)"
  state               = "ENABLED"  # Changed from DISABLED
}
```

---

### Phase 6: Testing Strategy

#### 6.1 Unit Tests

**Error Classifier Tests** (`src/util/video-error-classifier.test.ts`):
- Test each error category detection
- Test exponential backoff calculations
- Test edge cases (past timestamps, missing fields)

**StartFileUpload Tests** (`src/lambdas/StartFileUpload/test/index.test.ts`):
- Test scheduled video detection flow
- Test retry count increment and max retry behavior
- Test GitHub issue creation only for permanent failures
- Mock ElectroDB using `createElectroDBEntityMock`

**FileCoordinator Tests** (`src/lambdas/FileCoordinator/test/index.test.ts`):
- Test combined query for pending + scheduled files
- Test batch processing with delays
- Verify correct invocation of StartFileUpload

#### 6.2 Integration Tests (LocalStack)

**Scheduled Video Flow**:
1. Create file with `status: Scheduled`, `availableAt: past`
2. Run FileCoordinator
3. Verify StartFileUpload invoked
4. Mock successful download
5. Verify status changes to `Downloaded`

**Retry Exhaustion Flow**:
1. Create file with `retryCount: 5`, `maxRetries: 5`
2. Trigger retry
3. Verify status changes to `Failed`
4. Verify GitHub issue creation (mocked)

---

## Success Criteria

### User Experience
- [ ] 100% success rate for scheduled videos (eventually download when available)
- [ ] Zero manual intervention required for scheduled content
- [ ] iOS app shows "Scheduled for [date]" instead of "Failed"
- [ ] Zero support tickets for scheduled video "failures"

### Operational Excellence
- [ ] Zero GitHub issues for scheduled videos (noise eliminated)
- [ ] 100% actionable GitHub issues (only permanent failures)
- [ ] Average retry latency <= 1 hour after video becomes available
- [ ] Retry success rate >= 95%

### System Health
- [ ] CloudWatch metrics track scheduled video detection rate
- [ ] CloudWatch alarms alert on retry exhaustion
- [ ] Retry queue size monitored

---

## File Change Summary

### New Files
| File | Purpose |
|------|---------|
| `src/util/video-error-classifier.ts` | Error classification logic |
| `src/util/video-error-classifier.test.ts` | Unit tests |

### Modified Files
| File | Changes |
|------|---------|
| `src/types/enums.ts` | Add `Scheduled` to FileStatus |
| `src/types/youtube.ts` | Add scheduling fields to YtDlpVideoInfo |
| `src/entities/Files.ts` | Add retry metadata attributes |
| `src/lambdas/StartFileUpload/src/index.ts` | Integrate error classifier |
| `src/lambdas/StartFileUpload/test/index.test.ts` | Update tests |
| `src/lambdas/FileCoordinator/src/index.ts` | Query scheduled files |
| `src/lambdas/FileCoordinator/test/index.test.ts` | Update tests |
| `src/lib/vendor/YouTube.ts` | Add `fetchVideoInfoSafe()` |
| `terraform/file_coordinator.tf` | Enable schedule |

---

## Risk Mitigation

### Risk: yt-dlp doesn't return release_timestamp for some scheduled videos
**Mitigation**: Fall back to exponential backoff if `release_timestamp` unavailable. Use increasing delays: 15min -> 30min -> 1hr -> 2hr -> 4hr.

### Risk: Overwhelming YouTube with retries
**Mitigation**: Batch processing with delays, max 5 concurrent downloads, rate limiting in FileCoordinator.

### Risk: Retry queue grows unbounded
**Mitigation**: Max retries limit (default 5), CloudWatch alarm on queue size, retry exhaustion handling.

### Risk: DynamoDB hot partition for `status=Scheduled`
**Mitigation**: The existing GSI design handles this well. If needed, add a date-based partition suffix.

---

## Implementation Order

1. **Phase 1**: Error classifier (standalone, testable) - 4 hours
2. **Phase 2**: Schema updates (minimal risk) - 2 hours
3. **Phase 3**: StartFileUpload integration - 4 hours
4. **Phase 4**: FileCoordinator updates - 2 hours
5. **Phase 5**: Infrastructure & monitoring - 2 hours
6. **Phase 6**: Integration testing - 4 hours

**Total estimated effort**: 18 hours

---

## Appendix: Error Message Patterns

### Scheduled Video Patterns (from yt-dlp)
- "Video unavailable" + `release_timestamp` present
- "Premieres in X hours"
- "Scheduled for [date]"

### Permanent Failure Patterns
- "Video unavailable" (without release_timestamp)
- "This video is private"
- "This video has been removed"
- "This video is no longer available"
- "The uploader has not made this video available"
- "This video contains content from [X], who has blocked it"

### Transient Error Patterns
- "HTTP Error 429: Too Many Requests"
- "Connection reset by peer"
- "ECONNRESET"
- "ETIMEDOUT"
- "Network is unreachable"
</file>

<file path="docs/wiki/Authentication/Better-Auth-Architecture.md">
# Better Auth Architecture

## Overview

This project uses [Better Auth](https://www.better-auth.com/) for authentication, integrated with ElectroDB and DynamoDB in a serverless Lambda architecture. This represents the first production ElectroDB adapter for Better Auth.

## Architecture Components

### Core Stack

```
iOS App ‚Üí API Gateway ‚Üí Lambda ‚Üí Better Auth ‚Üí ElectroDB Adapter ‚Üí DynamoDB
                                                                    ‚Üì
                                                            Single Table Design
```

### Key Technologies

- **Better Auth 1.4.3**: Modern TypeScript authentication framework
- **ElectroDB**: Type-safe DynamoDB ORM with single-table design
- **Apple Sign In**: OAuth provider using ID token flow
- **AWS Lambda**: Serverless compute for auth endpoints
- **DynamoDB**: NoSQL database with optimized GSIs

## Better Auth Integration

### Configuration

Better Auth is configured as a singleton in `src/lib/vendor/BetterAuth/config.ts`:

```typescript
import {betterAuth} from 'better-auth'
import {createElectroDBAdapter} from './electrodb-adapter'
import {fixtureLoggingHooks} from '../../better-auth/fixture-hooks'

export const auth = betterAuth({
  database: createElectroDBAdapter(),
  baseURL: process.env.ApplicationUrl,
  socialProviders: {
    apple: {
      clientId: '<from config>',
      appBundleIdentifier: '<from config>',
      disableIdTokenSignin: false
    }
  },
  session: {
    expiresIn: 60 * 60 * 24 * 30, // 30 days
    updateAge: 60 * 60 * 24
  },
  hooks: fixtureLoggingHooks
})
```

### Key Features

1. **ID Token Authentication**: Direct ID token verification (eliminates 200-500ms token exchange)
2. **Session Management**: 30-day sessions with automatic refresh
3. **Mobile-First**: Token-based auth instead of cookies
4. **Fixture Logging**: Production debugging via CloudWatch
5. **Type Safety**: Full TypeScript support throughout stack

## Database Schema

### Entities

Better Auth uses four ElectroDB entities in the single-table design:

#### Users Entity

Stores user account data and identity provider information.

```typescript
{
  userId: string        // Primary key
  email: string         // Indexed via gsi3 (byEmail)
  emailVerified: boolean
  firstName: string
  lastName: string
  identityProviders: {  // OAuth provider data
    userId: string
    email: string
    emailVerified: boolean
    isPrivateEmail: boolean
    accessToken: string
    refreshToken: string
    tokenType: string
    expiresAt: number
  }
}
```

**Indexes**:
- Primary: `userId`
- byEmail (gsi3): `email` ‚Üí Fast email lookups

#### Sessions Entity

Manages active user sessions with device tracking.

```typescript
{
  sessionId: string     // Primary key
  userId: string        // Indexed via gsi1 (byUser)
  deviceId: string      // Indexed via gsi2 (byDevice)
  expiresAt: number
  token: string         // Hashed session token
  ipAddress: string
  userAgent: string
  createdAt: number
  updatedAt: number
}
```

**Indexes**:
- Primary: `sessionId`
- byUser (gsi1): `userId` + `expiresAt` ‚Üí All sessions for a user
- byDevice (gsi2): `deviceId` + `createdAt` ‚Üí All sessions for a device

#### Accounts Entity

Links users to OAuth providers (Apple, Google, etc.).

```typescript
{
  accountId: string         // Primary key
  userId: string            // Indexed via gsi1 (byUser)
  providerId: string        // 'apple', 'google', etc.
  providerAccountId: string // User ID from provider
  accessToken: string
  refreshToken: string
  expiresAt: number
  scope: string
  tokenType: string
  idToken: string           // OIDC ID token
  createdAt: number
  updatedAt: number
}
```

**Indexes**:
- Primary: `accountId`
- byUser (gsi1): `userId` + `providerId` ‚Üí All accounts for a user
- byProvider (gsi2): `providerId` + `providerAccountId` ‚Üí Reverse lookup

#### VerificationTokens Entity

Temporary tokens for email verification flows.

```typescript
{
  token: string         // Primary key
  identifier: string    // Email or user ID
  expiresAt: number
}
```

**Indexes**:
- Primary: `token`
- byIdentifier (gsi1): `identifier` ‚Üí Find tokens by email

### GSI Sharing Strategy

DynamoDB has 5 GSIs that are shared across all entities:

| GSI | ElectroDB Name | Primary Use | Also Used By |
|-----|---------------|-------------|--------------|
| gsi1 | UserCollection | Query by userId | Sessions, Accounts, VerificationTokens |
| gsi2 | FileCollection | Query by fileId | Sessions (by device), Accounts (by provider) |
| gsi3 | DeviceCollection | Query by deviceId | Users (by email) |
| gsi4 | StatusIndex | Query files by status | Files entity |
| gsi5 | KeyIndex | Query files by key | Files entity |

**Key Insight**: Multiple entities share GSIs by using different partition key prefixes (ElectroDB adds entity type prefixes automatically).

## Authentication Flows

### New User Registration

```
iOS App
  ‚Üì Sign in with Apple ‚Üí ID Token
  ‚Üì POST /registerUser {idToken, firstName, lastName}
RegisterUser Lambda
  ‚Üì auth.api.signInSocial({idToken, provider: 'apple'})
Better Auth
  ‚Üì Verify ID token with Apple's JWKS
  ‚Üì Create user via ElectroDB adapter
ElectroDB Adapter
  ‚Üì transformUserFromAuth()
  ‚Üì Users.create()
DynamoDB
  ‚Üì Store user + account + session
  ‚Üê Return session token
iOS App
  ‚Üê Save token for authenticated requests
```

### Existing User Login

```
iOS App
  ‚Üì Sign in with Apple ‚Üí ID Token
  ‚Üì POST /login {idToken}
LoginUser Lambda
  ‚Üì auth.api.signInSocial({idToken, provider: 'apple'})
Better Auth
  ‚Üì Verify ID token
  ‚Üì Lookup user by provider account ID
ElectroDB Adapter
  ‚Üì Accounts.query.byProvider()
  ‚Üì Create new session
DynamoDB
  ‚Üì Store session
  ‚Üê Return session token
iOS App
  ‚Üê Save token for authenticated requests
```

### Authenticated Request

```
iOS App
  ‚Üì GET /listFiles (Authorization: Bearer <token>)
API Gateway
  ‚Üì Invoke ApiGatewayAuthorizer Lambda
ApiGatewayAuthorizer
  ‚Üì auth.api.getSession({headers})
Better Auth
  ‚Üì Validate session token
ElectroDB Adapter
  ‚Üì Sessions.get({sessionId})
  ‚Üì Users.get({userId})
DynamoDB
  ‚Üê Return session + user
  ‚Üê Generate IAM policy (Allow/Deny)
API Gateway
  ‚Üì Forward request to ListFiles Lambda
  ‚Üê Return response
iOS App
  ‚Üê Receive data
```

## Lambda Integration

### Auth Lambdas

| Lambda | Endpoint | Better Auth API | Purpose |
|--------|----------|----------------|---------|
| RegisterUser | POST /registerUser | signInSocial() | Create new user account |
| LoginUser | POST /login | signInSocial() | Authenticate existing user |
| RefreshToken | POST /refreshToken | getSession() | Refresh expired session |
| ApiGatewayAuthorizer | ALL /* | getSession() | Validate session tokens |

### Environment Variables

Required environment variables for Better Auth:

- `ApplicationUrl`: Base URL for OAuth callbacks (e.g., `https://api.example.com`)
- `SignInWithAppleConfig`: JSON with `{client_id, bundle_id}`
- `DynamoDBTableName`: Name of DynamoDB table

### Error Handling

Better Auth operations are wrapped in try-catch blocks:

```typescript
try {
  const result = await auth.api.signInSocial({...})
  return response(context, 200, result)
} catch (error) {
  logError('Better Auth operation failed', {error})
  return response(context, 401, {message: 'Authentication failed'})
}
```

## ElectroDB Adapter

### Design Pattern

The adapter implements bidirectional transformers between Better Auth and ElectroDB:

```typescript
// Better Auth ‚Üí ElectroDB
transformUserFromAuth(authUser: User): ElectroUserCreate
transformSessionFromAuth(authSession: Session): ElectroSessionCreate

// ElectroDB ‚Üí Better Auth
transformUserToAuth(electroUser: ElectroUserItem): User
transformSessionToAuth(electroSession: ElectroSessionItem): Session
```

### Adapter Methods

The adapter implements the Better Auth database interface:

**User Operations**:
- `createUser(data)`: Create new user account
- `getUser(userId)`: Fetch user by ID
- `getUserByEmail(email)`: Fetch user by email (uses gsi3)
- `updateUser(userId, data)`: Update user fields
- `deleteUser(userId)`: Remove user account

**Session Operations**:
- `createSession(data)`: Create new session
- `getSession(sessionId)`: Fetch session by ID
- `updateSession(sessionId, data)`: Update session
- `deleteSession(sessionId)`: Remove session

**Account Operations**:
- `createAccount(data)`: Link OAuth provider
- `getAccount(accountId)`: Fetch account by ID
- `linkAccount(userId, accountId)`: Associate account with user

**Verification Token Operations**:
- `createVerificationToken(data)`: Create email verification token
- `getVerificationToken(token)`: Fetch verification token
- `deleteVerificationToken(token)`: Remove used token

### Type Safety

All transformers maintain full type safety:

```typescript
type ElectroUserCreate = {
  userId: string
  email: string
  emailVerified: boolean
  firstName: string
  lastName: string
  identityProviders: IdentityProvidersData
}

function transformUserFromAuth(authUser: Partial<User>): ElectroUserCreate {
  const {firstName, lastName} = splitFullName(authUser.name)
  return {
    userId: authUser.id || uuidv4(),
    email: authUser.email!,
    emailVerified: authUser.emailVerified ?? false,
    firstName,
    lastName,
    identityProviders: {...}
  }
}
```

## Fixture Logging

### Hook Integration

Better Auth hooks enable production debugging:

```typescript
import {fixtureLoggingHooks} from '../../better-auth/fixture-hooks'

export const auth = betterAuth({
  // ... config ...
  hooks: fixtureLoggingHooks
})
```

### How It Works

1. **Before Hook**: Logs incoming requests with fixture markers
2. **After Hook**: Logs outgoing responses with fixture markers
3. **CloudWatch**: Stores logs with `__FIXTURE_MARKER__` tag
4. **Extraction**: `bin/extract-fixtures.sh` pulls fixtures from CloudWatch
5. **Processing**: `bin/process-fixtures.js` deduplicates and formats
6. **Testing**: Fixtures used in integration tests

### Fixture Naming

Better Auth fixtures follow PascalCase naming:

- `/auth/sign-in` ‚Üí `BetterAuthSignIn`
- `/auth/sign-up` ‚Üí `BetterAuthSignUp`
- `/auth/refresh-token` ‚Üí `BetterAuthRefreshToken`

## Testing Strategy

### Unit Testing

Better Auth components are mocked using helpers:

```typescript
import {createBetterAuthMock} from '../../../test/helpers/better-auth-mock'

jest.mock('../../lib/vendor/BetterAuth/config', () => ({
  auth: createBetterAuthMock()
}))
```

ElectroDB entities are mocked consistently:

```typescript
import {createElectroDBEntityMock} from '../../../test/helpers/electrodb-mock'

jest.mock('../../entities/Users', () => ({
  Users: createElectroDBEntityMock()
}))
```

### Integration Testing

LocalStack tests validate full Better Auth flows:

1. Create user with Apple ID token
2. Create session
3. Validate session token
4. Query sessions by user
5. Delete session

See `docs/wiki/Testing/ElectroDB-Testing-Patterns.md` for detailed examples.

## Performance Considerations

### Query Optimization

1. **Email Lookup**: Uses gsi3 index instead of table scan (10-100x faster)
2. **Session Queries**: Uses gsi1 to fetch all user sessions efficiently
3. **Provider Lookup**: Uses gsi2 to find accounts by provider ID

### Cold Start Optimization

Better Auth singleton is initialized once per Lambda container:

```typescript
// At top of Lambda handler
import {auth} from '../../lib/vendor/BetterAuth/config'

// Lambda stays warm, auth instance reused
export const handler = async (event, context) => {
  const result = await auth.api.getSession(...)
  return response(context, 200, result)
}
```

### Connection Pooling

ElectroDB shares a single DynamoDB DocumentClient across all entities, reducing connection overhead.

## Security Features

### ID Token Verification

Better Auth verifies Apple ID tokens using:

1. Fetches Apple's public JWKS
2. Validates token signature (RS256)
3. Checks token expiration
4. Validates audience claim (bundle ID)
5. Validates issuer claim (Apple)

### Session Token Security

- Tokens are hashed before storage
- Session expiration enforced server-side
- IP address and user agent logged for audit
- Session invalidation supported

### Environment Variable Safety

Per project conventions, required environment variables are accessed without fallbacks:

```typescript
// ‚úì Correct - fails fast if missing
const config = JSON.parse(process.env.SignInWithAppleConfig)

// ‚úó Wrong - silent failures hide configuration errors
try {
  const config = JSON.parse(process.env.SignInWithAppleConfig)
} catch {
  return fallbackConfig
}
```

## Migration from JWT

The project migrated from custom JWT authentication to Better Auth:

- **Before**: Manual JWT signing/verification with JOSE
- **After**: Better Auth session-based authentication
- **Benefits**:
  - Session management built-in
  - OAuth provider support
  - Type-safe database operations
  - Better mobile app experience
  - Reduced latency (ID token flow)

See `docs/wiki/iOS/Apple-Sign-In-ID-Token-Migration.md` for iOS migration details.

## Troubleshooting

### "Invalid ID token" Error

Check that iOS app is sending `identityToken` not `authorizationCode`:

```swift
// ‚úì Correct
let idToken = String(data: credential.identityToken!, encoding: .utf8)

// ‚úó Wrong
let code = String(data: credential.authorizationCode!, encoding: .utf8)
```

### Session Not Found

Verify session hasn't expired:

```typescript
const session = await auth.api.getSession({headers})
if (!session || session.expiresAt < Date.now()) {
  return response(context, 401, {message: 'Session expired'})
}
```

### Email Lookup Slow

Ensure email GSI is created:

```bash
aws dynamodb describe-table --table-name MediaDownloader \
  | jq '.Table.GlobalSecondaryIndexes[] | select(.IndexName == "DeviceCollection")'
```

## References

- [Better Auth Documentation](https://www.better-auth.com/docs)
- [ElectroDB Documentation](https://electrodb.dev/)
- [Apple Sign In Documentation](https://developer.apple.com/sign-in-with-apple/)
- [iOS ID Token Migration](../iOS/Apple-Sign-In-ID-Token-Migration.md)
- [ElectroDB Adapter Design](ElectroDB-Adapter-Design.md)
- [ElectroDB Testing Patterns](../Testing/ElectroDB-Testing-Patterns.md)
</file>

<file path="docs/wiki/Authentication/ElectroDB-Adapter-Design.md">
# ElectroDB Adapter Design

## Overview

This project contains the **first production ElectroDB adapter for Better Auth**. The adapter bridges Better Auth's authentication framework with ElectroDB's type-safe DynamoDB ORM, enabling session-based authentication in a single-table design.

## Why This Matters

### The Problem

Better Auth provides official adapters for:
- Prisma (SQL databases)
- Drizzle ORM (SQL databases)
- Kysely (SQL databases)
- MongoDB

But had **no adapter for DynamoDB** - the most common serverless database.

### The Solution

This adapter enables:
- ‚úÖ Zero additional infrastructure (uses existing DynamoDB table)
- ‚úÖ Type-safe operations throughout
- ‚úÖ Single-table design (consistent with best practices)
- ‚úÖ Reusable across any DynamoDB + ElectroDB project
- ‚úÖ Full Better Auth feature support

### Community Value

This adapter can be extracted and published as `@your-org/better-auth-electrodb-adapter` to serve the broader serverless community.

## Architecture

### Layer Model

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Better Auth Framework           ‚îÇ
‚îÇ     (auth.api.signInSocial, etc.)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚îÇ Better Auth Adapter Interface
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      ElectroDB Adapter (this file)      ‚îÇ
‚îÇ  ‚Ä¢ Bidirectional transformers           ‚îÇ
‚îÇ  ‚Ä¢ Type conversions                     ‚îÇ
‚îÇ  ‚Ä¢ Error handling                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚îÇ ElectroDB Entity API
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     ElectroDB Entities (type-safe)      ‚îÇ
‚îÇ  Users | Sessions | Accounts | Tokens   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚îÇ DynamoDB DocumentClient
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   DynamoDB Single Table (MediaDownloader)‚îÇ
‚îÇ        With optimized GSIs              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Core Concepts

### 1. Bidirectional Transformers

The adapter implements transformation functions in both directions:

**Better Auth ‚Üí ElectroDB**:
```typescript
transformUserFromAuth(authUser: Partial<User>): ElectroUserCreate
transformSessionFromAuth(authSession: Partial<Session>): ElectroSessionCreate
transformAccountFromAuth(authAccount: Partial<ExtendedAccount>): ElectroAccountCreate
```

**ElectroDB ‚Üí Better Auth**:
```typescript
transformUserToAuth(electroUser: Partial<ElectroUserItem>): User
transformSessionToAuth(electroSession: Partial<ElectroSessionItem>): Session
transformAccountToAuth(electroAccount: ElectroAccountItem): ExtendedAccount
```

### 2. Type Safety

Every transformation maintains full TypeScript type safety:

```typescript
// ElectroDB create types - what we send to database
type ElectroUserCreate = {
  userId: string
  email: string
  emailVerified: boolean
  firstName: string
  lastName: string
  identityProviders: IdentityProvidersData
}

// ElectroDB response types - what we get back from database
type ElectroUserItem = EntityItem<typeof Users> & {
  createdAt?: number
  updatedAt?: number
}
```

### 3. Schema Mapping

Better Auth expects certain fields that don't map 1:1 to ElectroDB:

| Better Auth Field | ElectroDB Field | Transformation |
|------------------|----------------|----------------|
| `User.name` | `firstName` + `lastName` | Split/join name |
| `User.createdAt` (Date) | `createdAt` (number) | Date ‚Üî timestamp |
| `Session.expiresAt` (Date) | `expiresAt` (number) | Date ‚Üî timestamp |
| `Account.accountId` | `providerAccountId` | Field rename |
| `null` values | `undefined` | ElectroDB compatibility |

## Implementation Details

### Adapter Interface

The adapter implements Better Auth's expected database interface:

```typescript
export function createElectroDBAdapter() {
  return {
    id: 'electrodb',

    // User Operations
    createUser(data: Partial<User>): Promise<User>
    getUser(userId: string): Promise<User | null>
    getUserByEmail(email: string): Promise<User | null>
    updateUser(userId: string, data: Partial<User>): Promise<User>
    deleteUser(userId: string): Promise<void>

    // Session Operations
    createSession(data: Partial<Session>): Promise<Session>
    getSession(sessionId: string): Promise<Session | null>
    updateSession(sessionId: string, data: Partial<Session>): Promise<Session>
    deleteSession(sessionId: string): Promise<void>

    // Account Operations
    createAccount(data: Partial<ExtendedAccount>): Promise<ExtendedAccount>
    getAccount(accountId: string): Promise<ExtendedAccount | null>
    linkAccount(userId: string, accountId: string): Promise<void>

    // Verification Token Operations
    createVerificationToken(data: {identifier: string; token: string; expiresAt: Date}): Promise<void>
    getVerificationToken(token: string): Promise<{identifier: string; token: string; expiresAt: Date} | null>
    deleteVerificationToken(token: string): Promise<void>
  }
}
```

### Name Splitting Utility

Better Auth stores full names, but our schema separates first/last names:

```typescript
export function splitFullName(fullName?: string): {firstName: string; lastName: string} {
  const parts = (fullName || '').split(' ')
  return {
    firstName: parts[0] || '',
    lastName: parts.slice(1).join(' ') || ''
  }
}

// Examples:
splitFullName("John Doe")           // {firstName: "John", lastName: "Doe"}
splitFullName("John Doe Smith")     // {firstName: "John", lastName: "Doe Smith"}
splitFullName("John")               // {firstName: "John", lastName: ""}
splitFullName("")                   // {firstName: "", lastName: ""}
```

### User Transformations

**Better Auth ‚Üí ElectroDB**:
```typescript
function transformUserFromAuth(authUser: Partial<User> & {id?: string}): ElectroUserCreate {
  const {firstName, lastName} = splitFullName(authUser.name)

  // ElectroDB requires all fields in identityProviders map
  const identityProviders: IdentityProvidersData = {
    userId: '',
    email: '',
    emailVerified: false,
    isPrivateEmail: false,
    accessToken: '',
    refreshToken: '',
    tokenType: '',
    expiresAt: 0
  }

  return {
    userId: authUser.id || uuidv4(),
    email: authUser.email!,
    emailVerified: authUser.emailVerified ?? false,
    firstName,
    lastName,
    identityProviders
  }
}
```

**ElectroDB ‚Üí Better Auth**:
```typescript
function transformUserToAuth(electroUser: Partial<ElectroUserItem>): User {
  return {
    id: electroUser.userId!,
    email: electroUser.email!,
    emailVerified: electroUser.emailVerified ?? false,
    name: `${electroUser.firstName ?? ''} ${electroUser.lastName ?? ''}`.trim(),
    createdAt: new Date(electroUser.createdAt ?? Date.now()),
    updatedAt: new Date(electroUser.updatedAt ?? Date.now())
  }
}
```

### Session Transformations

**Better Auth ‚Üí ElectroDB**:
```typescript
function transformSessionFromAuth(authSession: Partial<Session> & {id?: string; deviceId?: string}): ElectroSessionCreate {
  return {
    sessionId: authSession.id || uuidv4(),
    userId: authSession.userId!,
    expiresAt: authSession.expiresAt
      ? authSession.expiresAt.getTime()
      : Date.now() + 30 * 24 * 60 * 60 * 1000,
    token: authSession.token || uuidv4(),
    ipAddress: authSession.ipAddress ?? undefined,  // null ‚Üí undefined
    userAgent: authSession.userAgent ?? undefined,  // null ‚Üí undefined
    deviceId: authSession.deviceId
  }
}
```

**ElectroDB ‚Üí Better Auth**:
```typescript
function transformSessionToAuth(electroSession: Partial<ElectroSessionItem>): Session {
  return {
    id: electroSession.sessionId!,
    userId: electroSession.userId!,
    expiresAt: new Date(electroSession.expiresAt!),
    token: electroSession.token!,
    ipAddress: electroSession.ipAddress ?? undefined,
    userAgent: electroSession.userAgent ?? undefined,
    createdAt: new Date(electroSession.createdAt ?? Date.now()),
    updatedAt: new Date(electroSession.updatedAt ?? Date.now())
  }
}
```

### Account Transformations

**Better Auth ‚Üí ElectroDB**:
```typescript
function transformAccountFromAuth(authAccount: Partial<ExtendedAccount> & {id?: string}): ElectroAccountCreate {
  return {
    accountId: authAccount.id || uuidv4(),
    userId: authAccount.userId!,
    providerId: authAccount.providerId!,
    providerAccountId: authAccount.accountId || '',  // Field name difference!
    accessToken: authAccount.accessToken ?? undefined,
    refreshToken: authAccount.refreshToken ?? undefined,
    expiresAt: authAccount.expiresAt ?? undefined,
    scope: authAccount.scope ?? undefined,
    tokenType: authAccount.tokenType ?? undefined,
    idToken: authAccount.idToken ?? undefined
  }
}
```

**ElectroDB ‚Üí Better Auth**:
```typescript
function transformAccountToAuth(electroAccount: ElectroAccountItem): ExtendedAccount {
  return {
    id: electroAccount.accountId,
    userId: electroAccount.userId,
    accountId: electroAccount.providerAccountId,  // Field name difference!
    providerId: electroAccount.providerId,
    accessToken: electroAccount.accessToken ?? null,
    refreshToken: electroAccount.refreshToken ?? null,
    idToken: electroAccount.idToken ?? null,
    scope: electroAccount.scope ?? null,
    tokenType: electroAccount.tokenType ?? null,
    expiresAt: electroAccount.expiresAt ?? null,
    createdAt: new Date(electroAccount.createdAt),
    updatedAt: new Date(electroAccount.updatedAt)
  }
}
```

### Update Operations

Updates use partial types to only send changed fields:

```typescript
type ElectroUserUpdate = Partial<Pick<ElectroUserCreate, 'email' | 'emailVerified' | 'firstName' | 'lastName'>>

function transformUserUpdateFromAuth(authUpdate: Partial<User>): ElectroUserUpdate {
  const updates: ElectroUserUpdate = {}

  if (authUpdate.email) updates.email = authUpdate.email
  if (authUpdate.emailVerified !== undefined) updates.emailVerified = authUpdate.emailVerified
  if (authUpdate.name) {
    const {firstName, lastName} = splitFullName(authUpdate.name)
    updates.firstName = firstName
    updates.lastName = lastName
  }

  return updates
}

async updateUser(userId: string, data: Partial<User>): Promise<User> {
  const updates = transformUserUpdateFromAuth(data)
  const result = await Users.update({userId}).set(updates).go()
  return transformUserToAuth(result.data)
}
```

## Query Optimization

### Email Lookup

Original implementation (slow):
```typescript
// ‚ùå Full table scan
const result = await Users.scan
  .where(({email: emailAttr}, {eq}) => eq(emailAttr, email))
  .go()
```

Optimized implementation (fast):
```typescript
// ‚úÖ Indexed query via gsi3
const result = await Users.query.byEmail({email}).go()
```

**Performance Improvement**: 10-100x faster depending on table size

### Session Queries

Efficiently query all sessions for a user:

```typescript
// Uses byUser index (gsi1)
const sessions = await Sessions.query.byUser({userId}).go()

// Sorted by expiresAt (composite sort key)
const activeSessions = sessions.data.filter(s => s.expiresAt > Date.now())
```

### Provider Account Lookup

Find account by OAuth provider:

```typescript
// Uses byProvider index (gsi2)
const account = await Accounts.query
  .byProvider({providerId: 'apple', providerAccountId: 'user123'})
  .go()
```

## Error Handling

### Graceful Degradation

All get operations return `null` instead of throwing:

```typescript
async getUser(userId: string): Promise<User | null> {
  try {
    const result = await Users.get({userId}).go()
    if (!result.data) return null
    return transformUserToAuth(result.data)
  } catch (error) {
    logError('ElectroDB Adapter: getUser failed', {userId, error})
    return null
  }
}
```

### Logging Strategy

All adapter operations log at DEBUG level:

```typescript
logDebug('ElectroDB Adapter: createUser', {data})
```

Errors log with context:

```typescript
logError('ElectroDB Adapter: getUserByEmail failed', {email, error})
```

## Extended Account Type

Better Auth's base `Account` type doesn't include OAuth metadata we persist:

```typescript
type ExtendedAccount = Account & {
  scope?: string | null
  tokenType?: string | null
  expiresAt?: number | null
}
```

This allows storing full OAuth token metadata while remaining compatible with Better Auth's interface.

## Link Account Operation

ElectroDB handles account linking implicitly via the userId composite key:

```typescript
async linkAccount(userId: string, accountId: string): Promise<void> {
  logDebug('ElectroDB Adapter: linkAccount', {userId, accountId})

  // ElectroDB entities already link via userId composite key
  // No additional operation needed - account is already linked via createAccount
}
```

## Testing Strategy

### Unit Testing

The adapter has comprehensive unit tests (`electrodb-adapter.test.ts`):

```typescript
describe('ElectroDB Adapter', () => {
  it('should create a user', async () => {
    const mockUser = {id: 'user-123', email: 'test@example.com', name: 'John Doe'}
    const result = await adapter.createUser(mockUser)

    expect(Users.create).toHaveBeenCalledWith({
      userId: 'user-123',
      email: 'test@example.com',
      emailVerified: false,
      firstName: 'John',
      lastName: 'Doe',
      identityProviders: {...}
    })
  })
})
```

### Integration Testing

LocalStack tests validate full round-trip operations:

```typescript
it('should create and retrieve user via email', async () => {
  const user = await adapter.createUser({email: 'test@example.com', name: 'John Doe'})
  const retrieved = await adapter.getUserByEmail('test@example.com')

  expect(retrieved).toEqual(user)
})
```

## Best Practices

### 1. Always Transform

Never pass data directly between layers:

```typescript
// ‚ùå Wrong - skips transformation
const result = await Users.create(authUser)

// ‚úÖ Correct - transforms types
const electroData = transformUserFromAuth(authUser)
const result = await Users.create(electroData)
return transformUserToAuth(result.data)
```

### 2. Handle Null vs Undefined

ElectroDB prefers `undefined`, Better Auth uses `null`:

```typescript
// ‚ùå Wrong - ElectroDB doesn't like null
{ipAddress: null}

// ‚úÖ Correct - Convert null to undefined
{ipAddress: authSession.ipAddress ?? undefined}
```

### 3. Generate IDs When Missing

Better Auth sometimes omits IDs, expecting the adapter to generate them:

```typescript
userId: authUser.id || uuidv4()
sessionId: authSession.id || uuidv4()
```

### 4. Validate Required Fields

Use TypeScript's `!` for fields required by Better Auth:

```typescript
email: authUser.email!,  // Better Auth guarantees this exists
userId: authSession.userId!  // Required field
```

## Performance Metrics

Measured improvements from adapter optimizations:

| Operation | Before | After | Improvement |
|-----------|--------|-------|-------------|
| getUserByEmail | 200-500ms (scan) | 10-50ms (index) | **10-50x faster** |
| createUser + Session | N/A | 50-100ms | New capability |
| getSession + User | 100-200ms | 50-100ms | **2x faster** (single table) |

## Publishing as npm Package

### Package Structure

```
better-auth-electrodb-adapter/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ adapter.ts          # Main adapter code
‚îÇ   ‚îú‚îÄ‚îÄ transformers.ts     # Transformation functions
‚îÇ   ‚îî‚îÄ‚îÄ types.ts            # TypeScript types
‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îî‚îÄ‚îÄ adapter.test.ts     # Unit tests
‚îú‚îÄ‚îÄ README.md               # Documentation
‚îú‚îÄ‚îÄ package.json            # Package metadata
‚îî‚îÄ‚îÄ tsconfig.json           # TypeScript config
```

### Usage Example

```typescript
import {betterAuth} from 'better-auth'
import {createElectroDBAdapter} from '@your-org/better-auth-electrodb-adapter'
import {Users, Sessions, Accounts, VerificationTokens} from './entities'

export const auth = betterAuth({
  database: createElectroDBAdapter({
    entities: {Users, Sessions, Accounts, VerificationTokens}
  })
})
```

### Documentation Requirements

For publication, include:
- Installation instructions
- Entity schema requirements
- GSI configuration guide
- Type definition examples
- Migration guide from other adapters
- Performance tuning tips

## Future Enhancements

### Potential Improvements

1. **Batch Operations**: Support Better Auth batch operations
2. **Caching Layer**: Add optional caching for frequently accessed data
3. **Metrics**: Built-in CloudWatch metrics for adapter operations
4. **Connection Pooling**: Optimize DynamoDB connection reuse
5. **Multi-Region**: Support DynamoDB global tables

### Community Contributions

Areas where the community could contribute:
- Additional OAuth providers
- Performance benchmarks
- Migration tools from other adapters
- Documentation improvements
- Example projects

## References

- [Better Auth Adapter Interface](https://www.better-auth.com/docs/adapters)
- [ElectroDB Documentation](https://electrodb.dev/)
- [DynamoDB Best Practices](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html)
- [Better Auth Architecture](Better-Auth-Architecture.md)
- [ElectroDB Testing Patterns](../Testing/ElectroDB-Testing-Patterns.md)
</file>

<file path="docs/wiki/AWS/X-Ray-Integration.md">
# X-Ray Integration

## Quick Reference
- **When to use**: All Lambda functions for distributed tracing
- **Enforcement**: Recommended
- **Impact if violated**: LOW - Reduced visibility

## X-Ray Decorator Pattern

```typescript
// lib/vendor/AWS/XRay.ts
import AWSXRay from 'aws-xray-sdk-core'

export function withXRay<TEvent = any, TResult = any>(
  handler: (event: TEvent, context: Context, metadata: {traceId: string}) => Promise<TResult>
) {
  return async (event: TEvent, context: Context): Promise<TResult> => {
    const segment = AWSXRay.getSegment()
    const traceId = (segment as any)?.trace_id || context.awsRequestId
    return handler(event, context, {traceId})
  }
}

export function captureAWSClient<T>(client: T): T {
  if (process.env.ENABLE_XRAY === 'false' || process.env.USE_LOCALSTACK === 'true') {
    return client
  }
  return AWSXRay.captureAWSv3Client(client)
}
```

## Lambda Usage

```typescript
// All Lambda handlers use withXRay
export const handler = withXRay(async (event, context, {traceId}) => {
  logInfo('event <=', event)
  // traceId available for correlation
  return response(context, 200, data)
})
```

## AWS SDK Integration

```typescript
// lib/vendor/AWS/S3.ts
import {S3Client} from '@aws-sdk/client-s3'
import {captureAWSClient} from './XRay'

const s3Client = captureAWSClient(new S3Client())
```

## Custom Subsegments

```typescript
import {captureAsyncFunc} from '../AWS/XRay'

export async function downloadVideo(url: string) {
  return captureAsyncFunc('downloadVideo', async (subsegment) => {
    subsegment?.addAnnotation('url', url)
    subsegment?.addMetadata('size', result.size)
    // Perform operation
    return result
  })
}
```

## Annotations vs Metadata

- **Annotations** - Indexed, searchable (userId, operation)
- **Metadata** - Detailed info, not searchable (request body, response)

## OpenTofu Configuration

```hcl
resource "aws_lambda_function" "process_file" {
  tracing_config {
    mode = "Active"
  }

  environment {
    variables = {
      EnableXRay = "true"
    }
  }
}

resource "aws_iam_role_policy_attachment" "xray" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess"
}
```

## Testing

```typescript
// Disable X-Ray in tests
beforeEach(() => {
  process.env.EnableXRay = 'false'
})

// Mock X-Ray
jest.unstable_mockModule('../../../lib/vendor/AWS/XRay', () => ({
  withXRay: (handler: any) => handler,
  captureAWSClient: (client: any) => client,
  captureAsyncFunc: async (name: string, fn: any) => fn()
}))
```

## Best Practices

‚úÖ Use withXRay decorator for all Lambdas
‚úÖ Wrap AWS SDK clients with captureAWSClient
‚úÖ Create subsegments for slow operations
‚úÖ Include trace ID in logs
‚úÖ Use annotations for searchable fields
‚úÖ Disable X-Ray in tests

## Related Patterns

- [CloudWatch Logging](CloudWatch-Logging.md)
- [Lambda Function Patterns](../TypeScript/Lambda-Function-Patterns.md)

---

*Use X-Ray for distributed tracing. The withXRay decorator provides automatic instrumentation.*
</file>

<file path="docs/wiki/Bash/Bash-Error-Handling.md">
# Bash Error Handling

## Quick Reference
- **When to use**: All bash scripts
- **Enforcement**: Required
- **Impact if violated**: CRITICAL - Silent failures

## Standard Setup

```bash
#!/usr/bin/env bash
set -euo pipefail

# Error handler
error() {
  echo "‚ùå Error: $1" >&2
  exit "${2:-1}"
}

# Cleanup trap
cleanup() {
  rm -f "$temp_file"
}
trap cleanup EXIT ERR
```

## Error Handling Patterns

### Expected Failures
```bash
# Use || true for optional commands
rm file.txt 2>/dev/null || true

# Conditional handling
if ! command_that_might_fail; then
  echo "Failed, trying alternative"
  alternative_command || error "Both failed"
fi
```

### Validation
```bash
# Check variables
[[ -z "${VAR:-}" ]] && error "VAR is required"

# Check files
[[ -f "$file" ]] || error "File not found: $file"

# Check commands
command -v aws >/dev/null || error "AWS CLI not installed"
```

### AWS CLI Errors
```bash
# Capture and check
if output=$(aws lambda invoke --function test 2>&1); then
  echo "‚úÖ Success"
else
  error "AWS failed: $output"
fi
```

## Best Practices

‚úÖ Always use `set -euo pipefail`
‚úÖ Provide context in errors
‚úÖ Clean up with traps
‚úÖ Use stderr for errors: `>&2`
‚úÖ Check dependencies first

## Exit Codes

| Code | Meaning |
|------|---------|
| 0 | Success |
| 1 | General error |
| 2 | Missing dependency |
| 3 | AWS operation failed |

## Related Patterns

- [Script Patterns](Script-Patterns.md)
- [Directory Resolution](Directory-Resolution.md)

---

*Use strict error handling. Fail fast with clear messages.*
</file>

<file path="docs/wiki/Bash/Directory-Resolution.md">
# Directory Resolution

## Quick Reference
- **When to use**: All Bash scripts that need to know their location
- **Enforcement**: Required - ensures scripts work from any directory
- **Impact if violated**: HIGH - Scripts fail when run from different locations

## The Rule

Use this standard pattern at the top of every script:

```bash
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
```

## Examples

### ‚úÖ Correct - Standard Pattern

```bash
#!/usr/bin/env bash

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"

# Now we can reliably reference project files
SOURCE_DIR="${PROJECT_ROOT}/src"
BUILD_DIR="${PROJECT_ROOT}/build"

cd "${PROJECT_ROOT}"
npm run build
```

### ‚ùå Incorrect - Using $0 or Relative Paths

```bash
# ‚ùå WRONG - Fails when script is sourced
script_dir="$(cd "$(dirname "$0")" && pwd)"

# ‚ùå WRONG - Breaks when run from different directory
cd ../src
npm run build

# ‚úÖ CORRECT
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
cd "${PROJECT_ROOT}/src"
```

## Why This Pattern Works

### BASH_SOURCE[0] vs $0

- `$0` fails when script is sourced (returns shell name)
- `$0` doesn't resolve symlinks properly
- `BASH_SOURCE[0]` works in all contexts (direct, sourced, symlinked)

### Quotes Handle Spaces

```bash
# ‚ùå WRONG - Breaks with spaces in path
SCRIPT_DIR=$(cd $(dirname ${BASH_SOURCE[0]}) && pwd)

# ‚úÖ CORRECT - Handles spaces
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
```

## Common Use Cases

### Scripts in bin/

```bash
#!/usr/bin/env bash
# bin/deploy.sh

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"

SRC_DIR="${PROJECT_ROOT}/src"
TERRAFORM_DIR="${PROJECT_ROOT}/terraform"
```

### Loading Configuration

```bash
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"

# Load config from same directory as script
if [[ -f "${SCRIPT_DIR}/config.sh" ]]; then
    source "${SCRIPT_DIR}/config.sh"
fi

# Load from project root
if [[ -f "${PROJECT_ROOT}/.env" ]]; then
    source "${PROJECT_ROOT}/.env"
fi
```

### Sourcing Utilities

```bash
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

source "${SCRIPT_DIR}/../lib/colors.sh"
source "${SCRIPT_DIR}/../lib/aws-helpers.sh"
```

## Template for New Scripts

```bash
#!/usr/bin/env bash

set -e  # Exit on error

# Directory resolution
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"

# Constants
RED='\033[0;31m'
GREEN='\033[0;32m'
NC='\033[0m'

# Main script logic
echo "Project root: ${PROJECT_ROOT}"
```

## Enforcement

### Code Review Checklist

- [ ] All scripts use BASH_SOURCE[0]
- [ ] Directory resolution at top of script
- [ ] All paths quoted to handle spaces
- [ ] No relative paths without resolution

## Related Patterns

- [Variable Naming](Variable-Naming.md) - UPPER_CASE for path constants
- [Script Patterns](Script-Patterns.md) - Overall script structure

---

*Always use BASH_SOURCE[0] with cd and pwd for directory resolution. This ensures scripts work correctly regardless of how they're invoked or where they're run from.*
</file>

<file path="docs/wiki/Bash/Variable-Naming.md">
# Variable Naming

## Quick Reference
- **When to use**: All Bash script variables
- **Enforcement**: Required - maintain consistency
- **Impact if violated**: MEDIUM - Confusion about variable scope and mutability

## The Rules

1. **snake_case** for regular variables (mutable, local)
2. **UPPER_CASE** for constants (immutable, configuration, paths)
3. **Descriptive names** that indicate purpose
4. **Avoid single letters** except loop counters

## Examples

### ‚úÖ Correct

```bash
#!/usr/bin/env bash

# Constants - UPPER_CASE
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
BUILD_DIR="${PROJECT_ROOT}/build"
MAX_RETRIES=3
RED='\033[0;31m'
GREEN='\033[0;32m'

# Environment variables - UPPER_CASE
AWS_PROFILE=${AWS_PROFILE:-default}
AWS_REGION=${AWS_REGION:-us-west-2}

# Regular variables - snake_case
file_name="data.json"
temp_dir=$(mktemp -d)
api_response=$(curl -s "https://api.example.com/data")
user_count=0

# Function with clear parameters
function deploy_lambda() {
    local function_name=$1
    local zip_file_path=$2

    echo "Deploying ${function_name} from ${zip_file_path}"
}
```

### ‚ùå Incorrect

```bash
# ‚ùå WRONG - Mixed casing
fileName="data.json"        # Should be file_name
TempDir=$(mktemp -d)        # Should be temp_dir
script_dir="$(cd ...)"      # Should be SCRIPT_DIR
max_retries=3               # Should be MAX_RETRIES

# ‚ùå WRONG - UPPER_CASE for mutable
FILE_COUNT=0
for file in *.txt; do
    ((FILE_COUNT++))        # Should be snake_case
done

# ‚ùå WRONG - Non-descriptive
f="data.json"               # What is f?
d=$(mktemp -d)              # What is d?
```

## Common Patterns

### Constants with readonly

```bash
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
readonly MAX_RETRIES=3
```

### Loop Variables

```bash
# Simple counters - single letter OK
for i in {1..10}; do
    echo "Iteration ${i}"
done

# Named variables - descriptive
for file_path in "${BUILD_DIR}"/*.zip; do
    echo "Processing ${file_path}"
done
```

### Boolean Flags

```bash
# Regular flags - snake_case
dry_run=false
verbose=false

# Constant flags - UPPER_CASE
DRY_RUN=${DRY_RUN:-false}
DEBUG=${DEBUG:-false}
```

## Enforcement

### Code Review Checklist

- [ ] Regular variables use snake_case
- [ ] Constants use UPPER_CASE
- [ ] Variable names are descriptive
- [ ] No single-letter names (except counters)

## Related Patterns

- [Script Patterns](Script-Patterns.md) - Overall script structure
- [Directory Resolution](Directory-Resolution.md) - BASH_SOURCE pattern

---

*Use snake_case for regular variables and UPPER_CASE for constants. Choose descriptive names that clearly indicate purpose.*
</file>

<file path="docs/wiki/Conventions/Code-Formatting.md">
# Code Formatting

## Quick Reference
- **When to use**: Understanding and overriding automatic code formatting
- **Formatter**: dprint (TypeScript plugin)
- **Config file**: `dprint.json`
- **Enforcement**: Automatic - `pnpm run format`

## The Rule

**Let the formatter handle formatting, with targeted overrides when readability suffers.**

This project uses [dprint](https://dprint.dev/) for code formatting. The formatter makes most decisions automatically based on line width (157 characters). Use the techniques below only when the automatic formatting harms readability.

## Key Configuration

| Setting | Value | Effect |
|---------|-------|--------|
| `lineWidth` | 157 | Maximum line length before wrapping |
| `semiColons` | "asi" | No semicolons (automatic semicolon insertion) |
| `quoteStyle` | "preferSingle" | Single quotes for strings |
| `trailingCommas` | "never" | No trailing commas |
| `objectExpression.preferSingleLine` | true | Objects on one line when possible |
| `arrayExpression.preferSingleLine` | false | Arrays expand to multiline |
| `arguments.preferHanging` | "always" | First argument on same line as function |

## Multiline Formatting Hint

### The Problem

When dprint wraps arrays/objects that exceed line width, it uses a "best fit" algorithm that can create ugly mixed inline/multiline formatting:

```typescript
// dprint's default - first elements inline, then wrap
const items = [{id: 1, name: 'first'}, {id: 2, name: 'second'}, {
  id: 3,
  name: 'third'
}]
```

### The Solution: `// fmt: multiline`

Add `// fmt: multiline` after the first element to force consistent multiline formatting:

```typescript
// Clean, consistent multiline
const items = [
  {id: 1, name: 'first'}, // fmt: multiline
  {id: 2, name: 'second'},
  {id: 3, name: 'third'}
]
```

### Why It Works

Line comments (`//`) cannot be collapsed to a single line. dprint must keep the line break, which triggers full multiline formatting for the entire expression.

### When to Use

Use `// fmt: multiline` when:
- An array/object wraps with some elements inline and others multiline
- The mixed formatting harms readability
- Elements are similar and should be visually aligned

Do NOT use when:
- The expression fits on one line (let dprint keep it compact)
- The multiline format is already clean
- Adding the comment would be the only thing preventing single-line

### Examples

#### Arrays of Similar Objects

```typescript
// Test fixtures with consistent structure
const mockDevices = [
  {deviceId: 'device-1', userId: 'user-123'}, // fmt: multiline
  {deviceId: 'device-2', userId: 'user-456'},
  {deviceId: 'device-3', userId: 'user-789'}
]

// Metrics with consistent structure
await putMetrics([
  {name: 'Success', value: 1, unit: 'Count'}, // fmt: multiline
  {name: 'Duration', value: duration, unit: 'Seconds'},
  {name: 'FileSize', value: fileSize, unit: 'Bytes'}
])
```

#### Configuration Arrays

```typescript
// DynamoDB indexes
indexes: [
  {name: 'Primary', pk: 'pk', sk: 'sk'}, // fmt: multiline
  {name: 'GSI1', pk: 'gsi1pk', sk: 'gsi1sk'},
  {name: 'GSI2', pk: 'gsi2pk', sk: 'gsi2sk'}
]
```

## Function Call Formatting

### Hanging Arguments

With `arguments.preferHanging: "always"`, when function arguments must wrap, the first argument stays on the same line as the function name:

```typescript
// Function name and first argument together (good context)
jest.unstable_mockModule('#lib/vendor/AWS/SNS',
  () => ({deleteEndpoint: jest.fn(), subscribe: jest.fn()}))

// NOT like this (loses context)
jest.unstable_mockModule(
  '#lib/vendor/AWS/SNS',
  () => ({deleteEndpoint: jest.fn(), subscribe: jest.fn()})
)
```

This keeps the function name and its primary argument visually connected.

## Complete Formatting Bypass

### `// dprint-ignore`

Skip formatting for a single statement:

```typescript
// dprint-ignore
const matrix = [
  [1, 0, 0],
  [0, 1, 0],
  [0, 0, 1]
];
```

### `// dprint-ignore-file`

Skip formatting for an entire file (use sparingly):

```typescript
// dprint-ignore-file

// ... rest of file is not formatted
```

### When to Use Ignore

- **Matrix/grid data** - Visual alignment matters
- **Complex regex** - Manual formatting aids readability
- **Generated code** - Preserve generator's formatting
- **Temporary debugging** - Will be removed soon

## Type Aliases for Line Width Management

### The Problem

Function signatures with multiple parameters, long return types, or complex generics can exceed the line width limit, causing dprint to wrap them across multiple lines:

```typescript
// Wrapped due to length - loses visual clarity
export async function createUserSession(
  userId: string,
  deviceId?: string,
  ipAddress?: string,
  userAgent?: string
): Promise<{token: string; expiresAt: number; sessionId: string}> {
```

### The Solution: Extract Type Aliases

Create type aliases for return types or parameter groups to keep signatures under the line width:

```typescript
// Type alias keeps the signature on one line
type SessionResult = {token: string; expiresAt: number; sessionId: string}

export async function createUserSession(userId: string, deviceId?: string, ipAddress?: string, userAgent?: string): Promise<SessionResult> {
```

### Guidelines

**When to create type aliases:**
- Function signature exceeds 157 characters
- Return type is complex (multiple properties)
- Generic type parameter is verbose
- Multiple functions share the same type

**When NOT to use type aliases:**
- Type is used only once and is self-explanatory
- Alias would obscure the actual type
- Signature already fits on one line

### Examples

#### Return Type Extraction

```typescript
// Before: 175 characters, wraps
export async function getResourceDetails(id: string): Promise<{userId: string; fileId: string; metadata: Record<string, unknown>}> {

// After: 89 characters, stays on one line
type ResourceDetails = {userId: string; fileId: string; metadata: Record<string, unknown>}

export async function getResourceDetails(id: string): Promise<ResourceDetails> {
```

#### Parameter Type Extraction

```typescript
// Before: Parameters cause wrapping
export function validateRequest(
  requestBody: Webhook | DeviceRegistrationRequest | UserRegistration | UserSubscribe | UserLogin,
  schema: Joi.ObjectSchema
): void {

// After: Union type extracted
type RequestPayload = Webhook | DeviceRegistrationRequest | UserRegistration | UserSubscribe | UserLogin

export function validateRequest(requestBody: RequestPayload, schema: Joi.ObjectSchema): void {
```

#### Generic Type Simplification

```typescript
// Before: Long generic wraps
export function captureAWSClient<T extends {
  middlewareStack: {remove: unknown; use: unknown};
  config: unknown
}>(client: T): T {

// After: Collapsed to one line (111 chars, fits)
export function captureAWSClient<T extends {middlewareStack: {remove: unknown; use: unknown}; config: unknown}>(client: T): T {
```

### Naming Conventions for Type Aliases

- **Result types**: `[Function]Result` - `SessionResult`, `ValidationResult`
- **Input types**: `[Function]Input` or `[Entity]Payload` - `RequestPayload`, `CreateUserInput`
- **Configuration types**: `[Feature]Config` - `AuthConfig`, `CacheConfig`

### Trade-offs

| Type Alias | Inline Type |
|------------|-------------|
| Reusable | Single use |
| Named (self-documenting) | Immediately visible |
| Shorter signatures | All info in one place |
| Requires navigation to understand | No navigation needed |

**Prefer type aliases when:**
- Type is complex (3+ properties)
- Type is reused
- Signature would exceed line width

**Prefer inline types when:**
- Type is simple (1-2 properties)
- Type is used once
- Signature fits comfortably

## Sequential Mock Return Values

### The Problem

When configuring multiple return values with `mockResolvedValueOnce`, method chaining can exceed line width and wrap awkwardly:

```typescript
// Chained - dprint wraps mid-chain (ugly, inconsistent)
mockOperation.mockResolvedValueOnce({data: ['page1'], cursor: 'cursor1'}).mockResolvedValueOnce({
  data: ['page2'],
  cursor: 'cursor2'
}).mockResolvedValueOnce({data: ['page3'], cursor: null})
```

### The Solution: Separate Statements

Use separate statements instead of chaining. `mockResolvedValueOnce` queues return values internally‚Äîchaining is syntactic sugar, not required:

```typescript
// Separate statements - clean, consistent, dprint-stable
mockOperation.mockResolvedValueOnce({data: ['page1'], cursor: 'cursor1'})
mockOperation.mockResolvedValueOnce({data: ['page2'], cursor: 'cursor2'})
mockOperation.mockResolvedValueOnce({data: ['page3'], cursor: null})
```

### Why It Works

1. **Readability** - Each return value on its own line, clear sequence
2. **dprint stability** - Separate statements won't collapse or wrap mid-chain
3. **Consistent** - Same visual structure regardless of content length

### Pattern

```typescript
// Type alias for the mock function signature
type ScanFn<T> = (cursor?: string) => Promise<{data: T[]; cursor: string | null}>

it('should paginate through multiple pages', async () => {
  // Declare mock with type
  const mockScan = jest.fn<ScanFn<string>>()

  // Configure sequential returns as separate statements
  mockScan.mockResolvedValueOnce({data: ['item1', 'item2'], cursor: 'cursor1'})
  mockScan.mockResolvedValueOnce({data: ['item3', 'item4'], cursor: 'cursor2'})
  mockScan.mockResolvedValueOnce({data: ['item5'], cursor: null})

  const result = await scanAllPages(mockScan)
  // ...assertions
})
```

### When to Apply

- **Always** for `mockResolvedValueOnce` sequences (2+ calls)
- **Always** for `mockReturnValueOnce` sequences (2+ calls)
- Single `mockResolvedValue` or `mockReturnValue` can stay on same line as mock declaration

## Running the Formatter

```bash
# Format all files
pnpm run format

# Check formatting without changing (CI)
pnpm run format:check

# Format specific file
npx dprint fmt path/to/file.ts
```

## Finding Format Issues

```bash
# Find potential // fmt: multiline candidates
# (arrays starting with element on same line as [, then wrapping)
grep -rn "= \[{.*}, {$" --include="*.ts" src/

# Find existing format hints
grep -rn "fmt: multiline" --include="*.ts" src/
```

## Rationale

1. **Consistency over preference** - Automated formatting eliminates style debates
2. **Targeted overrides** - Only intervene when readability suffers
3. **Self-documenting** - `// fmt: multiline` explains its purpose
4. **Searchable** - Easy to find all format hints in codebase

## Related Patterns

- [Code Comments](Code-Comments.md) - When and how to use comments
- [Naming Conventions](Naming-Conventions.md) - Variable and file naming
- [Import Organization](Import-Organization.md) - Import ordering

---

*Trust the formatter for most decisions. Use `// fmt: multiline` sparingly for arrays/objects where the automatic formatting creates inconsistent visual structure.*
</file>

<file path="docs/wiki/Conventions/ESLint-vs-MCP-Validation.md">
# ESLint vs MCP Validation: Comprehensive Analysis

## Executive Summary

This project uses two validation systems with complementary strengths:
- **ESLint**: Real-time, in-editor feedback with limited context
- **MCP Validation**: Deep analysis with full project context

**Recommendation**: Don't replicate all rules. Use ESLint for high-frequency, simple patterns; reserve MCP for complex, cross-file analysis.

---

## System Comparison

| Aspect | ESLint | MCP Validation |
|--------|--------|----------------|
| **Execution** | Real-time in editor, CI lint step | On-demand via MCP queries, CI validation |
| **Parser** | espree/typescript-eslint AST | ts-morph (full TypeScript AST + type info) |
| **Scope** | Single file at a time | Project-wide, cross-file analysis |
| **Type Information** | Limited (requires type-aware config) | Full TypeScript type inference |
| **Auto-fix** | Supported | Not supported |
| **Developer UX** | Immediate, familiar | Query-based, requires intention |
| **Performance** | Must be fast (editor responsiveness) | Can be slower (batch analysis) |

---

## Rule Portability Analysis

### CRITICAL Rules (5)

| MCP Rule | ESLint Portable? | Implemented? | Notes |
|----------|------------------|--------------|-------|
| `aws-sdk-encapsulation` | ‚úÖ Yes | ‚úÖ Done | Simple import pattern matching |
| `electrodb-mocking` | ‚úÖ Yes | ‚úÖ Done | Jest mock pattern detection |
| `cascade-safety` | ‚ö†Ô∏è Partial | ‚úÖ Partial | Promise.all detection works; entity hierarchy analysis not portable |
| `config-enforcement` | ‚ùå No | ‚Äî | Checks ESLint/TSConfig itself; circular dependency |
| `env-validation` | ‚ö†Ô∏è Partial | ‚úÖ Done | Detects `process.env.X` in Lambda/util files |

### HIGH Rules (5)

| MCP Rule | ESLint Portable? | Implemented? | Notes |
|----------|------------------|--------------|-------|
| `response-helpers` | ‚úÖ Yes | ‚úÖ Done | Detects raw `{statusCode, body}` returns |
| `types-location` | ‚ö†Ô∏è Partial | ‚Äî | Import checks possible; file path logic complex |
| `batch-retry` | ‚ö†Ô∏è Partial | ‚Äî | Can detect `batchWrite`; retry pattern harder |
| `scan-pagination` | ‚ö†Ô∏è Partial | ‚Äî | Can detect `scan()`; pagination check complex |
| `doc-sync` | ‚ùå No | ‚Äî | Requires markdown + code + filesystem analysis |

### MEDIUM Rules (3)

| MCP Rule | ESLint Portable? | Implemented? | Notes |
|----------|------------------|--------------|-------|
| `import-order` | ‚úÖ Yes | ‚Äî | eslint-plugin-import does this already |
| `response-enum` | ‚úÖ Yes | ‚Äî | Magic number detection; straightforward |
| `mock-formatting` | ‚úÖ Yes | ‚Äî | Chained mock pattern detection |

---

## Recommendation: Selective Replication

### Rules That SHOULD Be ESLint Rules

These provide significant value as real-time feedback:

1. **aws-sdk-encapsulation** ‚úÖ (Done)
   - High-frequency mistake
   - Simple pattern matching
   - Immediate feedback prevents wrong imports

2. **electrodb-mocking** ‚úÖ (Done)
   - Catches test anti-patterns early
   - Simple jest.mock pattern detection

3. **cascade-safety** ‚úÖ (Done, partial)
   - Promise.all with deletes is common mistake
   - Entity hierarchy check stays in MCP

4. **response-helpers** ‚úÖ (Done)
   - Catches raw response objects
   - Clear fix: use `response()` helper
   - High developer friction without it

5. **env-validation** ‚úÖ (Done)
   - `process.env.X` is easy to detect
   - Immediate feedback on missing validation

6. **response-enum** (Optional)
   - Magic status codes are easy to spot
   - Suggests `ResponseStatus` enum

### Rules That SHOULD NOT Be ESLint Rules

These require capabilities ESLint doesn't have:

1. **config-enforcement**
   - Checks ESLint config itself (circular)
   - Cross-file config analysis
   - Better as MCP + CI validation

2. **doc-sync**
   - Requires markdown parsing
   - Cross-references code + docs + filesystem
   - Project-wide consistency check

3. **types-location** (complex parts)
   - File path analysis beyond imports
   - Project structure awareness

4. **Full cascade-safety**
   - Entity hierarchy analysis
   - Cross-file relationship understanding

---

## Synchronization Strategy

### Option 1: Shared Constants (Recommended)

Extract shared patterns to a common file:

```typescript
// shared/validation-patterns.ts
export const FORBIDDEN_AWS_PACKAGES = [
  '@aws-sdk/client-',
  '@aws-sdk/lib-',
  // ...
]

export const ALLOWED_VENDOR_PATHS = [
  'lib/vendor/AWS',
  'lib/vendor/ElectroDB',
]

export const ENTITY_NAMES = [
  'Users', 'Files', 'Devices', ...
]
```

Both ESLint rules and MCP rules import from this shared file.

**Pros**: Single source of truth for patterns
**Cons**: ESLint rules need CommonJS, MCP uses ESM (requires build step)

### Option 2: Rule Mapping Document (Current)

Maintain a document tracking which rules exist where:

```markdown
| Rule | MCP | ESLint | Sync Status |
|------|-----|--------|-------------|
| aws-sdk-encapsulation | ‚úÖ | ‚úÖ | In sync |
| cascade-safety | ‚úÖ | ‚ö†Ô∏è partial | ESLint covers 60% |
```

**Pros**: Simple, no build complexity
**Cons**: Manual maintenance, can drift

### Option 3: Test-Based Verification

Write tests that feed the same code snippets to both systems:

```typescript
// test/validation-parity.test.ts
describe('ESLint and MCP parity', () => {
  const testCases = [
    {
      code: `import {DynamoDBClient} from '@aws-sdk/client-dynamodb'`,
      filename: 'src/lambdas/Test/src/index.ts',
      expectedViolation: 'aws-sdk-encapsulation'
    }
  ]

  for (const tc of testCases) {
    it(`both catch: ${tc.expectedViolation}`, async () => {
      const eslintResult = await runEslint(tc.code, tc.filename)
      const mcpResult = await runMcpValidation(tc.code, tc.filename)

      expect(eslintResult.hasViolation).toBe(true)
      expect(mcpResult.hasViolation).toBe(true)
    })
  }
})
```

**Pros**: Automated drift detection
**Cons**: Test maintenance overhead

---

## Recommended Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Developer Workflow                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ   Editor     ‚îÇ    ‚îÇ   Pre-commit ‚îÇ    ‚îÇ     CI       ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   (ESLint)   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   (ESLint)   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  (ESLint +   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ   MCP Full)  ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ                    ‚îÇ               ‚îÇ
‚îÇ         ‚ñº                   ‚ñº                    ‚ñº               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ              ESLint Rules (5-7 rules)                ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ aws-sdk-encapsulation  ‚Ä¢ electrodb-mocking        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ cascade-delete-order   ‚Ä¢ response-helpers         ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ env-validation         ‚Ä¢ response-enum            ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                              ‚îÇ                                   ‚îÇ
‚îÇ                              ‚îÇ Catches ~80% of violations        ‚îÇ
‚îÇ                              ‚ñº                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ              MCP Validation (13 rules)               ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ All ESLint rules + deeper analysis                ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ config-enforcement  ‚Ä¢ doc-sync                    ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ types-location      ‚Ä¢ full cascade-safety         ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ                              ‚îÇ                                   ‚îÇ
‚îÇ                              ‚îÇ Catches 100% with full context    ‚îÇ
‚îÇ                              ‚ñº                                   ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ
‚îÇ                    ‚îÇ   PR Check   ‚îÇ                              ‚îÇ
‚îÇ                    ‚îÇ   Passes     ‚îÇ                              ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Implementation Priorities

### Phase 1: CRITICAL (Done)
- ‚úÖ `no-direct-aws-sdk-import`
- ‚úÖ `cascade-delete-order`
- ‚úÖ `use-electrodb-mock-helper`

### Phase 2: HIGH (Done)
- ‚úÖ `response-helpers` - Detects raw `{statusCode, body}` returns
- ‚úÖ `env-validation` - Detects direct `process.env.X` access

### Phase 3: Nice to Have
- `response-enum` - Style enforcement
- `mock-formatting` - Test consistency

### Skip (Keep MCP Only)
- `config-enforcement` - Circular dependency
- `doc-sync` - Requires cross-file analysis
- `import-order` - Use eslint-plugin-import instead

---

## Maintenance Guidelines

### When Adding New MCP Rules

1. **Evaluate ESLint portability** using this checklist:
   - [ ] Single-file analysis sufficient?
   - [ ] No type inference required?
   - [ ] Simple AST pattern matching?
   - [ ] High-frequency developer mistake?

2. **If portable**, create ESLint equivalent:
   - Add to `eslint-local-rules/rules/`
   - Add tests in `eslint-local-rules/test/`
   - Update `eslint.config.mjs`
   - Document in this file

3. **If not portable**, document why:
   - Add to "Skip" section above
   - Explain capabilities needed

### Keeping Rules in Sync

1. **Shared constants**: Update patterns in both places
2. **Monthly review**: Check for drift between implementations
3. **PR template**: "Did you update both ESLint and MCP rules?"

---

## Conclusion

The dual-system approach provides defense in depth:
- **ESLint** catches common mistakes immediately (developer productivity)
- **MCP** catches everything with full context (correctness guarantee)

Don't aim for 100% replication. Instead:
1. Use ESLint for high-frequency, simple patterns
2. Use MCP for complex, cross-file analysis
3. Accept that some rules only exist in one system
4. Document the relationship and maintain intentionally
</file>

<file path="docs/wiki/Infrastructure/Environment-Variables.md">
# Infrastructure Environment Variables

## Quick Reference
- **When to use**: OpenTofu/Terraform Lambda configuration
- **Enforcement**: Required
- **Impact if violated**: HIGH - Deployment failures

## OpenTofu Configuration

Define environment variables in Lambda resources using CamelCase:

```hcl
resource "aws_lambda_function" "ProcessFile" {
  function_name = "ProcessFile"

  environment {
    variables = {
      DynamoDBTableName        = aws_dynamodb_table.main.name
      PlatformApplicationArn   = aws_sns_platform_application.apns.arn
      PushNotificationTopicArn = aws_sns_topic.push_notifications.arn
      FeedlyQueueUrl          = aws_sqs_queue.feedly.url
      EnableXRay              = var.enable_xray ? "true" : "false"
    }
  }
}
```

## Variable Sources

```hcl
# From AWS resources
DynamoDBTableName = aws_dynamodb_table.main.name

# From Terraform variables
EnableXRay = var.enable_xray

# From data sources
ApiToken = data.aws_secretsmanager_secret_version.api_token.secret_string
```

## LocalStack Support

```hcl
variable "use_localstack" {
  default = false
}

environment {
  variables = {
    UseLocalstack = var.use_localstack ? "true" : "false"
    DynamoDBEndpoint = var.use_localstack ? "http://localhost:4566" : null
  }
}
```

## Related Patterns

- [AWS/Lambda-Environment-Variables](../AWS/Lambda-Environment-Variables.md)
- [Resource Naming](Resource-Naming.md)

---

*Configure Lambda environment variables in OpenTofu using CamelCase naming.*
</file>

<file path="docs/wiki/Infrastructure/File-Organization.md">
# Infrastructure File Organization

## Quick Reference
- **When to use**: Organizing OpenTofu/Terraform files
- **Enforcement**: Required
- **Impact if violated**: MEDIUM - Difficult to navigate and maintain

## Directory Structure

```
terraform/
‚îú‚îÄ‚îÄ main.tf                    # Provider configuration
‚îú‚îÄ‚îÄ api_gateway.tf             # API Gateway resources
‚îú‚îÄ‚îÄ api_gateway_authorizer.tf  # Custom authorizer
‚îú‚îÄ‚îÄ configuration_apns.tf      # APNS configuration
‚îú‚îÄ‚îÄ file_bucket.tf             # S3 bucket resources
‚îú‚îÄ‚îÄ file_coordinator.tf        # File coordinator Lambda
‚îú‚îÄ‚îÄ list_files.tf              # List files Lambda
‚îú‚îÄ‚îÄ register_device.tf         # Register device Lambda
‚îú‚îÄ‚îÄ register_user.tf           # Register user Lambda
‚îú‚îÄ‚îÄ feedly_webhook.tf          # Feedly webhook Lambda
‚îú‚îÄ‚îÄ send_push_notification.tf  # Push notification Lambda
‚îî‚îÄ‚îÄ *.tf                       # Other function-specific files
```

## File Organization Rules

1. **One file per Lambda function** - Each Lambda gets its own snake_case file
2. **Include related resources** - Lambda file contains function, role, and policy
3. **Service-specific files** - Shared resources get descriptive snake_case names

## Service Files

```hcl
# file_bucket.tf - S3 bucket and related resources
resource "aws_s3_bucket" "media_files" { }
resource "aws_s3_bucket_versioning" "media_files_versioning" { }

# api_gateway.tf - API Gateway configuration
resource "aws_api_gateway_rest_api" "main" { }
resource "aws_api_gateway_deployment" "main" { }

# configuration_apns.tf - APNS platform configuration
resource "aws_sns_platform_application" "apns" { }
```

## Lambda Files

Each Lambda function gets its own snake_case file with all related resources:

```hcl
# list_files.tf

resource "aws_lambda_function" "ListFiles" {
  function_name = "ListFiles"
  role         = aws_iam_role.ListFilesRole.arn

  environment {
    variables = {
      DynamoDBTableName = aws_dynamodb_table.MediaDownloader.name
    }
  }
}

resource "aws_iam_role" "ListFilesRole" {
  name = "ListFilesRole"
  # Role configuration
}

resource "aws_iam_role_policy" "ListFilesPolicy" {
  name = "ListFilesPolicy"
  role = aws_iam_role.ListFilesRole.id
  # Policy configuration
}
```

## File Naming Convention

- **Lambda files**: snake_case function name
  - `list_files.tf`
  - `register_device.tf`
  - `file_coordinator.tf`
  - `feedly_webhook.tf`

- **Service/Shared files**: snake_case descriptive name
  - `api_gateway.tf`
  - `file_bucket.tf`
  - `configuration_apns.tf`

- **Core files**: lowercase
  - `main.tf`

## Related Patterns

- [Resource Naming](Resource-Naming.md) - Resource naming conventions
- [Environment Variables](Environment-Variables.md) - Lambda configuration

---

*Use snake_case for all terraform files. Each Lambda gets its own file with related IAM resources.*
</file>

<file path="docs/wiki/Infrastructure/Resource-Naming.md">
# Resource Naming

## Quick Reference
- **When to use**: All AWS resources in OpenTofu
- **Enforcement**: Required
- **Impact if violated**: MEDIUM - Inconsistent naming

## Naming Rules

1. **PascalCase for AWS resources**
2. **Match Terraform ID to AWS name**
3. **Include resource type suffix** (Role, Policy, Queue)
4. **Be descriptive and specific**

## Resource Patterns

### Lambda Functions
```hcl
resource "aws_lambda_function" "ProcessFile" {
  function_name = "ProcessFile"
}

resource "aws_lambda_function" "ListFiles" {
  function_name = "ListFiles"
}
```

### IAM Resources
```hcl
resource "aws_iam_role" "ProcessFileRole" {
  name = "ProcessFileRole"
}

resource "aws_iam_role_policy" "ProcessFilePolicy" {
  name = "ProcessFilePolicy"
  role = aws_iam_role.ProcessFileRole.id
}
```

### DynamoDB Tables
```hcl
resource "aws_dynamodb_table" "MediaDownloader" {
  name = "MediaDownloader"
}
```

### S3 Buckets
```hcl
resource "aws_s3_bucket" "MediaFiles" {
  bucket = "media-files-${var.aws_account_id}"
}
```

### SNS/SQS
```hcl
resource "aws_sns_topic" "PushNotifications" {
  name = "PushNotifications"
}

resource "aws_sqs_queue" "FeedlyQueue" {
  name = "FeedlyQueue"
}
```

## Naming Convention Table

| Resource Type | Pattern | Example |
|--------------|---------|---------|
| Lambda | `[Action][Object]` | `ProcessFile` |
| IAM Role | `[Function]Role` | `ProcessFileRole` |
| IAM Policy | `[Function]Policy` | `ProcessFilePolicy` |
| DynamoDB | `[ProjectName]` | `MediaDownloader` |
| S3 Bucket | `[purpose]-${account}` | `media-files-123456` |
| SNS Topic | `[Purpose]` | `PushNotifications` |
| SQS Queue | `[Source]Queue` | `FeedlyQueue` |

## Common Mistakes

```hcl
# ‚ùå Wrong - snake_case
resource "aws_lambda_function" "process_file" {
  function_name = "process_file"
}

# ‚úÖ Correct - PascalCase
resource "aws_lambda_function" "ProcessFile" {
  function_name = "ProcessFile"
}
```

## Best Practices

‚úÖ Use PascalCase consistently
‚úÖ Match Terraform ID to resource name
‚úÖ Include type suffix (Role, Policy, Queue)
‚úÖ Be specific about purpose
‚úÖ Avoid generic names

## Related Patterns

- [File Organization](File-Organization.md)
- [Environment Variables](Environment-Variables.md)
- [OpenTofu Patterns](OpenTofu-Patterns.md)

---

*Use PascalCase for all AWS resources. Match Terraform identifiers to AWS names.*
</file>

<file path="docs/wiki/iOS/Apple-Sign-In-ID-Token-Migration.md">
# Apple Sign In ID Token Migration

## Overview

As of November 2024, the backend API has migrated from Apple's authorization code flow to direct ID token authentication using Better Auth's OAuth capabilities. This eliminates the need for a server-side token exchange and reduces authentication latency by 200-500ms.

## What Changed

### Before (Authorization Code Flow)
```
iOS App ‚Üí Sign in with Apple ‚Üí Authorization Code ‚Üí Backend API
                                                    ‚Üì
                                     Token Exchange with Apple
                                                    ‚Üì
                                              User Session
```

### After (ID Token Flow)
```
iOS App ‚Üí Sign in with Apple ‚Üí ID Token ‚Üí Backend API
                                            ‚Üì
                                  Better Auth Verification
                                            ‚Üì
                                      User Session
```

## API Contract Changes

### RegisterUser Endpoint

**Old Request Format:**
```json
{
  "authorizationCode": "c1234567890abcdef",
  "firstName": "Jonathan",
  "lastName": "Lloyd"
}
```

**New Request Format:**
```json
{
  "idToken": "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...",
  "firstName": "Jonathan",
  "lastName": "Lloyd"
}
```

### LoginUser Endpoint

**Old Request Format:**
```json
{
  "authorizationCode": "c1234567890abcdef"
}
```

**New Request Format:**
```json
{
  "idToken": "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9..."
}
```

## iOS Implementation Guide

### Accessing the ID Token

When using `ASAuthorizationController` for Sign in with Apple, the ID token is available in the authorization credential response:

```swift
func authorizationController(controller: ASAuthorizationController,
                            didCompleteWithAuthorization authorization: ASAuthorization) {
    if let appleIDCredential = authorization.credential as? ASAuthorizationAppleIDCredential {
        // Get the ID token (this is what we need!)
        guard let identityTokenData = appleIDCredential.identityToken,
              let identityToken = String(data: identityTokenData, encoding: .utf8) else {
            print("Unable to fetch identity token")
            return
        }

        // Get user's full name (only available on first sign-in)
        let firstName = appleIDCredential.fullName?.givenName ?? ""
        let lastName = appleIDCredential.fullName?.familyName ?? ""

        // Determine if this is a new user (first sign-in)
        let isNewUser = appleIDCredential.fullName?.givenName != nil

        if isNewUser {
            // Call RegisterUser endpoint
            registerUser(idToken: identityToken,
                        firstName: firstName,
                        lastName: lastName)
        } else {
            // Call LoginUser endpoint
            loginUser(idToken: identityToken)
        }
    }
}
```

### RegisterUser API Call

```swift
func registerUser(idToken: String, firstName: String, lastName: String) {
    let url = URL(string: "\(baseURL)/registerUser")!
    var request = URLRequest(url: url)
    request.httpMethod = "POST"
    request.setValue("application/json", forHTTPHeaderField: "Content-Type")

    let body: [String: Any] = [
        "idToken": idToken,
        "firstName": firstName,
        "lastName": lastName
    ]

    request.httpBody = try? JSONSerialization.data(withJSONObject: body)

    URLSession.shared.dataTask(with: request) { data, response, error in
        guard let data = data, error == nil else {
            print("Registration failed: \(error?.localizedDescription ?? "Unknown error")")
            return
        }

        // Parse response
        if let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
           let body = json["body"] as? [String: Any],
           let token = body["token"] as? String,
           let expiresAt = body["expiresAt"] as? Double,
           let sessionId = body["sessionId"] as? String,
           let userId = body["userId"] as? String {

            // Save session token for authenticated requests
            saveSessionToken(token: token, expiresAt: expiresAt,
                           sessionId: sessionId, userId: userId)
        }
    }.resume()
}
```

### LoginUser API Call

```swift
func loginUser(idToken: String) {
    let url = URL(string: "\(baseURL)/login")!
    var request = URLRequest(url: url)
    request.httpMethod = "POST"
    request.setValue("application/json", forHTTPHeaderField: "Content-Type")

    let body: [String: Any] = [
        "idToken": idToken
    ]

    request.httpBody = try? JSONSerialization.data(withJSONObject: body)

    URLSession.shared.dataTask(with: request) { data, response, error in
        guard let data = data, error == nil else {
            print("Login failed: \(error?.localizedDescription ?? "Unknown error")")
            return
        }

        // Parse response (same format as RegisterUser)
        if let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
           let body = json["body"] as? [String: Any],
           let token = body["token"] as? String,
           let expiresAt = body["expiresAt"] as? Double,
           let sessionId = body["sessionId"] as? String,
           let userId = body["userId"] as? String {

            // Save session token for authenticated requests
            saveSessionToken(token: token, expiresAt: expiresAt,
                           sessionId: sessionId, userId: userId)
        }
    }.resume()
}
```

## Important Notes

### ID Token vs Authorization Code

- **ID Token**: A JWT containing user identity claims (email, sub, etc.). Available immediately after Apple Sign In.
- **Authorization Code**: A one-time code that must be exchanged for tokens via Apple's token endpoint. This is what we used before.

### Name Availability

Apple's privacy design means:
1. `fullName` is ONLY populated on the first sign-in
2. Subsequent sign-ins will have `nil` for `fullName`
3. The ID token does NOT contain first/last name for privacy reasons
4. This is why we send name separately in the RegisterUser request

The backend detects new users (created within last 5 seconds) and updates their name from the iOS app's request.

### Token Security

- ID tokens are short-lived (typically 10 minutes)
- They are signed by Apple using RS256 algorithm
- Better Auth verifies the signature using Apple's public JWKS
- Never cache or store ID tokens - they are single-use for authentication

### Session Management

Both RegisterUser and LoginUser return the same session response format:

```json
{
  "body": {
    "token": "session-token-string",
    "expiresAt": 1234567890000,
    "sessionId": "uuid-session-id",
    "userId": "uuid-user-id"
  }
}
```

Use the `token` value for subsequent authenticated API requests via the `Authorization` header:

```swift
request.setValue("Bearer \(sessionToken)", forHTTPHeaderField: "Authorization")
```

## Migration Checklist

- [ ] Update RegisterUser calls to use `idToken` instead of `authorizationCode`
- [ ] Update LoginUser calls to use `idToken` instead of `authorizationCode`
- [ ] Extract ID token from `ASAuthorizationAppleIDCredential.identityToken`
- [ ] Remove any authorization code handling logic
- [ ] Test new user registration flow
- [ ] Test existing user login flow
- [ ] Verify session token handling remains unchanged
- [ ] Update any error handling for new API contract

## Benefits

1. **Reduced Latency**: Eliminates 200-500ms token exchange round trip to Apple
2. **Simpler Flow**: Direct token verification vs. two-step exchange
3. **Better Auth Integration**: Leverages Better Auth's built-in OAuth capabilities
4. **Same Security**: ID token verification is cryptographically equivalent to authorization code flow

## Troubleshooting

### "Invalid ID token" Error

Check that you're using `identityToken` not `authorizationCode`:
```swift
// Correct
let identityToken = String(data: appleIDCredential.identityToken!, encoding: .utf8)

// Wrong
let authCode = String(data: appleIDCredential.authorizationCode!, encoding: .utf8)
```

### Name Not Saved for New Users

Ensure you're:
1. Checking if `fullName` is populated (indicates new user)
2. Sending `firstName` and `lastName` in RegisterUser request
3. Calling RegisterUser, not LoginUser, for new users

### Session Token Not Working

Verify the response format hasn't changed - both endpoints return identical session structure.

## References

- [Apple Sign In Documentation](https://developer.apple.com/documentation/sign_in_with_apple)
- [ASAuthorizationAppleIDCredential](https://developer.apple.com/documentation/authenticationservices/asauthorizationappleidcredential)
- [Better Auth OAuth Provider](https://www.better-auth.com/docs/authentication/oauth)
</file>

<file path="docs/wiki/Meta/Emerging-Conventions.md">
# Emerging Conventions

## Quick Reference
- **When to use**: Capturing new patterns discovered during development
- **Enforcement**: Ongoing - live append-only log
- **Impact if violated**: LOW - Lost institutional knowledge

## Overview

This is a live, append-only log of conventions discovered during development that haven't yet been promoted to formal wiki pages. Think of it as the "working memory" of the Convention Capture System.

## Format

Each entry follows this structure:

```markdown
## [Date] - [Convention Name]

**Type**: [Rule/Pattern/Methodology/Anti-Pattern]
**Priority**: [Critical/High/Medium/Low]
**Context**: [Where/when this applies]

**What**: [One-sentence description]

**Why**: [Rationale for the convention]

**Example**:
```[language]
// Code example
```

**Status**: [Emerging/Validated/Documented/Deprecated]
**Wiki Page**: [Link when promoted to wiki, or "Pending"]
```

## Current Emerging Conventions

### 2025-11-24 - ElectroDB Mock Helper Pattern

**Type**: Pattern
**Priority**: High
**Context**: Unit testing with ElectroDB entities

**What**: Always use `test/helpers/electrodb-mock.ts` for mocking ElectroDB entities

**Why**: 
- Consistent mock structure across all tests
- Handles ElectroDB's complex entity API
- Reduces test boilerplate
- Easier to update mocks when ElectroDB changes

**Example**:
```typescript
import {createElectroDBMock} from '../../../test/helpers/electrodb-mock'

jest.unstable_mockModule('../../../lib/vendor/ElectroDB/entity', () =>
  createElectroDBMock({
    get: jest.fn().mockResolvedValue({data: mockUser}),
    query: jest.fn().mockResolvedValue({data: [mockUser]})
  })
)
```

**Status**: Validated (in use across test suite)
**Wiki Page**: Mentioned in Testing/Jest-ESM-Mocking-Strategy.md

---

### 2025-11-24 - build/graph.json for Dependency Analysis

**Type**: Tool/Pattern
**Priority**: High
**Context**: Understanding code dependencies, especially for testing

**What**: Use auto-generated `build/graph.json` to find all transitive dependencies

**Why**:
- Shows complete dependency tree
- Critical for Jest tests (need to mock ALL transitive deps)
- Prevents "missing mock" test failures
- Automated (regenerated on every build)

**Example**:
```bash
# Find all dependencies of a file
cat build/graph.json | jq '.transitiveDependencies["src/lambdas/WebhookFeedly/src/index.ts"]'

# Output shows ALL files that need mocking
```

**Status**: Validated (documented in project conventions)
**Wiki Page**: Mentioned in AGENTS.md, could be separate Testing wiki page

---

### 2025-11-24 - LocalStack Vendor Wrapper Configuration

**Type**: Pattern
**Priority**: Medium
**Context**: Integration testing with LocalStack

**What**: Vendor wrappers check `USE_LOCALSTACK` env var and configure endpoints accordingly

**Why**:
- Single source for LocalStack configuration
- Tests don't need to mock AWS service endpoints
- Same vendor wrappers work in LocalStack and real AWS
- Easy to switch between local and cloud testing

**Example**:
```typescript
// lib/vendor/AWS/S3.ts
function getS3Client(): S3Client {
  if (!s3Client) {
    const isLocalStack = process.env.USE_LOCALSTACK === 'true'
    
    s3Client = new S3Client({
      region: process.env.AWS_REGION || 'us-west-2',
      endpoint: isLocalStack ? 'http://localhost:4566' : undefined,
      forcePathStyle: isLocalStack  // Required for LocalStack
    })
  }
  return s3Client
}
```

**Status**: Validated (used in multiple vendor wrappers)
**Wiki Page**: Could be added to Integration/LocalStack-Testing.md or Testing/Integration-Testing.md

---

## How to Use This Page

### For Developers

1. **Check before coding** - See if someone already solved your problem
2. **Add discoveries** - Append new conventions you find
3. **Validate patterns** - Mark as "Validated" after successful use
4. **Promote to wiki** - Create proper wiki page for important patterns

### For AI Assistants

1. **Monitor this page** - Check for patterns relevant to current task
2. **Flag new patterns** - When you notice repeated decisions, flag them
3. **Suggest promotion** - Recommend validated patterns for wiki pages
4. **Update status** - Mark patterns as you see them used/validated

### Adding an Entry

```markdown
## [Today's Date] - [Descriptive Name]

**Type**: [Rule/Pattern/Methodology/Anti-Pattern]
**Priority**: [Critical/High/Medium/Low]
**Context**: [When/where this applies]

**What**: [Clear one-sentence description]

**Why**: [Brief rationale - what problem does this solve?]

**Example**:
```[language]
[Clear, minimal code example]
```

**Status**: Emerging
**Wiki Page**: Pending
```

## Promotion Criteria

A convention should be promoted to a wiki page when:

1. **Validated** - Used successfully in 3+ places
2. **Important** - Priority High or Critical
3. **Stable** - Pattern unlikely to change
4. **General** - Applies broadly, not one-off solution
5. **Ready** - Enough examples and rationale to document properly

## Archive

When a convention is promoted to a wiki page, update the entry:

```markdown
## [Date] - [Convention Name]
[... existing content ...]

**Status**: Documented
**Wiki Page**: `../Category/Page-Name.md` (example path)
**Promoted**: [Date]
```

Leave the entry here for reference, but mark it as documented.

## Anti-Patterns to Avoid

Document anti-patterns (things NOT to do):

```markdown
## [Date] - [Anti-Pattern Name]

**Type**: Anti-Pattern
**Priority**: [Level]
**Context**: [Where people try this]

**What**: [What people mistakenly do]

**Why Not**: [Why this doesn't work]

**Instead**: [What to do instead]

**Example of Problem**:
```[language]
// Bad approach
```

**Correct Approach**:
```[language]
// Good approach
```

**Status**: [Status]
**Wiki Page**: [Link or Pending]
```

## Integration with Convention Capture

This page is part of the Convention Capture System:

1. **Detection** - Patterns emerge during development
2. **Flagging** - Developer or AI flags pattern (adds to this page)
3. **Validation** - Pattern used successfully multiple times
4. **Documentation** - Create proper wiki page
5. **Enforcement** - Add linting/testing where possible

See [Convention Capture System](Convention-Capture-System.md) for full methodology.

## Status Definitions

- **Emerging**: Just discovered, not yet validated
- **Validated**: Successfully used in production code
- **Documented**: Promoted to formal wiki page
- **Deprecated**: Pattern no longer recommended
- **Rejected**: Tried but doesn't work well

## Related Patterns

- [Convention Capture System](Convention-Capture-System.md) - Full methodology
- [Documentation Patterns](Documentation-Patterns.md) - Wiki organization
- [Working with AI Assistants](Working-with-AI-Assistants.md) - AI collaboration

---

*This is a living document. Append new conventions as they emerge. Update status as patterns are validated and promoted to wiki pages. Never delete entries - they provide history of pattern evolution.*
</file>

<file path="docs/wiki/Meta/pnpm-Migration.md">
# pnpm Migration Guide

## Overview

This project migrated from npm to pnpm v10+ to implement strategic security hardening against AI-targeted supply chain attacks while improving CI/CD performance by 50-66%.

## Why pnpm?

### Security: Lifecycle Script Protection

**The Primary Motivation**: Defense against AI-targeted typosquatting attacks.

**Attack Vector**:
1. Attackers study which package names LLMs frequently hallucinate
2. Create typosquatted packages matching those hallucinations (e.g., `stripe-node` vs `stripe`, `aws-s3` vs `@aws-sdk/client-s3`)
3. Add malicious `postinstall` scripts that execute **before** developer realizes the mistake
4. Scripts steal: AWS credentials, environment variables, source code, SSH keys

**pnpm v10+ Defense**:
- **Installation-time protection** ‚Üí Scripts blocked by default via `enable-pre-post-scripts=false`
- **Explicit allowlist** ‚Üí Force conscious decision per package in `.npmrc`
- **Audit window** ‚Üí Review code before any execution
- **Visibility** ‚Üí Know exactly which packages need script execution

**Example Protection**:
```bash
# npm (DANGEROUS)
npm install aws-dynamodb  # ‚ö†Ô∏è Runs postinstall script IMMEDIATELY
# If malicious: Credentials stolen before you realize it's wrong package

# pnpm (SAFE)
pnpm install aws-dynamodb  # ‚úÖ Refuses to run scripts
# Error: "aws-dynamodb" tried to run a postinstall script. 
# Add to .npmrc if you trust it: pnpm.onlyBuiltDependencies[]=aws-dynamodb
# You have time to: Check package, realize typo, remove package
```

### Performance: 50-66% Faster CI/CD

**CI/CD Improvements**:
- **Cold cache**: 2-3 min (npm) ‚Üí 45-60 sec (pnpm) = 50% faster
- **Warm cache**: 45-60 sec (npm) ‚Üí 15-20 sec (pnpm) = 66% faster

**ROI**: 30 builds/month √ó 1.5 min saved = **45 min/month** ‚Üí **9 hours/year** saved

### Future: Monorepo Architecture

pnpm workspaces enable future evolution:
- Extract `cloudwatch-fixture-extractor` as standalone package (#102)
- Extract `electrodb-dynamodb-adapter` for Better Auth (#85)
- Share ElectroDB entities across multiple projects
- Dependency isolation per Lambda package

## Configuration Files

### .npmrc (Security Hardening)

Located at project root with security-first settings:

```ini
# SECURITY: Disable all lifecycle scripts by default
enable-pre-post-scripts=false

# Explicitly allowlist packages requiring scripts (AUDIT BEFORE ADDING)
# Expected: NONE for this project (all pure JS/TS dependencies)
# pnpm.onlyBuiltDependencies[]=package-name

# PERFORMANCE: Use hard links (faster, disk-efficient)
package-import-method=hardlink

# Strict peer dependencies (catch compatibility issues)
strict-peer-dependencies=false

# Hoist pattern (compatibility with some tools)
shamefully-hoist=false
```

### pnpm-workspace.yaml (Future Monorepo)

Located at project root for future workspace support:

```yaml
packages:
  - 'packages/*'
  - 'apps/*'
```

Currently unused but positions project for growth.

## Usage

### Installation

```bash
# Install pnpm globally
npm install -g pnpm

# Install project dependencies
pnpm install

# Frozen lockfile for CI (like npm ci)
pnpm install --frozen-lockfile
```

### Common Commands

All npm commands work with pnpm by replacing `npm` with `pnpm`:

```bash
# Build
pnpm run build          # Was: npm run build

# Test
pnpm test               # Was: npm test
pnpm run test:integration  # Was: npm run test:integration

# Deploy
pnpm run deploy         # Was: npm run deploy

# Format
pnpm run format         # Was: npm run format

# Add dependency
pnpm add package-name   # Was: npm install package-name

# Add dev dependency
pnpm add -D package-name  # Was: npm install --save-dev package-name

# Production install
pnpm install --prod     # Was: npm install --only=production
```

### CI/CD Usage

GitHub Actions workflows updated to use pnpm:

```yaml
- name: Setup pnpm
  uses: pnpm/action-setup@v4
  with:
    version: 10

- name: Setup Node.js
  uses: actions/setup-node@v4
  with:
    node-version-file: '.nvmrc'
    cache: 'pnpm'  # Changed from 'npm'

- name: Install dependencies
  run: pnpm install --frozen-lockfile  # Was: npm ci --ignore-scripts
```

## Security Best Practices

### Adding New Dependencies

When adding a new package that requires install scripts:

1. **Try installing** - pnpm will block and error if scripts needed
2. **Audit the package** - Check package code on npm/GitHub
3. **Verify legitimacy** - Ensure it's the correct package (not typosquatted)
4. **Review scripts** - Look at `package.json` scripts field
5. **Add to allowlist** (only if safe):
   ```ini
   # .npmrc
   pnpm.onlyBuiltDependencies[]=package-name
   ```

### Monitoring for Typosquatting

Be extra vigilant when:
- LLM suggests a package name
- Installing based on AI code suggestions
- Package name seems "close but not quite right"
- Error mentions blocked install scripts

**Always verify**:
- Package name spelling
- Package exists on npmjs.com
- Package has legitimate install scripts (most don't need them)
- Package maintainers and download counts

### Expected Behavior

**Current Project**: 
- **Zero packages should require install scripts**
- All dependencies are pure JS/TS (no native bindings)
- If script error occurs ‚Üí likely typosquatted package or incorrect name

**If Adding Native Dependencies** (future):
- esbuild (native binary compilation)
- sharp (image processing)
- canvas (native graphics)

These would need explicit allowlist after audit.

## Dependency Audit Results

**Audit Date**: November 24, 2025
**Total Packages**: 962 resolved packages
**Packages with Build Scripts**: 2 (core-js, esbuild)

### Identified Build Scripts

| Package | Version | Purpose | Required? | Action |
|---------|---------|---------|-----------|--------|
| `core-js` | 3.47.0 | Polyfills setup | ‚ùå No | Scripts blocked - dev dependency of redoc, not needed for runtime |
| `esbuild` | 0.25.12 | Native binary compilation | ‚ùå No | Scripts blocked - comes with prebuilt binaries for darwin-arm64 |

### Security Assessment

‚úÖ **Zero packages require install scripts for this project**
- All production dependencies are pure JavaScript/TypeScript
- Both packages with scripts are dev dependencies (redoc, tsx)
- Both work correctly without running install scripts
- Build and tests pass successfully with scripts blocked

### Verification Commands

```bash
# Check which packages have build scripts
pnpm list core-js esbuild

# Verify no production deps have scripts
pnpm why core-js  # Result: dev dependency (redoc)
pnpm why esbuild  # Result: dev dependency (tsx)

# Test that everything works
pnpm run build    # ‚úÖ Succeeds
pnpm test         # ‚úÖ 163 tests pass
```

### Conclusion

The security configuration (`enable-pre-post-scripts=false`) successfully blocks all install scripts without impacting functionality. The two packages that have scripts are:
1. Dev-only dependencies (not in production bundles)
2. Work correctly with prebuilt binaries/configurations

**No allowlist additions needed.** ‚úÖ

## Troubleshooting

### Script Blocked Error

**Error**:
```
ERR_PNPM_LIFECYCLE_SCRIPT_NOT_FOUND "package-name" tried to run a postinstall script
```

**Solution**:
1. Verify package name is correct (not typosquatted)
2. Check package on npmjs.com
3. If legitimate, audit script code
4. Add to `.npmrc` allowlist if safe

### Lock File Issues

**Error**: `ERR_PNPM_LOCKFILE_BREAKING_CHANGE`

**Solution**:
```bash
# Remove old lock file
rm pnpm-lock.yaml

# Reinstall
pnpm install
```

### Cache Issues

**Error**: Weird dependency resolution

**Solution**:
```bash
# Clear pnpm store
pnpm store prune

# Reinstall
pnpm install
```

### Peer Dependency Warnings

**Warning**: `WARN ... has unmet peer dependency`

**Solution**:
- Review warning - may need to install peer dependency
- If known safe, leave as warning (strict-peer-dependencies=false)
- If causing issues, set `strict-peer-dependencies=true` in `.npmrc`

## Migration Checklist

For future projects or contributors:

- [ ] Install pnpm globally: `npm install -g pnpm`
- [ ] Remove npm artifacts: `rm -rf node_modules package-lock.json`
- [ ] Create `.npmrc` with security settings
- [ ] Create `pnpm-workspace.yaml` (for future monorepo)
- [ ] Update `package.json` engines field
- [ ] Update CI/CD workflows to use pnpm
- [ ] Generate `pnpm-lock.yaml`: `pnpm import` or `pnpm install`
- [ ] Test build: `pnpm run build`
- [ ] Test unit tests: `pnpm test`
- [ ] Test integration: `pnpm run test:integration`
- [ ] Update documentation (README.md, AGENTS.md)
- [ ] Add `pnpm-lock.yaml` to git, exclude `.pnpm-store` in `.gitignore`

## Emergency Rollback

If the pnpm migration causes critical issues in production, follow these steps to revert:

### Rollback Procedure

1. **Revert the migration commits**:
   ```bash
   # Find the migration commits
   git log --oneline --grep="pnpm"

   # Revert the changes (replace with actual commit SHAs)
   git revert <commit-sha-1> <commit-sha-2>
   ```

2. **Remove pnpm artifacts**:
   ```bash
   rm -rf node_modules pnpm-lock.yaml .pnpm-store
   rm -f .npmrc pnpm-workspace.yaml
   rm -rf packages/ apps/
   ```

3. **Restore npm lock file** (if available in git history):
   ```bash
   git checkout <pre-migration-commit> -- package-lock.json
   ```

4. **Reinstall with npm**:
   ```bash
   npm install
   npm run build
   npm test
   ```

5. **Update CI/CD workflows**:
   - Revert `.github/workflows/unit-tests.yml` to use npm
   - Revert `.github/workflows/integration-tests.yml` to use npm
   - Remove `pnpm/action-setup` step
   - Change `cache: 'pnpm'` back to `cache: 'npm'`

6. **Update documentation**:
   - Revert `README.md` and `AGENTS.md` command examples
   - Remove `docs/wiki/Meta/pnpm-Migration.md`
   - Revert shell script references

7. **Update package.json scripts** (if modified):
   ```json
   {
     "build": "node --loader ts-node/esm ./node_modules/.bin/webpack-cli --config ./config/webpack.config.ts",
     "test": "node --no-warnings --experimental-vm-modules ./node_modules/.bin/jest --silent --config config/jest.config.mjs"
   }
   ```

### Rollback Verification

After rollback, verify everything works:

```bash
# Build should succeed
npm run build

# Tests should pass
npm test

# CI should be green
git push && # Check GitHub Actions
```

### When to Rollback

Consider rollback only if:
- ‚ùå CI/CD consistently fails despite fixes
- ‚ùå Production deployments break
- ‚ùå Critical dependency issues arise
- ‚ùå Team cannot adapt to pnpm workflow

**Note**: Minor issues (warnings, local setup) should not trigger rollback. The security benefits outweigh minor friction.

## References

- **pnpm Security**: https://pnpm.io/cli/install#--ignore-scripts
- **pnpm v10 Release**: https://github.com/pnpm/pnpm/releases/tag/v10.0.0
- **Supply Chain Security**: https://slsa.dev/
- **Content-Addressable Storage**: https://pnpm.io/symlinked-node-modules-structure
- **Original Discussion**: [HN - AI-Targeted Package Typosquatting](https://news.ycombinator.com/item?id=42451576)

## Benefits Summary

**Security**:
- ‚úÖ Defense against AI-targeted typosquatting
- ‚úÖ Explicit consent for code execution
- ‚úÖ Audit window before script execution
- ‚úÖ 100% visibility into script dependencies

**Performance**:
- ‚úÖ 50-66% faster CI/CD installs
- ‚úÖ 40-60% disk space savings
- ‚úÖ Content-addressable storage (integrity verification)
- ‚úÖ Faster local development

**Architecture**:
- ‚úÖ Monorepo-ready with workspaces
- ‚úÖ Strict dependency resolution (no phantom deps)
- ‚úÖ Better organization for future growth

**The time invested in this migration is a proactive defense against existential security threats while accelerating development velocity.**
</file>

<file path="docs/wiki/Meta/Working-with-AI-Assistants.md">
# Working with AI Assistants

## Quick Reference
- **When to use**: All interactions with AI coding assistants
- **Enforcement**: Recommended - improves collaboration effectiveness
- **Impact if violated**: LOW - Less effective AI assistance

## The Rules

1. **Provide Complete Context** - Include relevant code, errors, and desired outcomes
2. **Be Specific About Constraints** - State conventions, patterns, and restrictions explicitly
3. **Request Incremental Changes** - Break large tasks into smaller, verifiable steps
4. **Review and Iterate** - Check AI output, provide feedback, refine results

## Effective Prompting Patterns

### ‚úÖ Correct - Specific with Context

```
Add error handling to the ProcessFile Lambda function.

Context:
- File: src/lambdas/ProcessFile/src/index.ts
- This is an API Gateway Lambda (must return responses, not throw)
- We use the withXRay decorator for all handlers
- Error logging uses logError from util/lambda-helpers

Requirements:
- Wrap main logic in try/catch
- Log errors with file ID and trace ID
- Return 500 status with generic error message
- Follow existing error handling pattern from DownloadVideo Lambda
```

### ‚úÖ Correct - Explicit Constraints

```
Create a new Lambda function called RegisterDevice.

Constraints:
- Use TypeScript with strict types
- Follow Lambda Function Patterns wiki guide
- Use vendor wrappers for AWS SDK (ZERO tolerance)
- Mock ALL transitive dependencies in tests
- Use PascalCase for function name
- Include X-Ray tracing with withXRay decorator

Structure:
src/lambdas/RegisterDevice/
‚îú‚îÄ‚îÄ src/index.ts
‚îî‚îÄ‚îÄ test/index.test.ts

Follow existing Lambda patterns (see ProcessFile, DownloadVideo).
```

### ‚ùå Incorrect - Vague Request

```
Add error handling to my Lambda.
```

Problems: Which Lambda? What type of error handling? What patterns to follow?

### ‚ùå Incorrect - Too Large

```
Create the entire media download system including all Lambdas, DynamoDB schema,
S3 buckets, API Gateway, testing, documentation, and deployment scripts.
```

Problems: Too many components at once, hard to review, difficult to debug.

## Providing Context

### Share Relevant Code

```
I need to refactor the download logic in DownloadVideo Lambda.

Current implementation:
[paste src/lambdas/DownloadVideo/src/index.ts]

Related utilities:
[paste relevant util functions]

Issue: Download times out for large videos (>500MB)

Goal: Implement streaming download with progress tracking
```

### Share Error Messages

```
Tests are failing with this error:

```
TypeError: Cannot read property 'send' of undefined
  at headObject (lib/vendor/AWS/S3.ts:15:25)
```

Test file: [paste test code]
Handler: [paste handler code]

I think it's a mocking issue but not sure what's missing.
```

## Iterative Refinement

### Step 1: Initial Request

```
Create a Lambda function to process video files from S3.
```

### Step 2: AI Response Review

```
The AI created the function but:
1. ‚ùå Used direct AWS SDK import (violates SDK-Encapsulation-Policy)
2. ‚úÖ Used correct error handling
3. ‚ùå Missing X-Ray tracing
4. ‚úÖ Tests included

Feedback:
Please fix issues #1 and #3:
- Replace AWS SDK imports with vendor wrappers (see lib/vendor/AWS/S3.ts)
- Add withXRay decorator (see existing Lambdas for pattern)
```

### Step 3: Verification

```
Perfect! The code now:
‚úÖ Uses vendor wrappers
‚úÖ Has X-Ray tracing
‚úÖ Follows all patterns

Let's proceed to Step 2: Add business logic.
```

## Common Scenarios

### Adding a New Feature

```
Task: Add retry logic to S3 uploads

Context: File lib/vendor/AWS/S3.ts
Current: createS3Upload() creates Upload instance
Issue: Uploads fail on network issues with no retry

Requirements:
1. Add retry logic (max 3 attempts)
2. Use exponential backoff (1s, 2s, 4s)
3. Log retry attempts with logWarn
4. Throw on final failure

Constraints:
- Don't change function signature
- Don't add new dependencies
- Keep lazy initialization pattern
```

### Fixing a Bug

```
Bug: ProcessFile Lambda returns 502 instead of 500 on errors

Current code: [paste handler]

Expected: Errors should return {statusCode: 500, body: {error: message}}
NOT throw (causes 502)

Please:
1. Identify where error is being thrown
2. Add proper error handling
3. Ensure we follow API Gateway pattern (return response, never throw)
```

## Zero-Tolerance Rules

Before making changes, always:
1. Read applicable wiki guide (docs/wiki/)
2. Check existing similar code
3. Follow established patterns
4. Ask if unsure

**Zero-tolerance violations**:
- AWS SDK Encapsulation (NEVER import @aws-sdk/* directly)
- No AI attribution in commits
- Git as source of truth (no commented-out code explanations)

## Related Patterns

- [Convention Capture System](Convention-Capture-System.md) - Detecting emerging conventions
- [AI Tool Context Files](AI-Tool-Context-Files.md) - AGENTS.md structure

---

*Provide complete context, be specific about constraints, request incremental changes, and iterate based on review. AI assistants work best with clear, specific instructions and relevant examples.*
</file>

<file path="docs/wiki/Methodologies/Convention-Over-Configuration.md">
# Convention Over Configuration

## Quick Reference
- **When to use**: All development decisions and architectural choices
- **Enforcement**: Philosophy, code reviews, established patterns
- **Impact if violated**: LOW - Increased complexity, inconsistency, maintenance burden

## Overview

Prefer established patterns with sensible defaults over flexible configuration. This principle reduces decision fatigue, improves consistency, speeds development, and makes the codebase more predictable.

## The Rules

1. **Use Project Defaults** - Don't configure what already has a sensible default
2. **Follow Established Patterns** - Use existing patterns before creating new ones
3. **Minimize Configuration Files** - Avoid proliferation of config files when conventions suffice
4. **Document Exceptions** - When configuration is necessary, document why the convention doesn't work

## Examples

### ‚úÖ Correct - Using Convention for Lambda Handlers

```typescript
// src/lambdas/ListFiles/src/index.ts

// Standard Lambda pattern - no configuration needed
import {lambdaErrorResponse, response, logInfo, getUserDetailsFromEvent} from '../../../util/lambda-helpers'
import {withXRay} from '../../../lib/vendor/AWS/XRay'
import {Files} from '../../../entities/Files'

export const handler = withXRay(async (event, context, {traceId}) => {
  logInfo('event <=', event)  // Standard logging pattern

  try {
    const {userId, userStatus} = getUserDetailsFromEvent(event)
    const files = await getFilesByUser(userId)

    // Standard success response
    return response(context, 200, {contents: files, keyCount: files.length})
  } catch (error) {
    // Standard error handling for API Gateway
    return lambdaErrorResponse(context, error)
  }
})
```

### ‚úÖ Correct - Standard File Structure

```
src/lambdas/FunctionName/
‚îú‚îÄ‚îÄ src/index.ts       # Convention: handler always here
‚îú‚îÄ‚îÄ test/index.test.ts # Convention: test mirrors source
‚îî‚îÄ‚îÄ fixtures/          # Convention: test data location
    ‚îî‚îÄ‚îÄ event.json

terraform/LambdaFunctionName.tf  # Convention: PascalCase matches function
```

No configuration needed - the structure itself is the convention.

### ‚ùå Incorrect - Over-Configuration

```typescript
// ‚ùå Custom configuration for standard behavior
const lambdaConfig = {
  errorHandler: customErrorHandler,
  responseFormatter: customResponseFormatter,
  loggingLevel: 'custom',
  timeout: 30,
  retryPolicy: customRetryPolicy
}

export const handler = createCustomHandler(lambdaConfig, async (event) => {
  // Now we need to maintain this custom configuration
})
```

Problems: Configuration duplicates what conventions provide, custom patterns when standard ones work.

### ‚ùå Incorrect - Configuration Files for Standard Behavior

```json
// ‚ùå lambda-config.json (unnecessary)
{
  "handlers": {
    "ProcessFile": {
      "path": "src/lambdas/ProcessFile/src/index.ts",
      "handler": "handler",
      "runtime": "nodejs22.x"
    }
  }
}
```

Instead: Use convention that all Lambdas follow the same structure.

## Real Project Examples

### Lambda Conventions

```typescript
// Convention: All API Gateway Lambdas return responses
// No need to configure response vs throw behavior
if (isApiGatewayLambda) {
  return response(context, statusCode, body)
} else {
  throw error  // Event-driven Lambdas throw for retry
}
```

### Testing Conventions

```typescript
// Convention: Test files mirror source files
src/lambdas/ProcessFile/src/index.ts
src/lambdas/ProcessFile/test/index.test.ts

// Convention: Fixtures in standard location
src/lambdas/ProcessFile/test/fixtures/event.json
```

### Infrastructure Conventions

```hcl
# Convention: Resource names match TypeScript names
resource "aws_lambda_function" "ProcessFile" {
  function_name = "ProcessFile"  # No configuration mapping needed
}
```

## When Configuration IS Appropriate

### Environment-Specific Settings

```typescript
// ‚úÖ Configuration for environment differences
const config = {
  apiUrl: process.env.API_URL,  // Varies by environment
  region: process.env.AWS_REGION || 'us-west-2'
}
```

### Security and Secrets

```typescript
// ‚úÖ Configuration for sensitive data
const config = {
  apiKey: process.env.API_KEY,  // Can't be hardcoded
  certificatePath: process.env.CERT_PATH
}
```

## Benefits

### Reduced Cognitive Load
- Developers don't need to make decisions already made
- New team members learn one way of doing things
- Less documentation needed

### Faster Development
- No time spent on configuration
- Copy existing patterns
- Focus on business logic

### Better Consistency
- All code follows same patterns
- Predictable structure
- Easier code reviews

## Implementation Guidelines

1. **Establish Conventions Early** - Define patterns at project start
2. **Document Conventions** - Create wiki pages for each convention
3. **Enforce Through Code Review** - Check that code follows established patterns
4. **Automate Where Possible** - ESLint rules for project conventions

## Related Patterns

- [Lambda Function Patterns](../TypeScript/Lambda-Function-Patterns.md) - Standard Lambda conventions
- [Naming Conventions](../Conventions/Naming-Conventions.md) - Consistent naming patterns
- [Testing Patterns](../Testing/Jest-ESM-Mocking-Strategy.md) - Test conventions

---

*Choose convention over configuration. Make the easy path the right path.*
</file>

<file path="docs/wiki/Methodologies/Dependabot-Resolution.md">
# Dependabot Resolution

## Quick Reference
- **When to use**: Handling dependency updates
- **Enforcement**: Automated via GitHub Actions
- **Impact if violated**: MEDIUM - Security vulnerabilities

## Auto-Merge Rules

### ‚úÖ Auto-Merge When
- Patch version (x.x.PATCH)
- All tests passing
- Dev dependencies only
- No breaking changes

### üîç Manual Review When
- Minor/major versions
- Production dependencies
- AWS SDK updates
- Security alerts

## Resolution Process

```bash
# 1. Check what changed
npm outdated
npm audit

# 2. Update and test
npm update <package>
npm test
npm run build

# 3. Verify
npm audit fix
git commit -m "chore(deps): update <package>"
```

## Common Issues

| Update Type | Action |
|------------|--------|
| Security alert | Fix immediately |
| Major version | Check migration guide |
| Type errors | Update @types packages |
| AWS SDK | Update all @aws-sdk/* together |
| Jest | Update all jest packages together |

## AWS SDK Special Handling

```bash
# Update all AWS packages
npm update @aws-sdk/client-*

# Add new services to webpack externals
externals: ['@aws-sdk/client-new-service']
```

## Priority Levels

1. **Critical Security** - Fix immediately
2. **High Security** - Within 24 hours
3. **Production Deps** - Within week
4. **Dev Dependencies** - Next sprint

## Related Patterns

- [Library Migration](Library-Migration-Checklist.md)
- [Testing Strategy](../Testing/Jest-ESM-Mocking-Strategy.md)

---

*Auto-merge patch updates. Review minor/major versions and production dependencies.*
</file>

<file path="docs/wiki/Methodologies/Library-Migration-Checklist.md">
# Library Migration Checklist

## Quick Reference
- **When to use**: Replacing major libraries
- **Enforcement**: Required for breaking changes
- **Impact if violated**: HIGH - Production failures

## Migration Phases

### 1. Planning
- Document current usage: `grep -r "old-library" src/`
- Check feature parity
- Review breaking changes

### 2. Parallel Implementation
```typescript
// lib/vendor/Wrapper.ts
export function getClient() {
  if (process.env.USE_NEW) {
    return newLibrary.create()
  }
  return oldLibrary.create()
}
```

### 3. Incremental Migration
- [ ] Install new library alongside old
- [ ] Create vendor wrapper with flag
- [ ] Migrate one Lambda at a time
- [ ] Test in staging
- [ ] Monitor metrics

### 4. Validation
- [ ] Unit tests pass
- [ ] Integration tests pass
- [ ] Performance acceptable
- [ ] No memory leaks

### 5. Cleanup
- [ ] Remove feature flag
- [ ] Uninstall old library
- [ ] Update documentation

## Common Migrations

### AWS SDK v2 ‚Üí v3
```bash
npm uninstall aws-sdk
npm install @aws-sdk/client-*
# Update webpack externals
```

### Jest Major Version
```bash
npm update jest @jest/globals @types/jest
# Check migration guide for config changes
```

### ElectroDB Updates
```bash
npm update electrodb
# Test all entity queries
```

## Rollback Strategy

1. Keep old library during migration
2. Use environment variable switch
3. Monitor error rates
4. Quick revert if needed

## Testing Requirements

‚úÖ All unit tests pass
‚úÖ Integration tests with LocalStack
‚úÖ Load testing completed
‚úÖ Error handling verified
‚úÖ Rollback tested

## Related Patterns

- [Dependabot Resolution](Dependabot-Resolution.md)
- [Vendor Wrappers](../AWS/SDK-Encapsulation-Policy.md)

---

*Migrate gradually with feature flags and vendor wrappers.*
</file>

<file path="docs/wiki/Testing/Dependency-Graph-Analysis.md">
# Dependency Graph Analysis

## Quick Reference
- **When to use**: Finding transitive dependencies for Jest mocking
- **Enforcement**: Required for accurate test mocking
- **Impact if violated**: HIGH - Missing mocks cause test failures

## The graph.json File

The `build/graph.json` file is automatically generated before builds and tests, ensuring fresh dependency analysis:

```json
{
  "directDependencies": {
    "src/lambdas/WebhookFeedly/src/index.ts": [
      "aws-lambda",
      "../../../entities/Files",
      "../../../entities/UserFiles",
      // ... all direct imports
    ]
  },
  "transitiveDependencies": {
    "src/lambdas/WebhookFeedly/src/index.ts": [
      "aws-lambda",
      "@aws-sdk/client-dynamodb",
      "@aws-sdk/lib-dynamodb",
      "electrodb",
      // ... ALL dependencies including transitive
    ]
  }
}
```

## Usage for Jest Testing

**CRITICAL**: Use `transitiveDependencies` to find ALL mocks needed for a test file.

### Finding Required Mocks
```bash
# List all transitive dependencies for a Lambda
cat build/graph.json | jq '.transitiveDependencies["src/lambdas/WebhookFeedly/src/index.ts"]'

# Check if a specific module is needed
cat build/graph.json | jq '.transitiveDependencies["src/lambdas/ListFiles/src/index.ts"]' | grep electrodb
```

### Test Setup Process
1. Generate the graph: `npm run generate-graph`
2. Find your file in `transitiveDependencies`
3. Mock ALL external packages listed
4. Mock ALL vendor wrappers (lib/vendor/*)
5. Mock ALL entities if using ElectroDB

## Common Patterns

### Lambda Test Mocking
```typescript
// 1. Check graph.json for Lambda's transitive dependencies
// 2. Mock everything external BEFORE imports

// If graph.json shows these dependencies:
// ["aws-lambda", "@aws-sdk/client-s3", "electrodb", "../../../entities/Files"]

// Then mock them all:
jest.unstable_mockModule('@aws-sdk/client-s3', () => ({
  S3Client: jest.fn()
}))

jest.unstable_mockModule('../../../entities/Files', () => ({
  Files: createElectroDBEntityMock().entity
}))

// 3. Import handler AFTER mocking
const {handler} = await import('../src/index')
```

### Debugging Missing Mocks
```bash
# If test fails with "Cannot find module X"
# Check if X is in transitive dependencies:
cat build/graph.json | jq '.transitiveDependencies["path/to/your/file.ts"]' | grep "X"

# If present, you forgot to mock it
# If absent, regenerate graph: npm run generate-graph
```

## Graph Generation

The graph is generated automatically:
- **Before builds**: `pnpm run build` runs `generate-graph` first
- **Before tests**: `pnpm test` runs `generate-graph` first
- **Before integration tests**: `pnpm test:integration` runs `generate-graph` first
- **Manual generation**: `pnpm run generate-graph`
- **Script location**: `scripts/generateDependencyGraph.ts`

**Note**: Lifecycle scripts are disabled for security (`enable-pre-post-scripts=false` in `.npmrc`), so scripts explicitly run `generate-graph` rather than using `prebuild`/`pretest` hooks.

## Best Practices

1. **Always regenerate** after adding new imports
2. **Check transitive deps** not just direct deps
3. **Mock everything external** shown in graph
4. **Use for code review** to verify import changes
5. **Automate in CI** to catch missing mocks

## Related Patterns

- [Jest ESM Mocking Strategy](Jest-ESM-Mocking-Strategy.md) - How to mock
- [Mock Type Annotations](Mock-Type-Annotations.md) - TypeScript patterns
- [Integration Testing](Integration-Testing.md) - When to use real deps

---

*Use build/graph.json to identify ALL transitive dependencies for comprehensive Jest mocking.*
</file>

<file path="docs/wiki/Testing/ElectroDB-Testing-Patterns.md">
# ElectroDB Testing Patterns

## Quick Reference
- **When to use**: Testing DynamoDB operations with ElectroDB
- **Enforcement**: Required for all DynamoDB code
- **Impact if violated**: HIGH - Untested database operations

## Overview

Comprehensive testing strategy for ElectroDB entities covering unit tests (mocked) and integration tests (LocalStack). Validates single-table design, Collections (JOIN operations), and type-safe queries.

## Unit Testing (Mocked)

### Using electrodb-mock Helper

```typescript
import {createElectroDBEntityMock} from '../../../test/helpers/electrodb-mock'
import {Files} from '../entities/Files'

// Mock the entity
const filesMock = createElectroDBEntityMock<FileData>({
  queryIndexes: ['byStatus', 'byUser']
})
jest.unstable_mockModule('../entities/Files', () => ({Files: filesMock.entity}))

// Setup test data
filesMock.mocks.query.byStatus!.go.mockResolvedValue({
  data: [{fileId: 'file-1', status: 'Downloaded'}]
})

// Test
const result = await Files.query.byStatus({status: 'Downloaded'}).go()
expect(result.data).toHaveLength(1)
expect(filesMock.mocks.query.byStatus!.go).toHaveBeenCalledTimes(1)
```

### Mock Operations Supported

**Query operations**:
```typescript
Files.query.byUser({userId}).go()
Files.query.byStatus({status}).go()
Files.query.byKey({fileId}).go()
```

**Get operations** (single and batch):
```typescript
Files.get({fileId}).go()              // Single
Files.get([{fileId: 'a'}, {fileId: 'b'}]).go()  // Batch
```

**Create/Update/Delete**:
```typescript
Files.create({fileId, ...}).go()
Files.update({fileId}).set({status: 'Downloaded'}).go()
Files.delete({fileId}).go()
```

### Available Query Indexes

Entity-specific indexes available via `queryIndexes` parameter:

**Files**: `['byStatus', 'byUser', 'byKey']`
**Users**: `['byEmail']` *(Better Auth - email lookup)*
**UserFiles**: `['byUser', 'byFile']`
**UserDevices**: `['byUser', 'byDevice']`
**Devices**: `['byDevice']`
**Sessions**: `['byUser', 'byDevice']` *(Better Auth)*
**Accounts**: `['byUser', 'byProvider']` *(Better Auth)*
**VerificationTokens**: `['byIdentifier']` *(Better Auth)*

## Integration Testing (LocalStack)

### LocalStack Setup

```typescript
import {setupLocalStackTable, cleanupLocalStackTable} from '../helpers/electrodb-localstack'

beforeAll(async () => {
  await setupLocalStackTable()
})

afterAll(async () => {
  await cleanupLocalStackTable()
})
```

**What it creates**:
- MediaDownloader table (single-table design)
- Primary index: PK, SK
- GSI1: userResources (Users + Files + Devices for user)
- GSI2: fileUsers (Users who have access to file)
- GSI3: deviceUsers (Devices associated with user)

### Collections Testing (JOIN Operations)

**Collections** enable JOIN-like queries across entities in single-table design.

#### userResources Collection

Get all resources for a user (Files + Devices):

```typescript
import {collections} from '../../../src/entities/Collections'

test('userResources - get all user resources', async () => {
  // Setup: Create user, files, devices
  await Users.create({userId: 'user-1', appleDeviceIdentifier: 'apple-1'}).go()
  await Files.create({fileId: 'file-1', status: 'Downloaded', url: 'https://...'}).go()
  await UserFiles.create({userId: 'user-1', fileId: 'file-1'}).go()
  await Devices.create({deviceId: 'dev-1', deviceName: 'iPhone'}).go()
  await UserDevices.create({userId: 'user-1', deviceId: 'dev-1'}).go()

  // Query: Get all resources for user
  const result = await collections.userResources({userId: 'user-1'}).go()

  // Validate
  expect(result.data.Users).toHaveLength(1)
  expect(result.data.Files).toHaveLength(1)
  expect(result.data.Devices).toHaveLength(1)
  expect(result.data.UserFiles).toHaveLength(1)
  expect(result.data.UserDevices).toHaveLength(1)
})
```

#### fileUsers Collection

Get all users who have access to a file (for notifications):

```typescript
test('fileUsers - notification use case', async () => {
  // Setup: Multiple users sharing file
  await Users.create({userId: 'user-1', appleDeviceIdentifier: 'apple-1'}).go()
  await Users.create({userId: 'user-2', appleDeviceIdentifier: 'apple-2'}).go()
  await Files.create({fileId: 'shared-file', status: 'Downloaded'}).go()
  await UserFiles.create({userId: 'user-1', fileId: 'shared-file'}).go()
  await UserFiles.create({userId: 'user-2', fileId: 'shared-file'}).go()

  // Query: Get all users to notify
  const result = await collections.fileUsers({fileId: 'shared-file'}).go()

  // Validate: Both users returned
  expect(result.data.Users).toHaveLength(2)
  expect(result.data.UserFiles).toHaveLength(2)
})
```

#### userSessions Collection (Better Auth)

Get all active sessions for a user:

```typescript
test('userSessions - authentication sessions', async () => {
  // Setup: User with multiple sessions
  await Users.create({userId: 'user-1', appleDeviceIdentifier: 'apple-1'}).go()
  await Sessions.create({
    sessionId: 'session-1',
    userId: 'user-1',
    token: 'token-1',
    expiresAt: Date.now() + 86400000
  }).go()
  await Sessions.create({
    sessionId: 'session-2',
    userId: 'user-1',
    token: 'token-2',
    expiresAt: Date.now() + 86400000
  }).go()

  // Query: Get all sessions
  const result = await collections.userSessions({userId: 'user-1'}).go()

  // Validate
  expect(result.data.Sessions).toHaveLength(2)
})
```

#### userAccounts Collection (Better Auth)

Get OAuth accounts linked to user:

```typescript
test('userAccounts - OAuth account linking', async () => {
  // Setup: User with Apple OAuth account
  await Users.create({userId: 'user-1', appleDeviceIdentifier: 'apple-1'}).go()
  await Accounts.create({
    accountId: 'account-1',
    userId: 'user-1',
    provider: 'apple',
    providerAccountId: 'apple-user-id'
  }).go()

  // Query: Get linked accounts
  const result = await collections.userAccounts({userId: 'user-1'}).go()

  // Validate
  expect(result.data.Accounts).toHaveLength(1)
  expect(result.data.Accounts[0].provider).toBe('apple')
})
```

### Better Auth Entity Testing

#### Session Entity CRUD

```typescript
import {Sessions} from '../../../src/entities/Sessions'

test('create and retrieve session', async () => {
  // Create session
  const sessionData = {
    sessionId: 'sess-123',
    userId: 'user-1',
    token: 'hashed-token',
    expiresAt: Date.now() + 86400000,  // 24 hours
    ipAddress: '192.168.1.1',
    userAgent: 'Mozilla/5.0...',
    deviceId: 'device-1'
  }

  await Sessions.create(sessionData).go()

  // Retrieve by sessionId
  const result = await Sessions.get({sessionId: 'sess-123'}).go()

  expect(result.data.userId).toBe('user-1')
  expect(result.data.token).toBe('hashed-token')
  expect(result.data.createdAt).toBeDefined()
  expect(result.data.updatedAt).toBeDefined()
})

test('query sessions by user', async () => {
  // Create multiple sessions for user
  await Sessions.create({
    sessionId: 'sess-1',
    userId: 'user-1',
    token: 'token-1',
    expiresAt: Date.now() + 86400000
  }).go()

  await Sessions.create({
    sessionId: 'sess-2',
    userId: 'user-1',
    token: 'token-2',
    expiresAt: Date.now() + 172800000  // 48 hours
  }).go()

  // Query all sessions for user
  const result = await Sessions.query.byUser({userId: 'user-1'}).go()

  expect(result.data).toHaveLength(2)
  expect(result.data[0].userId).toBe('user-1')
})

test('query sessions by device', async () => {
  // Create sessions for same device
  await Sessions.create({
    sessionId: 'sess-1',
    userId: 'user-1',
    deviceId: 'device-1',
    token: 'token-1',
    expiresAt: Date.now() + 86400000
  }).go()

  // Query sessions by device
  const result = await Sessions.query.byDevice({deviceId: 'device-1'}).go()

  expect(result.data).toHaveLength(1)
  expect(result.data[0].deviceId).toBe('device-1')
})
```

#### Account Entity OAuth Testing

```typescript
import {Accounts} from '../../../src/entities/Accounts'

test('create OAuth account', async () => {
  const accountData = {
    accountId: 'acc-123',
    userId: 'user-1',
    providerId: 'apple',
    providerAccountId: 'apple-user-123',
    accessToken: 'access-token',
    refreshToken: 'refresh-token',
    expiresAt: Date.now() + 3600000,  // 1 hour
    scope: 'email name',
    tokenType: 'Bearer',
    idToken: 'id-token'
  }

  await Accounts.create(accountData).go()

  // Retrieve account
  const result = await Accounts.get({accountId: 'acc-123'}).go()

  expect(result.data.providerId).toBe('apple')
  expect(result.data.providerAccountId).toBe('apple-user-123')
})

test('query accounts by user', async () => {
  // User with multiple OAuth providers
  await Accounts.create({
    accountId: 'acc-apple',
    userId: 'user-1',
    providerId: 'apple',
    providerAccountId: 'apple-123'
  }).go()

  await Accounts.create({
    accountId: 'acc-google',
    userId: 'user-1',
    providerId: 'google',
    providerAccountId: 'google-123'
  }).go()

  // Query all accounts for user
  const result = await Accounts.query.byUser({userId: 'user-1'}).go()

  expect(result.data).toHaveLength(2)
  expect(result.data.map(a => a.providerId)).toContain('apple')
  expect(result.data.map(a => a.providerId)).toContain('google')
})

test('query account by provider', async () => {
  await Accounts.create({
    accountId: 'acc-1',
    userId: 'user-1',
    providerId: 'apple',
    providerAccountId: 'apple-user-123'
  }).go()

  // Lookup by provider + provider account ID
  const result = await Accounts.query.byProvider({
    providerId: 'apple',
    providerAccountId: 'apple-user-123'
  }).go()

  expect(result.data).toHaveLength(1)
  expect(result.data[0].userId).toBe('user-1')
})
```

#### User Entity with Email Index

```typescript
import {Users} from '../../../src/entities/Users'

test('create user and query by email', async () => {
  // Create user
  await Users.create({
    userId: 'user-1',
    email: 'test@example.com',
    emailVerified: true,
    firstName: 'John',
    lastName: 'Doe',
    identityProviders: {
      userId: 'apple-123',
      email: 'test@example.com',
      emailVerified: true,
      isPrivateEmail: false,
      accessToken: 'token',
      refreshToken: 'refresh',
      tokenType: 'Bearer',
      expiresAt: Date.now() + 3600000
    }
  }).go()

  // Query by email (uses gsi3 byEmail index)
  const result = await Users.query.byEmail({email: 'test@example.com'}).go()

  expect(result.data).toHaveLength(1)
  expect(result.data[0].userId).toBe('user-1')
  expect(result.data[0].firstName).toBe('John')
})

test('email lookup returns empty for non-existent', async () => {
  const result = await Users.query.byEmail({email: 'nonexistent@example.com'}).go()

  expect(result.data).toHaveLength(0)
})
```

#### VerificationToken Entity

```typescript
import {VerificationTokens} from '../../../src/entities/VerificationTokens'

test('create and retrieve verification token', async () => {
  const tokenData = {
    token: 'verify-token-123',
    identifier: 'test@example.com',
    expiresAt: Date.now() + 3600000  // 1 hour
  }

  await VerificationTokens.create(tokenData).go()

  // Retrieve token
  const result = await VerificationTokens.get({token: 'verify-token-123'}).go()

  expect(result.data.identifier).toBe('test@example.com')
  expect(result.data.expiresAt).toBeGreaterThan(Date.now())
})

test('delete verification token after use', async () => {
  await VerificationTokens.create({
    token: 'temp-token',
    identifier: 'user@example.com',
    expiresAt: Date.now() + 3600000
  }).go()

  // Delete token
  await VerificationTokens.delete({token: 'temp-token'}).go()

  // Verify deletion
  const result = await VerificationTokens.get({token: 'temp-token'}).go()
  expect(result.data).toBeUndefined()
})
```

#### Complete Auth Flow Integration Test

```typescript
test('complete auth flow - register to session', async () => {
  // 1. Create user
  await Users.create({
    userId: 'user-1',
    email: 'newuser@example.com',
    emailVerified: false,
    firstName: 'Jane',
    lastName: 'Smith',
    identityProviders: {...}
  }).go()

  // 2. Create OAuth account
  await Accounts.create({
    accountId: 'acc-1',
    userId: 'user-1',
    providerId: 'apple',
    providerAccountId: 'apple-new-user'
  }).go()

  // 3. Create session
  await Sessions.create({
    sessionId: 'sess-1',
    userId: 'user-1',
    token: 'session-token',
    expiresAt: Date.now() + 86400000
  }).go()

  // 4. Verify complete setup via Collections
  const userResources = await collections.userSessions({userId: 'user-1'}).go()

  expect(userResources.data.Users).toHaveLength(1)
  expect(userResources.data.Sessions).toHaveLength(1)

  const userAccounts = await collections.userAccounts({userId: 'user-1'}).go()

  expect(userAccounts.data.Accounts).toHaveLength(1)
  expect(userAccounts.data.Accounts[0].providerId).toBe('apple')
})
```

#### Session Expiration Testing

```typescript
test('filter expired sessions', async () => {
  const now = Date.now()

  // Create expired session
  await Sessions.create({
    sessionId: 'sess-expired',
    userId: 'user-1',
    token: 'token-expired',
    expiresAt: now - 1000  // Already expired
  }).go()

  // Create active session
  await Sessions.create({
    sessionId: 'sess-active',
    userId: 'user-1',
    token: 'token-active',
    expiresAt: now + 86400000  // Future
  }).go()

  // Query all sessions
  const allSessions = await Sessions.query.byUser({userId: 'user-1'}).go()

  // Filter to active only
  const activeSessions = allSessions.data.filter(s => s.expiresAt > now)

  expect(allSessions.data).toHaveLength(2)
  expect(activeSessions).toHaveLength(1)
  expect(activeSessions[0].sessionId).toBe('sess-active')
})
```

### Batch Operations

#### Batch Get

```typescript
test('batch get - multiple files', async () => {
  await Files.create({fileId: 'file-1', status: 'Downloaded'}).go()
  await Files.create({fileId: 'file-2', status: 'Pending'}).go()
  await Files.create({fileId: 'file-3', status: 'Downloaded'}).go()

  const keys = [{fileId: 'file-1'}, {fileId: 'file-2'}, {fileId: 'file-3'}]
  const {data, unprocessed} = await Files.get(keys).go({concurrency: 5})

  expect(data).toHaveLength(3)
  expect(unprocessed).toHaveLength(0)
})
```

#### Batch Delete

```typescript
test('batch delete - cleanup orphaned records', async () => {
  await UserFiles.create({userId: 'user-1', fileId: 'file-1'}).go()
  await UserFiles.create({userId: 'user-1', fileId: 'file-2'}).go()

  const keys = [{userId: 'user-1', fileId: 'file-1'}, {userId: 'user-1', fileId: 'file-2'}]
  await UserFiles.delete(keys).go()

  // Verify deletion
  const result = await UserFiles.query.byUser({userId: 'user-1'}).go()
  expect(result.data).toHaveLength(0)
})
```

### Query Patterns

#### Pagination

```typescript
test('pagination - large result sets', async () => {
  // Create 100 files
  for (let i = 0; i < 100; i++) {
    await Files.create({fileId: `file-${i}`, status: 'Downloaded'}).go()
  }

  // First page
  const page1 = await Files.query.byStatus({status: 'Downloaded'}).go({pages: 1, limit: 25})
  expect(page1.data).toHaveLength(25)
  expect(page1.cursor).toBeDefined()

  // Second page
  const page2 = await Files.query.byStatus({status: 'Downloaded'})
    .go({cursor: page1.cursor, limit: 25})
  expect(page2.data).toHaveLength(25)
})
```

#### Filtering

```typescript
test('filter - conditional queries', async () => {
  await Files.create({fileId: 'file-1', status: 'Downloaded', size: 1000}).go()
  await Files.create({fileId: 'file-2', status: 'Downloaded', size: 5000}).go()
  await Files.create({fileId: 'file-3', status: 'Downloaded', size: 10000}).go()

  // Filter files > 2MB
  const result = await Files.query.byStatus({status: 'Downloaded'})
    .where(({size}, {gt}) => gt(size, 2000))
    .go()

  expect(result.data).toHaveLength(2)
  expect(result.data.every(f => f.size > 2000)).toBe(true)
})
```

### Edge Cases

#### Empty Results

```typescript
test('empty results - no data found', async () => {
  const result = await Files.query.byUser({userId: 'non-existent'}).go()
  expect(result.data).toHaveLength(0)
})
```

#### Conditional Create (Idempotency)

```typescript
test('conditional create - prevent duplicates', async () => {
  await UserFiles.create({userId: 'user-1', fileId: 'file-1'}).go()

  // Second create should fail
  await expect(
    UserFiles.create({userId: 'user-1', fileId: 'file-1'}).go()
  ).rejects.toThrow('The conditional request failed')
})
```

#### Update Non-Existent

```typescript
test('update non-existent - throws error', async () => {
  await expect(
    Files.update({fileId: 'non-existent'}).set({status: 'Downloaded'}).go()
  ).rejects.toThrow()
})
```

## Entity Reference

### Files
- **Primary**: `{fileId}`
- **Indexes**: byStatus, byUser (via UserFiles), byKey
- **Attributes**: status, size, url, title, publishDate, etc.

### Users
- **Primary**: `{userId}`
- **Indexes**: byUser, byDevice (via UserDevices)
- **Attributes**: appleDeviceIdentifier, email, name, etc.

### Devices
- **Primary**: `{deviceId}`
- **Indexes**: byDevice, byUser (via UserDevices)
- **Attributes**: deviceName, pushToken, platform, etc.

### UserFiles (Junction)
- **Primary**: `{userId, fileId}`
- **Indexes**: byUser, byFile
- **Purpose**: Many-to-many relationship

### UserDevices (Junction)
- **Primary**: `{userId, deviceId}`
- **Indexes**: byUser, byDevice
- **Purpose**: Many-to-many relationship

### Sessions (Better Auth)
- **Primary**: `{sessionId}`
- **Indexes**: byUser (gsi1), byDevice (gsi2)
- **Attributes**: userId, deviceId, token, expiresAt, ipAddress, userAgent, createdAt, updatedAt
- **Purpose**: Manage active user sessions with device tracking

### Accounts (Better Auth)
- **Primary**: `{accountId}`
- **Indexes**: byUser (gsi1), byProvider (gsi2)
- **Attributes**: userId, providerId, providerAccountId, accessToken, refreshToken, expiresAt, scope, tokenType, idToken
- **Purpose**: Link users to OAuth providers (Apple, Google, etc.)

### VerificationTokens (Better Auth)
- **Primary**: `{token}`
- **Indexes**: byIdentifier (gsi1)
- **Attributes**: identifier, expiresAt
- **Purpose**: Email verification and password reset tokens

### Users (Better Auth)
- **Primary**: `{userId}`
- **Indexes**: byEmail (gsi3)
- **Attributes**: email, emailVerified, firstName, lastName, identityProviders
- **Purpose**: Core user account data

## Best Practices

‚úÖ Use `createElectroDBEntityMock` for unit tests (fast, isolated)
‚úÖ Use LocalStack for integration tests (real DynamoDB operations)
‚úÖ Test Collections to validate single-table design
‚úÖ Test batch operations with realistic data volumes
‚úÖ Cover edge cases (empty results, duplicates, non-existent records)
‚úÖ Use pagination for large result sets
‚úÖ Verify GSI queries return correct data
‚úÖ Test conditional operations (idempotency)

‚ùå Don't test ElectroDB library itself (trust the library)
‚ùå Don't create integration tests for every query (unit tests for most)
‚ùå Don't mock DynamoDB clients directly (use electrodb-mock)

## Related Patterns

- [Fixture Extraction](Fixture-Extraction.md)
- [LocalStack Testing](../Integration/LocalStack-Testing.md)
- [Jest ESM Mocking Strategy](Jest-ESM-Mocking-Strategy.md)

---

*Validate single-table design with Collections. Trust ElectroDB for type safety.*
</file>

<file path="docs/wiki/Testing/Fixture-Extraction.md">
# Fixture Extraction

## Quick Reference
- **When to use**: Automated test fixture generation from production
- **Enforcement**: Always enabled (logs to CloudWatch, extract when needed)
- **Impact if violated**: LOW - Manual fixture maintenance

## Overview

Automatic extraction of production API requests/responses from CloudWatch logs for use as test fixtures. Transforms production reality into test truth weekly via GitHub Actions.

## Architecture

```
Production Lambda ‚Üí CloudWatch Logs ‚Üí extract-fixtures.sh ‚Üí process-fixtures.js ‚Üí test/fixtures/
```

### Workflow
1. **Production logging**: Lambda marks requests/responses with `__FIXTURE_MARKER__`
2. **Weekly extraction**: GitHub Actions queries CloudWatch (last 7 days)
3. **Processing**: Deduplicate, sanitize PII, format for tests
4. **PR creation**: Automated PR with fixture updates for review

## Fixture Logging Implementation

Fixture logging is always enabled in instrumented Lambdas. Add logging calls to capture requests/responses:

```typescript
import {logIncomingFixture, logOutgoingFixture} from '../../../util/lambda-helpers'

export const handler = withXRay(async (event, context) => {
  logIncomingFixture(event, 'webhook-feedly')

  // ... handler logic
  const result = response(context, 200, {status: 'Success'})

  logOutgoingFixture(result, 'webhook-feedly')
  return result
})
```

**Automatic PII sanitization**: Redacts Authorization, tokens, passwords, apiKey, secret, appleDeviceIdentifier

## Manual Extraction

```bash
# Extract from last 7 days
pnpm run extract-fixtures

# Extract from last 14 days
./bin/extract-fixtures.sh 14

# Process and deduplicate
pnpm run process-fixtures
```

**Output**: `test/fixtures/api-contracts/{LambdaName}/incoming.json`, `outgoing.json`

## GitHub Actions Automation (Planned)

The extraction pipeline is automation-ready. A workflow can be added at `.github/workflows/extract-fixtures.yml`:

```yaml
# Example workflow configuration
on:
  schedule:
    - cron: '0 2 * * 0'  # Weekly
  workflow_dispatch:      # Manual trigger
```

**Current Process** (manual):
1. Run `pnpm run extract-fixtures` locally
2. Run `pnpm run process-fixtures` to deduplicate
3. Review and commit fixture updates

**Automated Process** (when workflow is added):
1. Extract fixtures from CloudWatch
2. Deduplicate by structural similarity (90% threshold)
3. Create PR with updated fixtures
4. Requires manual review before merge

## Deduplication Strategy

**Structural similarity** (not exact match):
- Compares object structure, not values
- 90% similarity threshold
- Prevents 1000 identical fixtures with different IDs
- Keeps diverse edge cases

```javascript
// These are considered similar (deduplicated):
{userId: "user-1", status: "Downloaded"}
{userId: "user-2", status: "Downloaded"}

// These are kept (different structure):
{userId: "user-1", status: "Downloaded"}
{userId: "user-1", status: "Failed", error: "Network timeout"}
```

## Instrumented Lambdas

All 7 API Gateway Lambdas have fixture logging enabled:
- ListFiles
- LoginUser
- RefreshToken
- RegisterDevice
- UserDelete
- UserSubscribe
- WebhookFeedly

**Extraction script configured for** (in `bin/extract-fixtures.sh`):
- WebhookFeedly, ListFiles, RegisterDevice, LoginUser, StartFileUpload, SendPushNotification

**Add more**: Edit `LAMBDA_FUNCTIONS` array in `bin/extract-fixtures.sh`

## Security

### PII Sanitization

Automatically redacted fields:
- `Authorization` / `authorization`
- `token` / `Token`
- `password` / `Password`
- `apiKey` / `ApiKey`
- `secret` / `Secret`
- `appleDeviceIdentifier`

Recursive processing handles nested objects/arrays.

### Production Safety
- ‚úÖ No performance impact (async logging)
- ‚úÖ CloudWatch costs: ~$5.50/year
- ‚úÖ Manual PR review before merging

## Cost Analysis

**CloudWatch Logs Insights**:
- $0.005 per GB ingested
- $0.005 per GB scanned in queries
- ~10KB per fixture √ó 50 fixtures/week = 500KB/week = 26MB/year
- **Total**: ~$1.30/year ingestion + $4.20/year scanning = **$5.50/year**

## Troubleshooting

### No Fixtures Extracted

1. Verify Lambda has `logIncomingFixture`/`logOutgoingFixture` calls
2. Check Lambda was invoked in time window
3. Verify CloudWatch log group exists
4. Check log retention (default 30 days)

```bash
# Check log group exists
aws logs describe-log-groups --log-group-name-prefix /aws/lambda/WebhookFeedly

# Check recent log events
aws logs tail /aws/lambda/WebhookFeedly --since 1h
```

### Sensitive Data in Fixtures

1. Delete affected fixture files immediately
2. Add field to `sensitiveFields` array in `lambda-helpers.ts`
3. Re-run extraction
4. Audit git history if needed

```bash
# Remove from git history if committed
git filter-branch --force --index-filter \
  'git rm --cached --ignore-unmatch test/fixtures/api-contracts/WebhookFeedly/incoming.json' \
  --prune-empty --tag-name-filter cat -- --all
```

### GitHub Actions Fails

1. Verify AWS credentials in repository secrets:
   - `AWS_ACCESS_KEY_ID`
   - `AWS_SECRET_ACCESS_KEY`
2. Check IAM permissions (logs:FilterLogEvents)
3. Review workflow logs: `gh run view --log`

## Best Practices

‚úÖ Enable fixture logging in production only (not staging/dev)
‚úÖ Review PRs for sensitive data before merging
‚úÖ Add new Lambdas to extraction list as they're created
‚úÖ Keep fixture count manageable (~5-10 per endpoint)
‚úÖ Use fixtures for API contract tests, not unit tests

## Related Patterns

- [ElectroDB Testing Patterns](ElectroDB-Testing-Patterns.md)
- [Integration Testing](Integration-Testing.md)
- [Coverage Philosophy](Coverage-Philosophy.md)

---

*Use production data as test oracle. Weekly extraction keeps fixtures current.*
</file>

<file path="docs/wiki/Testing/Integration-Testing.md">
# Integration Testing

## Quick Reference
- **When to use**: Testing AWS service interactions
- **Enforcement**: Required for AWS changes
- **Impact if violated**: HIGH - Production issues

## LocalStack Setup

```bash
# Start LocalStack
npm run localstack:start

# Run integration tests
npm run test:integration

# Full suite with lifecycle
npm run test:integration:full
```

## Test Pattern

```typescript
// test/integration/dynamodb.test.ts
import {beforeAll, afterAll, test} from '@jest/globals'
import {setupLocalStack, teardownLocalStack} from '../helpers'

beforeAll(async () => {
  process.env.UseLocalstack = 'true'
  await setupLocalStack()
})

afterAll(async () => {
  await teardownLocalStack()
})

test('DynamoDB operations', async () => {
  const result = await queryItems({userId: 'test'})
  expect(result).toBeDefined()
})
```

## Service Testing

### DynamoDB
```typescript
const items = await Files.query.byUser({userId}).go()
```

### S3
```typescript
await createS3Upload('bucket', 'key', Buffer.from('data'))
```

### Lambda
```typescript
const result = await lambda.invoke({
  FunctionName: 'ProcessFile',
  Payload: JSON.stringify({fileId: 'test'})
})
```

## Docker Compose

```yaml
services:
  localstack:
    image: localstack/localstack:latest
    ports:
      - "4566:4566"
    environment:
      - SERVICES=s3,dynamodb,lambda,sns,sqs
```

## Best Practices

‚úÖ Use vendor wrappers (auto-detect LocalStack)
‚úÖ Clean state between tests
‚úÖ Mock external APIs
‚úÖ Test error cases
‚úÖ Verify AWS operations

## Common Issues

| Issue | Fix |
|-------|-----|
| Connection refused | Start LocalStack |
| Service unavailable | Check SERVICES env |
| State pollution | Clean between tests |

## Related Patterns

- [LocalStack Testing](../Integration/LocalStack-Testing.md)
- [Jest ESM Mocking](Jest-ESM-Mocking-Strategy.md)

---

*Test AWS integrations locally with LocalStack. Vendor wrappers auto-detect environment.*
</file>

<file path="docs/wiki/Testing/Lazy-Initialization-Pattern.md">
# Lazy Initialization Pattern

## Quick Reference
- **When to use**: Modules that create AWS SDK clients or external connections at module level
- **Enforcement**: Required - prevents test failures from premature initialization
- **Impact if violated**: HIGH - Tests fail with SDK errors despite mocking

## The Problem

AWS SDK clients and external connections initialized at module level execute immediately when the module loads, **before** mocks are set up.

## Core Pattern

### ‚ùå Incorrect - Eager Initialization
```typescript
// ‚ùå WRONG - Client created at module level
import {S3Client} from '@aws-sdk/client-s3'

const s3Client = new S3Client({region: 'us-west-2'})  // Runs immediately!

export async function headObject(bucket: string, key: string) {
  return await s3Client.send(command)  // Too late to mock
}
```

### ‚úÖ Correct - Lazy Initialization
```typescript
// ‚úÖ Client is null initially
let s3Client: S3Client | null = null

// ‚úÖ Initialize only when first used
function getS3Client(): S3Client {
  if (!s3Client) {
    s3Client = new S3Client({region: process.env.AWS_REGION || 'us-west-2'})
  }
  return s3Client
}

export async function headObject(bucket: string, key: string) {
  const client = getS3Client()  // Created on first call
  return await client.send(new HeadObjectCommand({Bucket: bucket, Key: key}))
}

export function resetS3Client(): void {
  s3Client = null
}
```

## Common Implementations

### With X-Ray Integration
```typescript
import {DynamoDBClient} from '@aws-sdk/client-dynamodb'
import {DynamoDBDocumentClient} from '@aws-sdk/lib-dynamodb'
import {captureAWSClient} from './XRay'

let dynamoClient: DynamoDBDocumentClient | null = null

function getDynamoClient(): DynamoDBDocumentClient {
  if (!dynamoClient) {
    const client = new DynamoDBClient({region: process.env.AWS_REGION || 'us-west-2'})
    dynamoClient = DynamoDBDocumentClient.from(captureAWSClient(client))
  }
  return dynamoClient
}

export async function query(tableName: string, key: string, value: string) {
  const client = getDynamoClient()
  // Use client...
}

export function resetDynamoClient(): void {
  dynamoClient = null
}
```

### With LocalStack Configuration
```typescript
let s3Client: S3Client | null = null

interface S3Config {
  region?: string
  endpoint?: string
  forcePathStyle?: boolean
}

function getS3Client(config?: S3Config): S3Client {
  if (!s3Client) {
    const isLocalStack = process.env.USE_LOCALSTACK === 'true'
    s3Client = new S3Client({
      region: config?.region || process.env.AWS_REGION || 'us-west-2',
      endpoint: config?.endpoint || (isLocalStack ? 'http://localhost:4566' : undefined),
      forcePathStyle: config?.forcePathStyle || isLocalStack
    })
  }
  return s3Client
}

export function configureS3Client(config: S3Config): void {
  s3Client = null
  getS3Client(config)
}
```

### ElectroDB Service
```typescript
import {Service} from 'electrodb'
import {Users, Files, Devices} from '../../src/entities'

let service: Service | null = null

export function getService(): Service {
  if (!service) {
    service = new Service({Users, Files, Devices})
  }
  return service
}

export const table = process.env.TABLE_NAME || 'MediaDownloader'
export function resetService(): void { service = null }
```

## Testing Benefits

### Works with Mocks
```typescript
// Mock BEFORE importing handler
jest.unstable_mockModule('../../../lib/vendor/AWS/S3', () => ({
  headObject: jest.fn<() => Promise<{ContentLength: number}>>()
    .mockResolvedValue({ContentLength: 1024}),
  resetS3Client: jest.fn()
}))

const {handler} = await import('../src/index')

describe('GetFile handler', () => {
  it('gets file info', async () => {
    const result = await handler(event, context)
    expect(result.statusCode).toBe(200)  // Works!
  })
})
```

## Common Mistakes

### Creating Client at Module Level
```typescript
// ‚ùå WRONG
const s3 = new S3Client({region: 'us-west-2'})

// ‚úÖ CORRECT
let s3: S3Client | null = null
function getS3Client() {
  if (!s3) s3 = new S3Client({region: 'us-west-2'})
  return s3
}
```

### Forgetting Reset Function
```typescript
// ‚ùå INCOMPLETE
let client: SomeClient | null = null
function getClient() {
  if (!client) client = new SomeClient()
  return client
}

// ‚úÖ COMPLETE
let client: SomeClient | null = null
function getClient() {
  if (!client) client = new SomeClient()
  return client
}
export function resetClient() { client = null }
```

## Why This Pattern?

1. **Testability** - Mocks applied before client creation
2. **Environment Safety** - Client only created when needed
3. **Configuration Flexibility** - Can apply config before initialization
4. **LocalStack Support** - Can point to LocalStack endpoint
5. **Minimal Overhead** - Getter function adds negligible cost

## Enforcement

```bash
# Check for eager initialization
grep -rn "= new.*Client({" lib/vendor/AWS/*.ts | grep -v "function\|if ("
```

## Related Patterns
- [Jest ESM Mocking Strategy](Jest-ESM-Mocking-Strategy.md) - Mocking before imports
- [LocalStack Testing](../Integration/LocalStack-Testing.md) - Testing with LocalStack
- [AWS SDK Encapsulation Policy](../AWS/SDK-Encapsulation-Policy.md) - Vendor wrapper pattern

---

*Defer client initialization until first use. This enables proper mocking in tests and supports flexible configuration.*
</file>

<file path="docs/wiki/TypeScript/Module-Best-Practices.md">
# Module Best Practices

## Quick Reference
- **When to use**: Structuring TypeScript modules
- **Enforcement**: Required
- **Impact if violated**: MEDIUM - Circular dependencies

## Module Rules

1. **One primary export per file**
2. **Use named exports** (not default)
3. **Barrel files for public APIs**
4. **Avoid circular dependencies**

## Export Patterns

### ‚úÖ Named Exports (Preferred)
```typescript
// util/transformers.ts
export function toCamelCase(obj: Record<string, any>) {}
export function toSnakeCase(obj: Record<string, any>) {}

// Usage
import {toCamelCase, toSnakeCase} from './transformers'
```

### ‚ùå Default Exports (Avoid)
```typescript
// Harder to refactor
export default class Processor {}
```

## Barrel Files

```typescript
// entities/index.ts
export {Files} from './Files'
export {Users} from './Users'
export {collections} from './Collections'

// Usage
import {Files, Users, collections} from '../entities'
```

## Vendor Wrapper Pattern

```typescript
// lib/vendor/AWS/S3.ts
import {S3Client} from '@aws-sdk/client-s3'

let client: S3Client | null = null

function getS3Client(): S3Client {
  if (!client) {
    client = new S3Client()
  }
  return client
}

// Export only functions, not SDK
export function uploadToS3(bucket: string, key: string) {
  const client = getS3Client()
  // Use client
}
```

## Lazy Initialization

```typescript
let expensive: Resource | null = null

export function getResource(): Resource {
  if (!expensive) {
    expensive = createExpensive()
  }
  return expensive
}
```

## Type-Only Imports

```typescript
// Prefer type imports when possible
import type {FileData} from '../types/main'
import {processFile} from '../processors'
```

## Best Practices

‚úÖ Named exports only
‚úÖ One concern per module
‚úÖ Vendor wrappers for external libs
‚úÖ Lazy init expensive resources
‚úÖ Type-only imports when possible
‚úÖ Clear module boundaries

## Related Patterns

- [Import Organization](../Conventions/Import-Organization.md)
- [Type Definitions](Type-Definitions.md)
- [Lambda Function Patterns](Lambda-Function-Patterns.md)

---

*Use named exports and maintain clear module boundaries.*
</file>

<file path="docs/wiki/TypeScript/TypeScript-Error-Handling.md">
# Error Handling

## Quick Reference
- **When to use**: All Lambda functions and error-prone operations
- **Enforcement**: Required - consistent error handling across all functions
- **Impact if violated**: HIGH - Poor user experience, inconsistent error responses

## The Rules

### API Gateway Lambdas: Always Return Response

**NEVER throw errors in API Gateway Lambda handlers.**

API Gateway Lambdas must ALWAYS return a well-formed response object, even for errors. Throwing an error returns a 502 Bad Gateway to the client.

### Event-Driven Lambdas: Throw Errors for Retries

**DO throw errors in event-driven Lambdas.**

Event-driven Lambdas (SNS, SQS, EventBridge) should throw errors to trigger automatic retries and DLQ processing.

## Examples

### ‚úÖ Correct - API Gateway Error Handling

```typescript
// src/lambdas/ListFiles/src/index.ts
import {APIGatewayProxyResult, Context} from 'aws-lambda'
import {lambdaErrorResponse, response, getUserDetailsFromEvent} from '../../../util/lambda-helpers'
import {withXRay} from '../../../lib/vendor/AWS/XRay'

export const handler = withXRay(async (event, context, {traceId}): Promise<APIGatewayProxyResult> => {
  const {userId, userStatus} = getUserDetailsFromEvent(event)

  // Return 401 for unauthenticated users
  if (userStatus == UserStatus.Unauthenticated) {
    return lambdaErrorResponse(context, generateUnauthorizedError())
  }

  try {
    const files = await getFilesByUser(userId as string)
    // Return 200 with data
    return response(context, 200, {contents: files, keyCount: files.length})
  } catch (error) {
    // Return error response, don't throw
    return lambdaErrorResponse(context, error)
  }
})
```

### ‚úÖ Correct - Event-Driven Error Handling

```typescript
// src/lambdas/S3ObjectCreated/src/index.ts
import {logError, logInfo} from '../../../util/lambda-helpers'
import {withXRay} from '../../../lib/vendor/AWS/XRay'
import {S3Event, Context} from 'aws-lambda'

export const handler = withXRay(async (event: S3Event, context: Context, {traceId}) => {
  logInfo('S3ObjectCreated triggered', event)

  try {
    for (const record of event.Records) {
      await processS3Record(record)
    }
    return {statusCode: 200}
  } catch (error) {
    logError('Failed to process S3 event', error)
    // Throw to trigger retry/DLQ
    throw error
  }
})
```

### ‚ùå Incorrect - Throwing in API Gateway Lambda

```typescript
// ‚ùå WRONG - Throws error, returns 502 to client
export const handler = async (event, context) => {
  const result = await riskyOperation()  // Might throw
  return response(context, 200, result)
}

// ‚ùå WRONG - Throws instead of returning error response
export const handler = async (event, context) => {
  if (!event.body) {
    throw new Error('Missing body')  // Returns 502!
  }
}
```

### ‚ùå Incorrect - Not Throwing in Event-Driven Lambda

```typescript
// ‚ùå WRONG - Swallows error, no retry triggered
export const handler = async (event, context) => {
  try {
    await processEvent(event)
  } catch (error) {
    console.error(error)
    return {statusCode: 500}  // Event appears "successful"
  }
}
```

## Input Validation Errors

### API Gateway: Return 400 Bad Request

```typescript
import {getPayloadFromEvent, validateRequest} from '../../../util/apigateway-helpers'
import {lambdaErrorResponse, response} from '../../../util/lambda-helpers'

export const handler = withXRay(async (event, context, {traceId}) => {
  let requestBody
  try {
    requestBody = getPayloadFromEvent(event)
    validateRequest(requestBody, registerDeviceSchema)
  } catch (error) {
    // Validation errors return 400 via lambdaErrorResponse
    return lambdaErrorResponse(context, error)
  }

  try {
    const device = await registerDevice(requestBody)
    return response(context, 200, {endpointArn: device.endpointArn})
  } catch (error) {
    return lambdaErrorResponse(context, error)
  }
})
```

## HTTP Status Codes

Use the `response` helper from lambda-helpers with appropriate status codes:

```typescript
// 200 - Success with data
return response(context, 200, {contents: files})

// 201 - Created
return response(context, 201, {endpointArn: device.endpointArn})

// 400 - Bad Request (via lambdaErrorResponse)
throw new BadRequestError('Invalid parameters')

// 401 - Unauthorized
return lambdaErrorResponse(context, generateUnauthorizedError())

// 404 - Not Found
throw new NotFoundError('Resource not found')

// 500 - Internal Server Error
return lambdaErrorResponse(context, error)
```

## Error Logging

Always use `logError` from lambda-helpers:

```typescript
import {logError} from '../../../util/lambda-helpers'

try {
  await operation()
} catch (error) {
  logError(error, {
    context: 'operation',
    traceId,
    userId: event.userId
  })

  // Handle appropriately for Lambda type
}
```

## Enforcement

### Code Review Checklist

- [ ] API Gateway Lambdas NEVER throw errors
- [ ] API Gateway Lambdas return proper status codes
- [ ] Event-driven Lambdas throw errors on failure
- [ ] All errors logged with `logError()`
- [ ] Input validation returns 400 for API Gateway

## Related Patterns

- [Lambda Function Patterns](Lambda-Function-Patterns.md) - Handler structure
- [CloudWatch Logging](../AWS/CloudWatch-Logging.md) - Structured logging

---

*Handle errors appropriately for your Lambda invocation type. API Gateway Lambdas return error responses, event-driven Lambdas throw errors for retries.*
</file>

<file path="docs/doc-code-mapping.json">
{
  "$schema": "./doc-code-mapping.schema.json",
  "version": "1.0.0",
  "description": "Maps documentation claims to filesystem paths for validation. Used by bin/validate-doc-sync.sh",

  "entityValidation": {
    "documentedIn": "AGENTS.md",
    "sourceDirectory": "src/entities",
    "pattern": "*.ts",
    "excludePatterns": ["*.test.ts", "index.ts"],
    "assertion": "count-match",
    "description": "Entity count in AGENTS.md project structure must match src/entities/*.ts files"
  },

  "lambdaValidation": {
    "documentedIn": "AGENTS.md",
    "sourceDirectory": "src/lambdas",
    "tableSection": "### Lambda Trigger Patterns",
    "assertion": "count-match",
    "description": "Lambda count in trigger table must match src/lambdas/* directories"
  },

  "mcpRuleValidation": {
    "documentedIn": ["src/mcp/validation/index.ts", "docs/wiki/MCP/Convention-Tools.md"],
    "sourceDirectory": "src/mcp/validation/rules",
    "pattern": "*.ts",
    "excludePatterns": ["*.test.ts", "index.ts", "types.ts"],
    "assertion": "count-match",
    "description": "MCP validation rule files must match registered rules in index.ts"
  },

  "pathValidations": [
    {
      "documentedPath": "src/lib/vendor/AWS/",
      "mustExist": true,
      "documentedIn": ["AGENTS.md", "docs/wiki/AWS/SDK-Encapsulation-Policy.md"]
    },
    {
      "documentedPath": "src/lib/vendor/BetterAuth/",
      "mustExist": true,
      "documentedIn": ["AGENTS.md"]
    },
    {
      "documentedPath": "src/lib/vendor/ElectroDB/",
      "mustExist": true,
      "documentedIn": ["AGENTS.md"]
    },
    {
      "documentedPath": "src/mcp/",
      "mustExist": true,
      "documentedIn": ["AGENTS.md"]
    },
    {
      "documentedPath": "src/mcp/validation/",
      "mustExist": true,
      "documentedIn": ["AGENTS.md", "docs/wiki/MCP/Convention-Tools.md"]
    },
    {
      "documentedPath": "test/helpers/",
      "mustExist": true,
      "documentedIn": ["AGENTS.md", "docs/wiki/Testing/ElectroDB-Testing-Patterns.md"]
    },
    {
      "documentedPath": "graphrag/",
      "mustExist": true,
      "documentedIn": ["AGENTS.md"]
    }
  ],

  "contentValidations": [
    {
      "file": "AGENTS.md",
      "mustContain": "dprint",
      "mustNotContain": "Prettier",
      "description": "AGENTS.md must reference dprint (not Prettier)"
    },
    {
      "file": "AGENTS.md",
      "mustContain": "src/lib/vendor/",
      "description": "AGENTS.md must use correct vendor path with src/ prefix"
    }
  ],

  "relationshipValidations": [
    {
      "source": "graphrag/metadata.json",
      "target": "src/entities",
      "assertion": "entities-documented",
      "description": "All entities in src/entities must appear in graphrag metadata entityRelationships"
    }
  ],

  "wikiLinkValidation": {
    "directory": "docs/wiki",
    "pattern": "**/*.md",
    "assertion": "links-resolve",
    "description": "All relative markdown links in wiki must resolve to existing files"
  },

  "autoFixSuggestions": {
    "entityCountMismatch": {
      "command": "find src/entities -name '*.ts' ! -name '*.test.ts' ! -name 'index.ts' -exec basename {} \\;",
      "action": "Update AGENTS.md project structure to include all entity files",
      "severity": "HIGH"
    },
    "lambdaCountMismatch": {
      "command": "find src/lambdas -mindepth 1 -maxdepth 1 -type d -exec basename {} \\;",
      "action": "Update AGENTS.md Lambda Trigger Patterns table",
      "severity": "HIGH"
    },
    "graphragMissing": {
      "command": "pnpm run graphrag:extract",
      "action": "Regenerate GraphRAG knowledge graph",
      "severity": "MEDIUM"
    },
    "stalePattern": {
      "patterns": {
        "Prettier": "dprint",
        "lib/vendor/AWS": "src/lib/vendor/AWS"
      },
      "action": "Replace outdated references in documentation",
      "severity": "HIGH"
    }
  }
}
</file>

<file path="docs/doc-code-mapping.schema.json">
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "doc-code-mapping.schema.json",
  "title": "Documentation-Code Mapping",
  "description": "Schema for mapping documentation assertions to filesystem paths for validation",
  "type": "object",
  "required": ["version", "description"],
  "properties": {
    "$schema": {
      "type": "string",
      "description": "Reference to this schema file"
    },
    "version": {
      "type": "string",
      "pattern": "^\\d+\\.\\d+\\.\\d+$",
      "description": "Semantic version of the mapping file"
    },
    "description": {
      "type": "string",
      "description": "Human-readable description of the mapping file purpose"
    },
    "entityValidation": {
      "$ref": "#/definitions/countValidation",
      "description": "Validates entity file count matches documentation"
    },
    "lambdaValidation": {
      "$ref": "#/definitions/countValidation",
      "description": "Validates Lambda directory count matches documentation"
    },
    "mcpRuleValidation": {
      "$ref": "#/definitions/countValidation",
      "description": "Validates MCP validation rule count matches documentation"
    },
    "pathValidations": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/pathValidation"
      },
      "description": "List of paths that must exist in filesystem"
    },
    "contentValidations": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/contentValidation"
      },
      "description": "Content pattern validations for documentation files"
    },
    "relationshipValidations": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/relationshipValidation"
      },
      "description": "Cross-file relationship validations"
    },
    "wikiLinkValidation": {
      "$ref": "#/definitions/wikiLinkValidation",
      "description": "Wiki internal link validation configuration"
    },
    "autoFixSuggestions": {
      "type": "object",
      "additionalProperties": {
        "$ref": "#/definitions/autoFixSuggestion"
      },
      "description": "Auto-fix suggestions for common validation failures"
    }
  },
  "definitions": {
    "countValidation": {
      "type": "object",
      "required": ["documentedIn", "sourceDirectory", "assertion"],
      "properties": {
        "documentedIn": {
          "oneOf": [
            { "type": "string" },
            { "type": "array", "items": { "type": "string" } }
          ],
          "description": "File(s) where the count is documented"
        },
        "sourceDirectory": {
          "type": "string",
          "description": "Directory to count items from"
        },
        "pattern": {
          "type": "string",
          "description": "Glob pattern for files to count"
        },
        "excludePatterns": {
          "type": "array",
          "items": { "type": "string" },
          "description": "Patterns to exclude from count"
        },
        "tableSection": {
          "type": "string",
          "description": "Markdown section header containing the count table"
        },
        "assertion": {
          "type": "string",
          "enum": ["count-match"],
          "description": "Type of assertion to perform"
        },
        "description": {
          "type": "string",
          "description": "Human-readable description of this validation"
        }
      }
    },
    "pathValidation": {
      "type": "object",
      "required": ["documentedPath", "mustExist", "documentedIn"],
      "properties": {
        "documentedPath": {
          "type": "string",
          "description": "Path that is referenced in documentation"
        },
        "mustExist": {
          "type": "boolean",
          "description": "Whether the path must exist in filesystem"
        },
        "documentedIn": {
          "type": "array",
          "items": { "type": "string" },
          "description": "Documentation files that reference this path"
        }
      }
    },
    "contentValidation": {
      "type": "object",
      "required": ["file"],
      "properties": {
        "file": {
          "type": "string",
          "description": "File to check content in"
        },
        "mustContain": {
          "type": "string",
          "description": "Pattern that must be present in the file"
        },
        "mustNotContain": {
          "type": "string",
          "description": "Pattern that must NOT be present in the file"
        },
        "description": {
          "type": "string",
          "description": "Human-readable description of this validation"
        }
      }
    },
    "relationshipValidation": {
      "type": "object",
      "required": ["source", "target", "assertion"],
      "properties": {
        "source": {
          "type": "string",
          "description": "Source file to check"
        },
        "target": {
          "type": "string",
          "description": "Target directory or file to validate against"
        },
        "assertion": {
          "type": "string",
          "enum": ["entities-documented", "lambdas-documented", "all-referenced"],
          "description": "Type of relationship assertion"
        },
        "description": {
          "type": "string",
          "description": "Human-readable description of this validation"
        }
      }
    },
    "wikiLinkValidation": {
      "type": "object",
      "required": ["directory", "pattern", "assertion"],
      "properties": {
        "directory": {
          "type": "string",
          "description": "Root directory for wiki files"
        },
        "pattern": {
          "type": "string",
          "description": "Glob pattern for wiki files"
        },
        "assertion": {
          "type": "string",
          "enum": ["links-resolve"],
          "description": "Type of link assertion"
        },
        "description": {
          "type": "string",
          "description": "Human-readable description"
        }
      }
    },
    "autoFixSuggestion": {
      "type": "object",
      "required": ["action", "severity"],
      "properties": {
        "command": {
          "type": "string",
          "description": "Shell command to help diagnose or fix the issue"
        },
        "patterns": {
          "type": "object",
          "additionalProperties": { "type": "string" },
          "description": "Find/replace patterns for stale content"
        },
        "action": {
          "type": "string",
          "description": "Human-readable action to take"
        },
        "severity": {
          "type": "string",
          "enum": ["CRITICAL", "HIGH", "MEDIUM", "LOW"],
          "description": "Severity of the issue"
        }
      }
    }
  }
}
</file>

<file path="docs/MCP-GUIDE.md">
# MCP (Model Context Protocol) Guide

## Overview

The Model Context Protocol (MCP) is an open protocol that standardizes how AI assistants interact with external data sources and tools. For this project, we've implemented an MCP server that provides structured, queryable access to our codebase architecture.

## Why MCP?

Traditional approaches to sharing codebase context with AI involve:
- Copying entire files into the conversation (wastes tokens)
- Manually describing architecture (error-prone, gets stale)
- Creating static documentation (requires constant updates)

MCP solves these problems by providing:
- **Dynamic Queries**: AI asks specific questions, gets specific answers
- **Structured Data**: JSON responses instead of parsing code
- **Zero Token Waste**: Only relevant information is returned
- **Always Current**: Queries run against actual codebase state

## How It Works

```mermaid
graph LR
    A[Claude/AI] -->|MCP Query| B[MCP Server]
    B -->|Read| C[graph.json]
    B -->|Read| D[Entity Schemas]
    B -->|Read| E[Lambda Configs]
    B -->|Read| F[Infrastructure]
    B -->|JSON Response| A
```

The MCP server acts as an intelligent API for your codebase, translating high-level queries into structured responses.

## Available Query Types

### 1. Entity Queries
- **Schema**: Get ElectroDB entity definitions
- **Relationships**: Understand entity associations
- **Collections**: Learn about JOIN-like queries

### 2. Lambda Queries
- **List**: Get all Lambda functions
- **Config**: Memory, timeout, runtime settings
- **Triggers**: What invokes each Lambda
- **Dependencies**: Libraries and AWS services used
- **Environment**: Required environment variables

### 3. Infrastructure Queries
- **Config**: AWS resource configurations
- **Usage**: Which Lambdas use which resources
- **Dependencies**: Service interdependencies

### 4. Dependency Queries
- **Imports**: Direct file imports
- **Transitive**: All dependencies (for mocking)
- **Dependents**: Who imports a file
- **Circular**: Detect circular dependencies

## Setup Instructions

### 1. Install MCP Server

The MCP server is already installed as part of the project:

```bash
# Verify installation
ls -la src/mcp/server.ts

# Test the server
node --loader ts-node/esm src/mcp/server.ts
```

### 2. Configure Claude Desktop

Add to `~/Library/Application Support/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "media-downloader": {
      "command": "node",
      "args": [
        "--loader",
        "ts-node/esm",
        "/Users/YOUR_USERNAME/path/to/aws-cloudformation-media-downloader-2/src/mcp/server.ts"
      ]
    }
  }
}
```

### 3. Restart Claude Desktop

After configuration, restart Claude Desktop to load the MCP server.

## Usage Examples

### Example 1: Understanding Data Model

```
User: "What entities are in the database and how are they related?"

Claude uses:
- query_entities({ query: "schema" })
- query_entities({ query: "relationships" })

Response: Structured JSON showing all entities and their relationships
```

### Example 2: Lambda Dependencies for Testing

```
User: "What do I need to mock to test the ListFiles Lambda?"

Claude uses:
- query_dependencies({
    file: "src/lambdas/ListFiles/src/index.ts",
    query: "transitive"
  })

Response: Complete list of all dependencies to mock
```

### Example 3: Infrastructure Analysis

```
User: "Which Lambda functions interact with S3?"

Claude uses:
- query_infrastructure({ resource: "s3", query: "usage" })

Response: List of Lambdas and their S3 operations
```

## Best Practices

### 1. Use MCP for Architecture Questions
Instead of: "Read the Users entity file"
Use: "Query MCP for Users entity schema"

### 2. Combine with File Reading When Needed
- Use MCP to understand structure
- Read specific files only for implementation details

### 3. Leverage Dependency Queries for Testing
- Use transitive dependencies to identify all mocks needed
- Check for circular dependencies before refactoring

### 4. Keep Handlers Updated
When adding new Lambdas or entities:
1. Update the relevant handler in `src/mcp/handlers/`
2. Run tests to ensure consistency
3. Document new query patterns

## Troubleshooting

### MCP Server Not Responding

1. Check server is in config:
```bash
cat ~/Library/Application\ Support/Claude/claude_desktop_config.json | grep media-downloader
```

2. Test server directly:
```bash
node --loader ts-node/esm src/mcp/server.ts
# Should output: "MCP Server running on stdio"
```

3. Verify dependencies:
```bash
pnpm list @modelcontextprotocol/sdk
```

### Queries Return Errors

1. Ensure graph.json exists:
```bash
pnpm run generate-graph
```

2. Check handler imports:
```bash
node -e "import('./src/mcp/handlers/electrodb.js')"
```

### Claude Not Finding Tools

1. Restart Claude Desktop after config changes
2. Check for typos in tool names
3. Verify server path is absolute, not relative

## Extending the MCP Server

### Adding a New Query Type

1. Define tool in `server.ts`:
```typescript
{
  name: 'query_new_thing',
  description: 'Query new thing',
  inputSchema: { /* ... */ }
}
```

2. Add handler case:
```typescript
case 'query_new_thing':
  return await handleNewThingQuery(args);
```

3. Create handler file:
```typescript
// src/mcp/handlers/newthing.ts
export async function handleNewThingQuery(args: any) {
  // Implementation
}
```

4. Test with inspector:
```bash
npx @modelcontextprotocol/inspector src/mcp/server.ts
```

## Performance Considerations

- **Caching**: MCP server reads from disk on each query (stateless)
- **Graph.json**: Regenerate with `pnpm run generate-graph` after structural changes
- **Response Size**: Large responses may be truncated by Claude
- **Query Optimization**: Use specific queries over "all" when possible

## Security Notes

- MCP server has read-only access to codebase
- No write operations are exposed
- Runs with same permissions as Claude Desktop
- Consider path restrictions in production deployments

## Future Enhancements

Potential improvements to the MCP server:

1. **Caching Layer**: Cache responses for frequently queried data
2. **GraphQL Interface**: More flexible querying
3. **Real-time Updates**: Watch mode for file changes
4. **Metrics**: Track most common queries for optimization
5. **Schema Validation**: Ensure handler responses match expected format

## Related Documentation

- [MCP Specification](https://modelcontextprotocol.io/docs)
- [Project Architecture](./ARCHITECTURE.md)
- [Dependency Graph](../build/graph.json)
- [MCP Server Implementation](../src/mcp/README.md)
</file>

<file path="eslint-local-rules/rules/authenticated-handler-enforcement.cjs">
/**
 * authenticated-handler-enforcement
 * HIGH: Detects manual getUserDetailsFromEvent + UserStatus checks in Lambda handlers
 * Suggests using wrapAuthenticatedHandler or wrapOptionalAuthHandler instead
 *
 * Mirrors: src/mcp/validation/rules/authenticated-handler-enforcement.ts
 */
‚ãÆ----
function getSuggestion(hasAnonymousCheck, hasUnauthenticatedCheck)
‚ãÆ----
create(context)
‚ãÆ----
// Only apply to Lambda handler files
‚ãÆ----
// Skip test files
‚ãÆ----
// Track if new wrappers are used
ImportDeclaration(node)
‚ãÆ----
// Track UserStatus checks in the file
BinaryExpression(node)
‚ãÆ----
// Detect getUserDetailsFromEvent calls
CallExpression(node)
‚ãÆ----
// Redundant call - wrapper already handles this
‚ãÆ----
// Manual auth handling - suggest using new wrapper
</file>

<file path="eslint-local-rules/rules/cascade-delete-order.cjs">
/**
 * cascade-delete-order
 * WARN: Detect Promise.all with delete operations
 *
 * Mirrors: src/mcp/validation/rules/cascade-safety.ts
 */
‚ãÆ----
create(context)
‚ãÆ----
CallExpression(node)
‚ãÆ----
// Check for Promise.all
</file>

<file path="eslint-local-rules/rules/env-validation.cjs">
/**
 * env-validation
 * CRITICAL: Detect direct process.env access without validated helpers
 *
 * Mirrors: src/mcp/validation/rules/env-validation.ts
 */
‚ãÆ----
// AWS Lambda runtime-provided env vars (always set by AWS, don't need validation)
‚ãÆ----
create(context)
‚ãÆ----
// Only check Lambda source files and utilities
‚ãÆ----
// Skip test files
‚ãÆ----
// Skip the env-validation utility itself
‚ãÆ----
/**
     * Check if node is inside a getRequiredEnv/getOptionalEnv call
     */
function isInsideHelperCall(node)
‚ãÆ----
MemberExpression(node)
‚ãÆ----
// Check for process.env.SOMETHING pattern
‚ãÆ----
// This is process.env.X
‚ãÆ----
// Allow AWS runtime-provided env vars (don't need validation)
‚ãÆ----
// Check for process.env['X'] pattern (computed property)
</file>

<file path="eslint-local-rules/rules/response-helpers.cjs">
/**
 * response-helpers
 * HIGH: Lambda handlers must use response() helper, not raw objects
 *
 * Mirrors: src/mcp/validation/rules/response-helpers.ts
 */
‚ãÆ----
create(context)
‚ãÆ----
// Only check Lambda handler files
‚ãÆ----
// Skip test files
‚ãÆ----
ImportDeclaration(node)
‚ãÆ----
ReturnStatement(node)
‚ãÆ----
// Check for object literal with statusCode property
‚ãÆ----
// If file has statusCode returns but no response import, warn
‚ãÆ----
// This is already caught by the individual return statements
</file>

<file path="eslint-local-rules/rules/use-electrodb-mock-helper.cjs">
/**
 * use-electrodb-mock-helper
 * ERROR: Test files must use createElectroDBEntityMock() helper
 *
 * Mirrors: src/mcp/validation/rules/electrodb-mocking.ts
 */
‚ãÆ----
create(context)
‚ãÆ----
// Only apply to test files
‚ãÆ----
// Skip the helper file itself
‚ãÆ----
// Track variables created with createElectroDBEntityMock
‚ãÆ----
VariableDeclarator(node)
‚ãÆ----
// Track: const fooMock = createElectroDBEntityMock(...)
‚ãÆ----
CallExpression(node)
‚ãÆ----
// Check for jest.unstable_mockModule or jest.mock
‚ãÆ----
// Check if mocking an entity
‚ãÆ----
// Check if mock uses createElectroDBEntityMock directly or via a tracked variable
</file>

<file path="eslint-local-rules/test/cascade-delete-order.test.cjs">
/**
 * Tests for cascade-delete-order ESLint rule
 */
‚ãÆ----
// Allowed: Promise.allSettled with deletes
‚ãÆ----
// Allowed: Promise.all without deletes
‚ãÆ----
// Allowed: Sequential deletes (not in Promise.all)
‚ãÆ----
// Allowed: Promise.all with non-delete operations
‚ãÆ----
// Forbidden: Promise.all with .delete() operations
‚ãÆ----
// Forbidden: Promise.all with .remove() operations
‚ãÆ----
// Forbidden: Promise.all with batchWrite
</file>

<file path="eslint-local-rules/test/env-validation.test.cjs">
/**
 * Tests for env-validation ESLint rule
 */
‚ãÆ----
// Allowed: Using getRequiredEnv helper
‚ãÆ----
// Allowed: Using getOptionalEnv helper
‚ãÆ----
// Allowed: Using getRequiredEnvNumber helper
‚ãÆ----
// Allowed: Test file can use process.env directly
‚ãÆ----
// Allowed: env-validation.ts itself can use process.env
‚ãÆ----
// Allowed: Non-Lambda, non-utility file
‚ãÆ----
// Allowed: AWS runtime env vars (always set by Lambda, don't need validation)
‚ãÆ----
// Forbidden: Direct process.env.X in Lambda
‚ãÆ----
// Forbidden: Direct process.env.X in utility (user-defined env var)
‚ãÆ----
// Forbidden: Multiple direct accesses
</file>

<file path="eslint-local-rules/test/response-helpers.test.cjs">
/**
 * Tests for response-helpers ESLint rule
 */
‚ãÆ----
// Allowed: Using response() helper
‚ãÆ----
// Allowed: Using lambdaErrorResponse() helper
‚ãÆ----
// Allowed: Non-Lambda file
‚ãÆ----
// Allowed: Test file
‚ãÆ----
// Allowed: Return without statusCode
‚ãÆ----
// Forbidden: Raw response object with statusCode and body
‚ãÆ----
// Forbidden: Raw response object with statusCode and headers
‚ãÆ----
// Forbidden: Raw error response
</file>

<file path="eslint-local-rules/test/use-electrodb-mock-helper.test.cjs">
/**
 * Tests for use-electrodb-mock-helper ESLint rule
 */
‚ãÆ----
// Allowed: Using createElectroDBEntityMock in test file
‚ãÆ----
// Allowed: Non-entity mock in test file
‚ãÆ----
// Allowed: Non-test file (rule doesn't apply)
‚ãÆ----
// Allowed: electrodb-mock helper file itself
‚ãÆ----
// Allowed: Mock uses createElectroDBEntityMock (jest.mock)
‚ãÆ----
// Allowed: Variable assigned from createElectroDBEntityMock, then .entity used
‚ãÆ----
// Forbidden: Manual entity mock in test file
‚ãÆ----
// Forbidden: Manual mock for Files entity
‚ãÆ----
// Forbidden: jest.mock with manual entity mock
‚ãÆ----
// Forbidden: Mocking UserFiles without helper
</file>

<file path="graphrag/metadata.schema.json">
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "metadata.schema.json",
  "title": "GraphRAG Metadata",
  "description": "Schema for semantic metadata that cannot be auto-derived from code analysis. Used by graphrag/extract.ts to generate knowledge-graph.json",
  "type": "object",
  "required": ["description", "lambdas", "externalServices", "awsServices", "entityRelationships"],
  "properties": {
    "$schema": {
      "type": "string",
      "description": "Reference to this schema file"
    },
    "description": {
      "type": "string",
      "description": "Human-readable description of this metadata file"
    },
    "lambdas": {
      "type": "object",
      "additionalProperties": {
        "$ref": "#/definitions/lambdaMetadata"
      },
      "description": "Lambda function metadata keyed by function name"
    },
    "externalServices": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/externalService"
      },
      "description": "External services the system integrates with"
    },
    "awsServices": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/awsService"
      },
      "description": "AWS services used by the system"
    },
    "entityRelationships": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/entityRelationship"
      },
      "description": "Relationships between ElectroDB entities"
    },
    "lambdaInvocations": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/lambdaInvocation"
      },
      "description": "Lambda-to-Lambda invocation chains"
    },
    "serviceToServiceEdges": {
      "type": "array",
      "items": {
        "$ref": "#/definitions/serviceEdge"
      },
      "description": "Service-level trigger relationships"
    }
  },
  "definitions": {
    "lambdaMetadata": {
      "type": "object",
      "required": ["trigger", "purpose"],
      "properties": {
        "trigger": {
          "type": "string",
          "enum": [
            "API Gateway",
            "CloudFront",
            "CloudWatch Events",
            "S3 Event",
            "SQS",
            "SNS",
            "Lambda Invoke",
            "Step Functions",
            "EventBridge"
          ],
          "description": "What triggers this Lambda function"
        },
        "purpose": {
          "type": "string",
          "description": "Human-readable description of what this Lambda does"
        }
      }
    },
    "externalService": {
      "type": "object",
      "required": ["name", "type", "description"],
      "properties": {
        "name": {
          "type": "string",
          "description": "Name of the external service"
        },
        "type": {
          "type": "string",
          "enum": ["content", "media", "notification", "auth", "integration", "storage"],
          "description": "Category of the external service"
        },
        "description": {
          "type": "string",
          "description": "What the service is used for"
        }
      }
    },
    "awsService": {
      "type": "object",
      "required": ["name", "type"],
      "properties": {
        "name": {
          "type": "string",
          "description": "AWS service name"
        },
        "type": {
          "type": "string",
          "enum": [
            "database",
            "storage",
            "notification",
            "queue",
            "compute",
            "monitoring",
            "secrets",
            "orchestration",
            "tracing",
            "api",
            "cdn"
          ],
          "description": "Category of the AWS service"
        },
        "vendorPath": {
          "type": ["string", "null"],
          "description": "Path to vendor wrapper in src/lib/vendor/AWS/, or null if no wrapper"
        }
      }
    },
    "entityRelationship": {
      "type": "object",
      "required": ["from", "to", "type"],
      "properties": {
        "from": {
          "type": "string",
          "description": "Source entity name"
        },
        "to": {
          "type": "string",
          "description": "Target entity name"
        },
        "type": {
          "type": "string",
          "enum": ["has_many", "belongs_to", "has_one"],
          "description": "Type of relationship"
        }
      }
    },
    "lambdaInvocation": {
      "type": "object",
      "required": ["from", "to", "via"],
      "properties": {
        "from": {
          "type": "string",
          "description": "Source Lambda function name"
        },
        "to": {
          "type": "string",
          "description": "Target Lambda function name"
        },
        "via": {
          "type": "string",
          "description": "Invocation mechanism (e.g., 'Lambda invokeAsync', 'SQS', 'SNS')"
        }
      }
    },
    "serviceEdge": {
      "type": "object",
      "required": ["from", "to", "relationship"],
      "properties": {
        "from": {
          "type": "string",
          "description": "Source service name"
        },
        "to": {
          "type": "string",
          "description": "Target service name"
        },
        "relationship": {
          "type": "string",
          "enum": ["triggers", "invokes", "reads", "writes"],
          "description": "Type of relationship"
        },
        "event": {
          "type": "string",
          "description": "Specific event type (e.g., 's3:ObjectCreated', 'scheduled')"
        }
      }
    }
  }
}
</file>

<file path="graphrag/query.ts">
/**
 * GraphRAG query interface for multi-hop reasoning
 * Enables complex queries across Lambda chains and entity relationships
 */
import fs from 'fs/promises'
import path from 'path'
import {fileURLToPath} from 'url'
// Type definitions (duplicated to avoid circular imports)
interface Node {
  id: string
  type: 'Lambda' | 'Entity' | 'Service' | 'External'
  properties: Record<string, unknown>
}
interface Edge {
  source: string
  target: string
  relationship: string
  properties?: Record<string, unknown>
}
interface KnowledgeGraph {
  nodes: Node[]
  edges: Edge[]
  metadata: {
    created: string
    version: string
    description: string
  }
}
‚ãÆ----
/**
 * Load knowledge graph from file
 */
async function loadKnowledgeGraph(): Promise<KnowledgeGraph>
/**
 * Multi-hop query functions
 */
export class GraphQuery
‚ãÆ----
constructor(private graph: KnowledgeGraph)
/**
   * Find all paths between two nodes
   */
findPaths(startId: string, endId: string, maxHops: number = 5): string[][]
‚ãÆ----
const dfs = (current: string, path: string[]) =>
‚ãÆ----
/**
   * Find impact of changes to a node (what depends on it)
   */
findImpact(nodeId: string, depth: number = 3): Map<number, string[]>
‚ãÆ----
// Find all nodes that this node affects
‚ãÆ----
/**
   * Find dependencies of a node (what it depends on)
   */
findDependencies(nodeId: string, depth: number = 3): Map<number, string[]>
‚ãÆ----
// Find all nodes that this node depends on
‚ãÆ----
/**
   * Complex query: What happens if a file is deleted?
   * Note: fileId parameter would be used to find specific file relationships
   */
// eslint-disable-next-line @typescript-eslint/no-unused-vars
fileDeleteImpact(_fileId: string):
‚ãÆ----
// Find UserFiles entries for this file (used for determining affected users)
‚ãÆ----
// Find users through UserFiles
‚ãÆ----
// Find Lambdas that access Files entity
‚ãÆ----
// Cascade operations
‚ãÆ----
/**
   * Complex query: What is the data flow for file upload?
   */
fileUploadFlow(): Array<
/**
   * Find circular dependencies
   */
findCircularDependencies(): string[][]
‚ãÆ----
// Found a cycle
‚ãÆ----
/**
   * Community detection: Find clusters of related nodes
   */
findCommunities(): Map<string, string[]>
‚ãÆ----
// Simple community detection based on node types and connections
‚ãÆ----
/**
   * Answer complex questions about the system
   */
answerQuestion(question: string): unknown
‚ãÆ----
/**
 * CLI interface for testing queries
 */
async function main()
‚ãÆ----
// Example queries
‚ãÆ----
// Multi-hop reasoning example
‚ãÆ----
// Impact analysis
‚ãÆ----
// Dependency analysis
‚ãÆ----
// Run if executed directly
</file>

<file path="layers/yt-dlp/VERSION">
2025.11.12
</file>

<file path="packages/.gitkeep">

</file>

<file path="scripts/validateConfig.ts">
/**
 * Configuration validation script for CI
 * Validates config files against project conventions using MCP validation rules
 *
 * Usage: pnpm run validate:config
 */
‚ãÆ----
import {configEnforcementRule} from '../src/mcp/validation/rules/config-enforcement'
import {Project} from 'ts-morph'
‚ãÆ----
async function main()
‚ãÆ----
// File doesn't exist or can't be parsed - skip
</file>

<file path="scripts/visualize-bundles.ts">
/**
 * Generates bundle visualization from esbuild metafiles
 * Creates an HTML report showing bundle composition for all Lambda functions
 */
‚ãÆ----
import {glob} from 'glob'
interface MetafileInput {
  bytes: number
  imports: {path: string; kind: string}[]
}
interface MetafileOutput {
  bytes: number
  inputs: Record<string, {bytesInOutput: number}>
}
interface Metafile {
  inputs: Record<string, MetafileInput>
  outputs: Record<string, MetafileOutput>
}
interface BundleInfo {
  name: string
  totalBytes: number
  inputs: {path: string; bytes: number}[]
}
function formatBytes(bytes: number): string
async function main()
‚ãÆ----
// Get the output file info
‚ãÆ----
// Sort bundles by size
‚ãÆ----
// Generate HTML report
</file>

<file path="src/lambdas/CloudfrontMiddleware/types/index.ts">
import type {CloudFrontResponse, CloudFrontResultResponse} from 'aws-lambda'
import type {CloudFrontCustomOrigin, CloudFrontRequest} from 'aws-lambda/common/cloudfront'
export type CloudFrontHandlerResult = CloudFrontRequest | CloudFrontResultResponse | CloudFrontResponse
export type CustomCloudFrontOrigin = {custom: CloudFrontCustomOrigin}
export interface CustomCloudFrontRequest extends CloudFrontRequest {
  clientIp: string
  origin: CustomCloudFrontOrigin
}
</file>

<file path="src/lambdas/PruneDevices/types/index.ts">
/**
 * Result of the PruneDevices operation
 */
export interface PruneDevicesResult {
  devicesChecked: number
  devicesPruned: number
  errors: string[]
}
export interface ApplePushNotificationResponse {
  statusCode: number
  reason?: string
}
</file>

<file path="src/lambdas/StartFileUpload/src/file-helpers.ts">
/**
 * File Helper Functions
 *
 * Internal utilities for file operations in StartFileUpload Lambda.
 */
import {Files} from '#entities/Files'
import {logDebug} from '#util/logging'
import type {File} from '#types/domain-models'
/**
 * Upsert a File object in DynamoDB
 * @param item - The DynamoDB item to be added
 */
export async function upsertFile(item: File)
</file>

<file path="src/lib/vendor/Powertools/parser.ts">
/**
 * AWS Lambda Powertools Parser integration with Zod
 * Provides middleware-based payload validation for Lambda handlers
 * @see https://docs.aws.amazon.com/powertools/typescript/latest/utilities/parser/
 */
import {parser} from '@aws-lambda-powertools/parser/middleware'
import {ApiGatewayEnvelope, ApiGatewayV2Envelope} from '@aws-lambda-powertools/parser/envelopes'
import type {ZodSchema} from 'zod'
/**
 * Create a Powertools parser middleware for API Gateway events with Zod schema
 * Automatically extracts and validates the request body against the provided schema
 *
 * @param schema - Zod schema to validate request body
 * @returns Middy middleware that parses and validates the request body
 *
 * @example
 * ```typescript
 * import middy from '@middy/core'
 * import {feedlyEventSchema} from '#util/constraints'
 * import {createApiBodyParser} from '#lib/vendor/Powertools/parser'
 *
 * const handler = middy(async (event) => {
 *   // event.body is now typed and validated as FeedlyEventInput
 *   const {articleURL, backgroundMode} = event.body
 *   // ...
 * }).use(createApiBodyParser(feedlyEventSchema))
 * ```
 */
export function createApiBodyParser<T>(schema: ZodSchema<T>)
/**
 * Create a parser middleware for API Gateway V2 (HTTP API) events
 */
export function createApiV2BodyParser<T>(schema: ZodSchema<T>)
// Re-export parser utilities
</file>

<file path="src/mcp/handlers/coverage.ts">
/**
 * Coverage analysis handler for MCP server
 * Analyzes which dependencies need mocking for Jest tests
 *
 * Uses build/graph.json for transitive dependency analysis
 */
import fs from 'fs/promises'
import path from 'path'
import {discoverEntities, loadDependencyGraph} from './data-loader.js'
export type CoverageQueryType = 'required' | 'missing' | 'all' | 'summary'
export interface CoverageQueryArgs {
  file: string
  query: CoverageQueryType
}
interface MockCategory {
  entities: Array<{name: string; path: string; mockHelper: string}>
  vendors: Array<{name: string; path: string; exports?: string[]}>
  external: Array<{name: string; suggestion: string}>
  utilities: Array<{path: string; needsMock: boolean; reason?: string}>
}
/**
 * Categorize a dependency for mocking purposes
 */
function categorizeDependency(dep: string, entityNames: string[]):
‚ãÆ----
// Entity files
‚ãÆ----
// AWS Vendor wrappers
‚ãÆ----
// Other vendor wrappers
‚ãÆ----
// Utilities that typically need mocking
‚ãÆ----
/**
 * Analyze an existing test file for current mocks
 */
async function analyzeExistingTestMocks(testFilePath: string): Promise<string[]>
‚ãÆ----
// Find jest.unstable_mockModule calls
‚ãÆ----
// Find jest.mock calls
‚ãÆ----
/**
 * Convert file path to test file path
 */
function getTestFilePath(filePath: string): string
‚ãÆ----
// src/lambdas/Name/src/index.ts -> src/lambdas/Name/test/index.test.ts
‚ãÆ----
export async function handleCoverageQuery(args: CoverageQueryArgs)
‚ãÆ----
// Load dependency graph and entity names
‚ãÆ----
// Get transitive dependencies for the file
‚ãÆ----
// Try to find the file in the graph with different path formats
‚ãÆ----
// Categorize all dependencies
‚ãÆ----
// Return what needs to be mocked
‚ãÆ----
// Compare against existing test file
‚ãÆ----
// Find missing mocks
‚ãÆ----
// Check if any existing mock matches this path
‚ãÆ----
// Full analysis
</file>

<file path="src/mcp/handlers/data-loader.test.ts">
/**
 * Unit tests for data-loader module
 * Tests shared data loading for MCP handlers
 *
 * Note: This module uses import.meta.url which makes mocking difficult.
 * These tests run against the actual project files (integration-style).
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
// Module loaded via dynamic import
‚ãÆ----
// Check for at least some expected Lambdas
‚ãÆ----
// Check for expected entities
</file>

<file path="src/mcp/handlers/impact.ts">
/**
 * Impact analysis handler for MCP server
 * Shows what's affected by changing a Lambda function or shared module
 *
 * Uses build/graph.json for dependency analysis and metadata for Lambda info
 */
import {getLambdaInvocations, loadDependencyGraph, loadMetadata} from './data-loader.js'
export type ImpactQueryType = 'dependents' | 'cascade' | 'tests' | 'infrastructure' | 'all'
export interface ImpactQueryArgs {
  file: string
  query: ImpactQueryType
}
/**
 * Convert a file path to its corresponding test file path
 */
function getTestFilePath(filePath: string): string | null
‚ãÆ----
// Lambda handler: src/lambdas/Name/src/index.ts -> src/lambdas/Name/test/index.test.ts
‚ãÆ----
// Entity: src/entities/Name.ts -> could have tests in multiple places
‚ãÆ----
return null // Entities are tested via Lambda tests
‚ãÆ----
// Utility: src/util/name.ts -> test/util/name.test.ts or src/util/name.test.ts
‚ãÆ----
/**
 * Extract Lambda name from a file path
 */
function extractLambdaName(filePath: string): string | null
/**
 * Find all files that import a given file (reverse dependency lookup)
 */
function findDependents(filePath: string, graph: Record<string,
/**
 * Recursively find all files affected by changing a file
 */
function findCascade(filePath: string, graph: Record<string,
export async function handleImpactQuery(args: ImpactQueryArgs)
‚ãÆ----
// Load data
‚ãÆ----
// Normalize file path
‚ãÆ----
// Check if file exists in graph
‚ãÆ----
// Find direct dependents
‚ãÆ----
// Categorize dependents
‚ãÆ----
// Find full cascade of affected files
‚ãÆ----
// Categorize by type
‚ãÆ----
// Find test files that need updating
‚ãÆ----
// Add test for the changed file itself
‚ãÆ----
// Add tests for affected files
‚ãÆ----
// This is already a test file
‚ãÆ----
// Group by Lambda
‚ãÆ----
// Find related Terraform files
‚ãÆ----
// Extract Lambda names from affected files
‚ãÆ----
// Map to Terraform files
‚ãÆ----
// Check for Lambda invocations
‚ãÆ----
// Comprehensive impact analysis
‚ãÆ----
// Extract affected Lambdas
‚ãÆ----
// Determine severity
‚ãÆ----
severity = 'HIGH' // Entity changes affect multiple Lambdas
‚ãÆ----
severity = 'CRITICAL' // Core utility changes
</file>

<file path="src/mcp/handlers/test-scaffold.ts">
/**
 * Test scaffolding handler for MCP server
 * Generates complete test file scaffolding with all required mocks
 *
 * Uses build/graph.json for dependency analysis and existing test patterns
 */
import {discoverEntities, loadDependencyGraph} from './data-loader.js'
export type TestScaffoldQueryType = 'scaffold' | 'mocks' | 'fixtures' | 'structure'
export interface TestScaffoldQueryArgs {
  file: string
  query: TestScaffoldQueryType
}
interface MockInfo {
  type: 'entity' | 'vendor' | 'utility' | 'external'
  name: string
  path: string
  importAlias: string
  mockCode: string
}
/**
 * Extract Lambda name from file path
 */
function extractLambdaName(filePath: string): string | null
/**
 * Generate mock code for a dependency
 */
function generateMockCode(dep: string, entityNames: string[]): MockInfo | null
‚ãÆ----
// Entity mocks
‚ãÆ----
// AWS Vendor mocks
‚ãÆ----
// Other vendor mocks (YouTube, etc.)
‚ãÆ----
// X-Ray wrapper
‚ãÆ----
/**
 * Get common mock functions for AWS services
 */
function getAwsServiceMocks(service: string): string[]
/**
 * Generate complete test file scaffold
 */
function generateTestScaffold(lambdaName: string, mocks: MockInfo[]): string
‚ãÆ----
// Imports
‚ãÆ----
// Environment setup
‚ãÆ----
// Entity mocks (must be before other mocks)
‚ãÆ----
// Vendor mocks
‚ãÆ----
// Import handler after mocks
‚ãÆ----
// Test context helper
‚ãÆ----
// Test suite
‚ãÆ----
// Reset entity mocks
‚ãÆ----
// Set up mock returns for entities
‚ãÆ----
export async function handleTestScaffoldQuery(args: TestScaffoldQueryArgs)
‚ãÆ----
// Load data
‚ãÆ----
// Normalize file path
‚ãÆ----
// Get transitive dependencies
‚ãÆ----
// Extract Lambda name
‚ãÆ----
// Generate mocks
‚ãÆ----
// Calculate test file path
‚ãÆ----
// Return just the mock setup code
‚ãÆ----
// Suggest fixture files based on dependencies
‚ãÆ----
// API Gateway event fixture
‚ãÆ----
// Entity-specific fixtures
‚ãÆ----
// Response fixtures
‚ãÆ----
// Return test file structure without full code
</file>

<file path="src/mcp/handlers/validation.ts">
/**
 * Validation handler for MCP server
 * Provides AST-based pattern validation against project conventions
 *
 * Uses the shared validation core from src/mcp/validation/
 */
import path from 'path'
import {fileURLToPath} from 'url'
import {allRules, rulesByName, validateFile} from '../validation/index.js'
‚ãÆ----
export type ValidationQueryType = 'all' | 'aws-sdk' | 'electrodb' | 'imports' | 'response' | 'rules' | 'summary'
export interface ValidationQueryArgs {
  file?: string
  query: ValidationQueryType
}
export async function handleValidationQuery(args: ValidationQueryArgs)
‚ãÆ----
// List all available validation rules
‚ãÆ----
// Validate a file and return a concise summary
</file>

<file path="src/mcp/parsers/convention-parser.test.ts">
/**
 * Unit tests for convention-parser.ts
 * Tests the pure functions for parsing conventions-tracking.md
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {readFileSync} from 'fs'
import {join} from 'path'
import type {Convention} from './convention-parser'
// Load fixture from project root (Jest runs from rootDir)
‚ãÆ----
// Module functions loaded via dynamic import
‚ãÆ----
// May be empty depending on fixture
</file>

<file path="src/mcp/parsers/convention-parser.ts">
/**
 * Parser for conventions-tracking.md
 * Extracts structured convention data from markdown format
 */
export type ConventionCategory = 'testing' | 'aws' | 'typescript' | 'git' | 'infrastructure' | 'security' | 'meta' | 'patterns' | 'unknown'
export type ConventionSeverity = 'CRITICAL' | 'HIGH' | 'MEDIUM' | 'LOW'
export type ConventionStatus = 'pending' | 'documented' | 'proposed' | 'archived'
export interface Convention {
  name: string
  type: string
  category: ConventionCategory
  severity: ConventionSeverity
  status: ConventionStatus
  what: string
  why: string
  wikiPath?: string
  enforcement?: string
  detectedDate?: string
}
interface ParsedConventions {
  conventions: Convention[]
  metadata: {totalCount: number; documentedCount: number; pendingCount: number; lastUpdated?: string}
}
/**
 * Infer category from convention type or wiki path
 */
function inferCategory(type: string, wikiPath?: string): ConventionCategory
/**
 * Parse severity from priority string
 */
function parseSeverity(priority?: string): ConventionSeverity
/**
 * Parse a single convention block from markdown
 */
function parseConventionBlock(block: string, defaultStatus: ConventionStatus): Convention | null
‚ãÆ----
// Match convention header: "1. **Name** (Type)" or "### Name"
‚ãÆ----
// Extract fields using regex
‚ãÆ----
// Determine actual status from status field or section default
‚ãÆ----
/**
 * Parse conventions-tracking.md content into structured data
 */
export function parseConventions(content: string): ParsedConventions
‚ãÆ----
// Extract metadata
‚ãÆ----
// Split by sections
‚ãÆ----
// Split section into convention blocks (numbered items or ### headers)
‚ãÆ----
/**
 * Search conventions by term (searches name, what, why fields)
 */
export function searchConventions(conventions: Convention[], term: string): Convention[]
/**
 * Filter conventions by category
 */
export function filterByCategory(conventions: Convention[], category: ConventionCategory): Convention[]
/**
 * Filter conventions by severity
 */
export function filterBySeverity(conventions: Convention[], severity: ConventionSeverity): Convention[]
/**
 * Filter conventions by status
 */
export function filterByStatus(conventions: Convention[], status: ConventionStatus): Convention[]
</file>

<file path="src/mcp/validation/rules/batch-retry.test.ts">
/**
 * Unit tests for batch-retry rule
 * HIGH: Enforce retry handling for batch operations
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
</file>

<file path="src/mcp/validation/rules/electrodb-mocking.test.ts">
/**
 * Unit tests for electrodb-mocking rule
 * CRITICAL: Test files must use createElectroDBEntityMock() helper
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
// Module loaded via dynamic import
‚ãÆ----
// Create ts-morph project for in-memory source files
</file>

<file path="src/mcp/validation/rules/response-enum.test.ts">
/**
 * Unit tests for response-enum rule
 * MEDIUM: Enforce ResponseStatus enum usage
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
‚ãÆ----
// Should not flag 'successful' in message property
</file>

<file path="src/mcp/validation/types.ts">
/**
 * Shared validation types for MCP convention checking
 * These types can be reused by CI scripts and future tooling
 */
import type {SourceFile} from 'ts-morph'
export type ValidationSeverity = 'CRITICAL' | 'HIGH' | 'MEDIUM' | 'LOW'
export interface Violation {
  rule: string
  severity: ValidationSeverity
  line: number
  column?: number
  message: string
  suggestion?: string
  codeSnippet?: string
}
export interface ValidationResult {
  file: string
  valid: boolean
  violations: Violation[]
  passed: string[]
  skipped: string[]
}
export interface ValidationRule {
  /** Unique identifier for the rule */
  name: string
  /** Human-readable description */
  description: string
  /** Severity level if violated */
  severity: ValidationSeverity
  /** File patterns this rule applies to (glob-like) */
  appliesTo: string[]
  /** File patterns to exclude from this rule */
  excludes?: string[]
  /** Validate a source file and return violations */
  validate(sourceFile: SourceFile, filePath: string): Violation[]
}
‚ãÆ----
/** Unique identifier for the rule */
‚ãÆ----
/** Human-readable description */
‚ãÆ----
/** Severity level if violated */
‚ãÆ----
/** File patterns this rule applies to (glob-like) */
‚ãÆ----
/** File patterns to exclude from this rule */
‚ãÆ----
/** Validate a source file and return violations */
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
export interface ValidationContext {
  /** Project root directory */
  projectRoot: string
  /** File being validated */
  filePath: string
  /** Whether this is a test file */
  isTestFile: boolean
  /** Whether this is a Lambda handler */
  isLambdaHandler: boolean
  /** Whether this is in the vendor directory */
  isVendorFile: boolean
}
‚ãÆ----
/** Project root directory */
‚ãÆ----
/** File being validated */
‚ãÆ----
/** Whether this is a test file */
‚ãÆ----
/** Whether this is a Lambda handler */
‚ãÆ----
/** Whether this is in the vendor directory */
‚ãÆ----
/**
 * Helper to create a violation object
 */
export function createViolation(
  rule: string,
  severity: ValidationSeverity,
  line: number,
  message: string,
  options?: {column?: number; suggestion?: string; codeSnippet?: string}
): Violation
/**
 * Determine validation context from file path
 */
export function getValidationContext(filePath: string, projectRoot: string): ValidationContext
</file>

<file path="src/templates/github-issues/user-deletion-failure.md">
## User Deletion Failed

A user deletion operation failed and requires manual investigation.

**Triggered By**:
- **User ID**: ${userId}
- **Request ID**: ${requestId}
- **Error Type**: ${error.constructor.name}
- **Error Message**: ${error.message}
- **Timestamp**: ${new Date().toISOString()}

---

## Affected Devices

${devices.length > 0 ? `This user had **${devices.length}** registered device(s):

${devices.map((device, index) => `### Device ${index + 1}
- **Device Token**: \`${device.deviceToken}\`
- **Platform**: ${device.platform || 'Unknown'}
- **Last Updated**: ${device.updatedAt || 'Unknown'}
`).join('\n')}` : 'No devices were registered for this user.'}

---

## Required Action

1. **Investigate DynamoDB State**:
   \`\`\`bash
   aws dynamodb scan \\
     --table-name Users \\
     --filter-expression "userId = :userId" \\
     --expression-attribute-values '{":userId":{"S":"${userId}"}}' \\
     --region us-west-2
   \`\`\`

2. **Check S3 Bucket for User Files**:
   \`\`\`bash
   aws s3 ls s3://\${BUCKET_NAME}/${userId}/ --recursive
   \`\`\`

3. **Verify SNS Subscriptions**:
   \`\`\`bash
   aws sns list-subscriptions --region us-west-2 | grep ${userId}
   \`\`\`

4. **Manual Cleanup** (if needed):
   - Remove DynamoDB entries
   - Delete S3 objects
   - Unsubscribe SNS endpoints
   - Remove device registrations

---

## Stack Trace

\`\`\`
${error.stack || 'No stack trace available'}
\`\`\`

---

## Context

**Request ID**: ${requestId}

This deletion was likely triggered by:
- User account closure request
- Data retention policy
- Manual cleanup operation

**Lambda Function**: DeleteUser
**DynamoDB Table**: Users, Devices
**S3 Bucket**: User-specific media files

---

This issue was automatically created by the user management system. Deletion failures typically indicate:
- Concurrent modification conflicts
- Permission issues
- Orphaned resources
- DynamoDB conditional check failures
</file>

<file path="src/templates/github-issues/video-download-failure.md">
## Video Download Failure

A video download operation failed and requires investigation.

**File Details**:
- **File ID**: ${fileId}
- **Video URL**: ${fileUrl}
- **Error Type**: ${error.constructor.name}
- **Error Message**: ${error.message}
- **Timestamp**: ${new Date().toISOString()}

---

${errorDetails ? `## Additional Details

\`\`\`
${errorDetails}
\`\`\`

---

` : ''}## Stack Trace

\`\`\`
${error.stack || 'No stack trace available'}
\`\`\`

---

## Debugging Steps

### 1. Check File Status in DynamoDB
\`\`\`bash
aws dynamodb get-item \\
  --table-name \${DYNAMODB_TABLE_FILES} \\
  --key '{"fileId":{"S":"${fileId}"}}' \\
  --region us-west-2
\`\`\`

### 2. Verify Video Accessibility
\`\`\`bash
# Test if yt-dlp can access the video
yt-dlp --simulate "${fileUrl}"
\`\`\`

### 3. Check Lambda Logs
\`\`\`bash
aws logs tail /aws/lambda/StartFileUpload \\
  --region us-west-2 \\
  --since 1h \\
  --filter-pattern "${fileId}"
\`\`\`

### 4. Inspect S3 Bucket
\`\`\`bash
aws s3 ls s3://\${BUCKET_NAME}/ --recursive | grep "${fileId}"
\`\`\`

---

## Common Failure Causes

| Error Pattern | Likely Cause | Solution |
|--------------|--------------|----------|
| **403 Forbidden** | Cookie expiration or bot detection | Refresh YouTube cookies |
| **404 Not Found** | Video deleted or made private | Remove from queue |
| **Network timeout** | Large file or slow connection | Retry or increase timeout |
| **Stream error** | HLS/DASH fragmentation issue | Check /tmp disk space |
| **S3 upload error** | Multipart upload failure | Check S3 permissions |

---

## Manual Recovery

If the video is still accessible, trigger a manual retry:

\`\`\`bash
aws lambda invoke \\
  --function-name FileCoordinator \\
  --region us-west-2 \\
  --payload '{"fileId":"${fileId}"}' \\
  /dev/null
\`\`\`

---

## Related Resources

- **Lambda Function**: StartFileUpload
- **S3 Bucket**: Media storage bucket
- **DynamoDB Table**: Files table (status: Failed)
- **CloudWatch Metrics**: VideoDownloadFailure

**Documentation**: See \`docs/YT-DLP-MIGRATION-STRATEGY.md\` for architecture details.

---

This issue was automatically created by the video download monitoring system.
</file>

<file path="src/types/domain-models.d.ts">
import {FileStatus} from './enums'
export interface User {
  userId: string
  email: string
  emailVerified: boolean
  firstName: string
  lastName?: string
}
/**
 * Media file metadata.
 * Status values: 'Queued' | 'Downloading' | 'Downloaded' | 'Failed'
 */
export interface File {
  fileId: string
  size: number
  authorName: string
  authorUser: string
  publishDate: string
  description: string
  key: string
  url?: string
  contentType: string
  title: string
  status: FileStatus
}
export interface Device {
  name: string
  token: string
  systemVersion: string
  deviceId: string
  systemName: string
  endpointArn: string
}
export interface IdentityProvider {
  accessToken: string
  refreshToken: string
  tokenType: string
  expiresAt: number
}
</file>

<file path="src/types/notification-types.d.ts">
export type FileNotificationType = 'MetadataNotification' | 'DownloadReadyNotification'
export interface MetadataNotification {
  fileId: string
  key: string
  title: string
  authorName: string
  authorUser: string
  description: string
  publishDate: string
  contentType: string
  status: 'pending'
}
export interface DownloadReadyNotification {
  fileId: string
  key: string
  size: number
  url: string
}
</file>

<file path="src/types/persistence-types.d.ts">
/**
 * Persistence/relationship types for DynamoDB join tables
 */
export interface UserDevice {
  userId: string
  deviceId: string
}
export interface UserFile {
  fileId: string
  userId: string
}
</file>

<file path="src/types/video.ts">
/**
 * Video Processing Types
 *
 * Type definitions for video download, error classification, and yt-dlp integration.
 *
 * @see src/util/video-error-classifier.ts - Error classification implementation
 * @see src/lib/vendor/YouTube.ts - YouTube/yt-dlp integration
 */
import type {YtDlpVideoInfo} from './youtube'
/**
 * Categories of video download errors for intelligent retry handling
 */
export type VideoErrorCategory =
  | 'scheduled' // Scheduled video, retry at release_timestamp
  | 'livestream_upcoming' // Livestream not started, retry when starts
  | 'transient' // Network/temporary error, exponential backoff
  | 'cookie_expired' // Cookie needs refresh, requires manual intervention
  | 'permanent' // Deleted, geo-blocked, private - no retry
‚ãÆ----
| 'scheduled' // Scheduled video, retry at release_timestamp
| 'livestream_upcoming' // Livestream not started, retry when starts
| 'transient' // Network/temporary error, exponential backoff
| 'cookie_expired' // Cookie needs refresh, requires manual intervention
| 'permanent' // Deleted, geo-blocked, private - no retry
/**
 * Result of classifying a video download error
 */
export interface VideoErrorClassification {
  /** The category of error determining retry behavior */
  category: VideoErrorCategory
  /** Whether this error is retryable */
  retryable: boolean
  /** Unix timestamp (seconds) for when to retry, undefined if not retryable */
  retryAfter?: number
  /** Override default max retries for this category */
  maxRetries?: number
  /** Human-readable reason for the classification */
  reason: string
}
‚ãÆ----
/** The category of error determining retry behavior */
‚ãÆ----
/** Whether this error is retryable */
‚ãÆ----
/** Unix timestamp (seconds) for when to retry, undefined if not retryable */
‚ãÆ----
/** Override default max retries for this category */
‚ãÆ----
/** Human-readable reason for the classification */
‚ãÆ----
/**
 * Extended video info with scheduling fields from yt-dlp
 * These fields are provided by yt-dlp for scheduled/livestream content
 */
export interface SchedulingVideoInfo {
  release_timestamp?: number
  is_live?: boolean
  live_status?: 'is_live' | 'is_upcoming' | 'was_live' | 'not_live'
  availability?: 'public' | 'unlisted' | 'private' | 'needs_auth' | 'subscriber_only'
}
/**
 * Result of fetching video info - either success with info or failure with error details
 */
export interface FetchVideoInfoResult {
  success: boolean
  info?: YtDlpVideoInfo
  error?: Error
  isCookieError?: boolean
}
</file>

<file path="src/util/test/better-auth-helpers.test.ts">
/**
 * Unit tests for Better Auth Helper Functions
 *
 * Tests session management, validation, and token generation using electrodb-mock.
 */
import {beforeEach, describe, expect, it, jest} from '@jest/globals'
import {UnauthorizedError} from '../errors'
import {createElectroDBEntityMock} from '#test/helpers/electrodb-mock'
// Create entity mocks
‚ãÆ----
// Mock Sessions entity
‚ãÆ----
// Import after mocking
‚ãÆ----
/**
 * Mock session data overrides for testing
 */
interface MockSessionOverrides {
  sessionId?: string
  userId?: string
  token?: string
  expiresAt?: number
  deviceId?: string
  ipAddress?: string
  userAgent?: string
  createdAt?: number
  updatedAt?: number
}
/**
 * Helper to create mock session objects with sensible defaults
 */
function createMockSession(overrides?: MockSessionOverrides)
‚ãÆ----
expiresAt: now + 30 * 24 * 60 * 60 * 1000, // 30 days future
‚ãÆ----
// Clear all mock call history
‚ãÆ----
// Mock query.byToken operation (using GSI instead of scan)
‚ãÆ----
// Mock update operation
‚ãÆ----
// Should update lastActiveAt
‚ãÆ----
expiresAt: now - 1000, // Expired 1 second ago
‚ãÆ----
const originalExpiration = Date.now() + 10 * 24 * 60 * 60 * 1000 // 10 days
const newExpiration = Date.now() + 30 * 24 * 60 * 60 * 1000 // 30 days
</file>

<file path="src/util/test/github-helpers.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {Device} from '#types/domain-models'
// Mock Octokit
‚ãÆ----
class MockOctokit
‚ãÆ----
constructor()
‚ãÆ----
// Mock template helpers
</file>

<file path="src/util/test/lambda-helpers.test.ts">
import {afterEach, beforeEach, describe, expect, it, jest} from '@jest/globals'
import type {APIGatewayRequestAuthorizerEvent, Context, CustomAuthorizerResult, S3Event, ScheduledEvent, SQSEvent} from 'aws-lambda'
‚ãÆ----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
type TestEvent = any
‚ãÆ----
// Should have at least 2 fixture logs (incoming and outgoing)
‚ãÆ----
return false // Skip non-JSON logs
‚ãÆ----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
‚ãÆ----
// Event with valid Authorization header and userId from authorizer
‚ãÆ----
// Event with Authorization header but no valid userId (Unauthenticated)
‚ãÆ----
// Event with no Authorization header (Anonymous)
‚ãÆ----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
‚ãÆ----
// Event with valid Authorization header and userId from authorizer
‚ãÆ----
// Event with Authorization header but no valid userId (Unauthenticated)
‚ãÆ----
// Event with no Authorization header (Anonymous)
‚ãÆ----
type RecordType = {id: string; shouldFail?: boolean}
‚ãÆ----
// Should process records 1 and 3, skip 2
</file>

<file path="src/util/test/template-helpers.test.ts">
import {describe, expect, test} from '@jest/globals'
import {renderGithubIssueTemplate} from '../template-helpers'
‚ãÆ----
{deviceToken: 'device-token-1', platform: 'iOS', updatedAt: '2024-01-01T00:00:00.000Z'}, // fmt: multiline
‚ãÆ----
// Verify core content is present
‚ãÆ----
// Verify template variables were replaced (bash variables like ${BUCKET_NAME} are OK)
‚ãÆ----
// Verify template variables were replaced
‚ãÆ----
// Verify template variables were replaced
‚ãÆ----
// Verify template variables were replaced
‚ãÆ----
// Template uses new Date().toISOString() - verify format
</file>

<file path="src/util/test/video-error-classifier.test.ts">
import {beforeEach, describe, expect, it, jest} from '@jest/globals'
// Mock the errors module before importing the classifier
‚ãÆ----
constructor(message: string)
‚ãÆ----
const futureTimestamp = Math.floor(Date.now() / 1000) + 3600 // 1 hour from now
‚ãÆ----
expect(result.retryAfter).toBe(futureTimestamp + 300) // +5 min buffer
‚ãÆ----
const pastTimestamp = Math.floor(Date.now() / 1000) - 3600 // 1 hour ago
‚ãÆ----
const futureTimestamp = Math.floor(Date.now() / 1000) + 7200 // 2 hours from now
‚ãÆ----
expect(calculateExponentialBackoff(0)).toBe(now + 15 * 60) // 15 min
expect(calculateExponentialBackoff(1)).toBe(now + 30 * 60) // 30 min
expect(calculateExponentialBackoff(2)).toBe(now + 60 * 60) // 1 hour
expect(calculateExponentialBackoff(3)).toBe(now + 2 * 60 * 60) // 2 hours
expect(calculateExponentialBackoff(4)).toBe(now + 4 * 60 * 60) // 4 hours
‚ãÆ----
const maxDelay = 4 * 60 * 60 // 4 hours default
// For retry count 10, the exponential would be huge, but should cap
‚ãÆ----
const baseDelay = 60 // 1 minute
const maxDelay = 300 // 5 minutes
</file>

<file path="src/util/device-helpers.ts">
/**
 * Device Helper Functions
 *
 * Shared utilities for device management across multiple Lambda functions.
 * Handles device registration, unregistration, and SNS subscriptions.
 */
import {Devices} from '#entities/Devices'
import {UserDevices} from '#entities/UserDevices'
import {logDebug} from './logging'
import type {UserDevice} from '#types/persistence-types'
import type {Device} from '#types/domain-models'
import {deleteEndpoint, subscribe, unsubscribe} from '#lib/vendor/AWS/SNS'
/**
 * Disassociates a deviceId from a User by deleting the UserDevice record
 * @param userId - The UUID of the User
 * @param deviceId - The UUID of the Device
 * @see {@link lambdas/PruneDevices/src!#handler | PruneDevices }
 */
export async function deleteUserDevice(userId: string, deviceId: string): Promise<void>
/**
 * Removes a Device from DynamoDB.
 * This includes deleting the associated endpoint from SNS.
 * @param device - The Device object from DynamoDB
 * @see {@link lambdas/PruneDevices/src!#handler | PruneDevices }
 * @see {@link lambdas/UserDelete/src!#handler | UserDelete }
 */
export async function deleteDevice(device: Device): Promise<void>
/**
 * Queries a user's device parameters from DynamoDB
 * Returns array of UserDevice records (one per device association)
 * @param userId - The userId
 * @see {@link lambdas/UserDelete/src!#handler | UserDelete }
 * @see {@link lambdas/RegisterDevice/src!#handler | RegisterDevice }
 */
export async function getUserDevices(userId: string): Promise<UserDevice[]>
/**
 * Subscribes an endpoint (a client device) to an SNS topic
 * @param endpointArn - The EndpointArn of a mobile app and device
 * @param topicArn - The ARN of the topic you want to subscribe to
 * @see {@link lambdas/RegisterDevice/src!#handler | RegisterDevice }
 * @see {@link lambdas/UserSubscribe/src!#handler | UserSubscribe }
 */
export async function subscribeEndpointToTopic(endpointArn: string, topicArn: string)
/**
 * Unsubscribes an endpoint (a client device) from an SNS topic
 * @param subscriptionArn - The SubscriptionArn of an endpoint+topic
 * @see {@link lambdas/RegisterDevice/src!#handler | RegisterDevice }
 */
export async function unsubscribeEndpointToTopic(subscriptionArn: string)
</file>

<file path="src/util/lambda-invoke-helpers.ts">
/**
 * Lambda Invocation Helpers
 *
 * Shared utilities for Lambda-to-Lambda invocations.
 */
import {logDebug} from './logging'
import {invokeAsync} from '#lib/vendor/AWS/Lambda'
/**
 * Initiates a file download by invoking the StartFileUpload Lambda
 * Uses asynchronous invocation (Event type) to avoid blocking
 * @param fileId - The YouTube video ID to download
 * @param correlationId - Optional correlation ID for end-to-end request tracing
 * @see {@link lambdas/FileCoordinator/src!#handler | FileCoordinator }
 * @see {@link lambdas/WebhookFeedly/src!#handler | WebhookFeedly }
 */
export async function initiateFileDownload(fileId: string, correlationId?: string)
</file>

<file path="src/util/user-file-helpers.ts">
/**
 * User-File Helper Functions
 *
 * Shared utilities for managing user-file relationships across multiple Lambda functions.
 * Handles file associations, ownership, and sharing operations.
 */
import {UserFiles} from '#entities/UserFiles'
import {logDebug} from './logging'
/**
 * Associates a File to a User by creating a UserFile record
 * Creates individual record for the user-file relationship
 * Idempotent - returns gracefully if association already exists
 * @param fileId - The unique file identifier
 * @param userId - The UUID of the user
 * @see {@link lambdas/WebhookFeedly/src!#handler | WebhookFeedly }
 */
export async function associateFileToUser(fileId: string, userId: string)
</file>

<file path="terraform/configuration_apns.tf">
// The configuration file the APNS
</file>

<file path="terraform/dynamodb_idempotency.tf">
# DynamoDB table for Powertools Idempotency
# Stores idempotency records to prevent duplicate processing
# TTL automatically cleans up expired records

resource "aws_dynamodb_table" "IdempotencyTable" {
  name         = "MediaDownloader-Idempotency"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "id"

  attribute {
    name = "id"
    type = "S"
  }

  ttl {
    attribute_name = "expiration"
    enabled        = true
  }

  tags = {
    Name        = "MediaDownloader-Idempotency"
    Environment = "production"
    Purpose     = "Powertools idempotency storage"
  }
}

output "idempotency_table_name" {
  description = "Name of the Idempotency DynamoDB table"
  value       = aws_dynamodb_table.IdempotencyTable.name
}

output "idempotency_table_arn" {
  description = "ARN of the Idempotency DynamoDB table"
  value       = aws_dynamodb_table.IdempotencyTable.arn
}
</file>

<file path="tsp/examples/list-files-response.json">
{
  "contents": [
    {
      "fileId": "PaZ1EmPOE_k",
      "key": "20150826-[The School of Life].mp4",
      "title": "On Feeling Melancholy",
      "status": "Downloaded",
      "size": 12023572,
      "contentType": "video/mp4",
      "authorName": "The School of Life"
    }
  ],
  "keyCount": 1
}
</file>

<file path="tsp/examples/login-user-request.json">
{
  "authorizationCode": "c1234567890abcdef.0.a.b1234567890abcdef"
}
</file>

<file path="tsp/examples/login-user-response.json">
{
  "token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
}
</file>

<file path="tsp/examples/README.md">
# TypeSpec Example Files

This directory contains example JSON files that are referenced in the TypeSpec API documentation.

## Purpose

Instead of copying JSON content into TypeSpec documentation, we reference these external files. This approach:

1. **Maintains a single source of truth** - Test fixtures drive the examples
2. **Reduces duplication** - No need to copy JSON in multiple places
3. **Ensures consistency** - Examples stay in sync with test data
4. **Improves maintainability** - Update once, reflects everywhere

## File Structure

All example files in this directory are automatically synchronized from API-specific test fixtures. These fixtures follow the naming convention `apiRequest-*.json` and `apiResponse-*.json` in each lambda's test fixtures directory.

| File | Description |
|------|-------------|
| `list-files-response.json` | Response from GET /files endpoint |
| `register-device-request.json` | Request body for POST /device/register |
| `register-device-response.json` | Response from POST /device/register |
| `webhook-feedly-request.json` | Request body for POST /feedly |
| `webhook-feedly-response.json` | Response from POST /feedly |
| `login-user-request.json` | Request body for POST /user/login |
| `login-user-response.json` | Response from POST /user/login |
| `register-user-request.json` | Request body for POST /user/register |
| `register-user-response.json` | Response from POST /user/register |

## Syncing Examples

These files are automatically synced when you generate documentation:

```bash
npm run document-api
```

The sync happens automatically as the first step of the documentation generation process.

You can also run the sync script manually if needed:

```bash
./bin/sync-examples.sh
```

## Usage in TypeSpec

In TypeSpec operation documentation, examples are referenced like this:

```typescript
/**
 * Example request: See `tsp/examples/register-device-request.json`
 * (sourced from `src/lambdas/RegisterDevice/test/fixtures/apiRequest-POST-device.json`)
 */
```

This reference appears in the generated OpenAPI documentation, allowing readers to:
1. View the example file directly
2. Trace back to the original test fixture
3. Verify the example against actual test data

## Development Workflow

1. **Create or update API fixtures** in `src/lambdas/[LambdaName]/test/fixtures/` using the naming convention:
   - `apiRequest-[METHOD]-[note].json` for request examples
   - `apiResponse-[METHOD]-[statusCode]-[note].json` for response examples
2. **Regenerate docs**: `npm run document-api` (automatically syncs examples and generates documentation)
3. **Verify changes**: Check `docs/api/openapi.yaml` and `docs/api/index.html`

The sync script automatically discovers all `apiRequest-*.json` and `apiResponse-*.json` files and copies them to this directory with standardized names.

## Benefits

- ‚úÖ **No duplication** - Examples aren't copied into TypeSpec files
- ‚úÖ **Always in sync** - Sync script keeps examples current
- ‚úÖ **Traceable** - Clear connection between tests and docs
- ‚úÖ **Maintainable** - Update fixtures, run sync, done
- ‚úÖ **Testable** - Examples are actual test data
</file>

<file path="tsp/examples/register-device-request.json">
{
  "systemVersion": "16.0.2",
  "deviceId": "67C431DE-37D2-4BBA-9055-E9D2766517E1",
  "name": "iPhone",
  "systemName": "iOS",
  "token": "1270ac093113154918d1ae96e90247d068b98766842654b3cc2400c7342dc4ba"
}
</file>

<file path="tsp/examples/register-device-response.json">
{
  "endpointArn": "arn:aws:sns:us-west-2:123456789012:endpoint/APNS/MyApp/abcd1234-5678-90ab-cdef-1234567890ab"
}
</file>

<file path="tsp/examples/register-user-request.json">
{
  "authorizationCode": "c1234567890abcdef.0.a.b1234567890abcdef",
  "firstName": "John",
  "lastName": "Doe"
}
</file>

<file path="tsp/examples/register-user-response.json">
{
  "token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
}
</file>

<file path="tsp/examples/webhook-feedly-request.json">
{
  "articleFirstImageURL": "https://i.ytimg.com/vi/7jEzw5WLiMI/maxresdefault.jpg",
  "articleCategories": "YouTube",
  "articlePublishedAt": "April 27, 2020 at 04:10PM",
  "articleTitle": "WOW! Ariana Grande Meme Backlash & Meme War, COVID-19 Contact Tracing Problems, Mr. Beast & More",
  "articleURL": "https://www.youtube.com/watch?v=wRG7lAGdRII",
  "createdAt": "April 27, 2020 at 04:10PM",
  "sourceFeedURL": "https://www.youtube.com/playlist?list=UUlFSU9_bUb4Rc6OYfTt5SPw",
  "sourceTitle": "Philip DeFranco (uploads) on YouTube",
  "sourceURL": "https://youtube.com/playlist?list=UUlFSU9_bUb4Rc6OYfTt5SPw"
}
</file>

<file path="tsp/examples/webhook-feedly-response.json">
{
  "status": "Dispatched"
}
</file>

<file path="tsp/operations/operations.tsp">
import "@typespec/http";
import "../main.tsp";
import "../models/models.tsp";

using TypeSpec.Http;
using OfflineMediaDownloader.Models;

namespace OfflineMediaDownloader.Operations;

/**
 * File operations
 */
@route("/files")
@tag("Files")
interface Files {
  /**
   * List all files available to the authenticated user.
   * 
   * Returns files based on authentication status:
   * - Authenticated users: returns their personal file library
   * - Anonymous users: returns a demo file for training purposes
   * - Unauthenticated users: returns 401 Unauthorized
   * 
   * Example response: See `tsp/examples/list-files-response.json`
   * (sourced from `src/lambdas/ListFiles/test/fixtures/apiResponse-GET-200-OK.json`)
   * 
   * @returns List of available files
   */
  @get
  @summary("List available files")
  listFiles(
    @header("Authorization") authorization: string,
    @header("X-API-Key") apiKey: string,
  ): {
    @statusCode statusCode: 200;
    @body body: FileListResponse;
  } | {
    @statusCode statusCode: 401;
    @body body: UnauthorizedError;
  } | {
    @statusCode statusCode: 403;
    @body body: ForbiddenError;
  } | {
    @statusCode statusCode: 500;
    @body body: InternalServerError;
  };
}

/**
 * Device registration operations
 */
@route("/device")
@tag("Devices")
interface Devices {
  /**
   * Register a device to receive push notifications.
   * 
   * This is an idempotent operation that:
   * 1. Creates an AWS SNS endpoint for the device
   * 2. Associates the device with the authenticated user
   * 3. Subscribes the device to push notification topics
   * 
   * Example request: See `tsp/examples/register-device-request.json`
   * (sourced from `src/lambdas/RegisterDevice/test/fixtures/apiRequest-POST-device.json`)
   * 
   * Example response: See `tsp/examples/register-device-response.json`
   * (sourced from `src/lambdas/RegisterDevice/test/fixtures/apiResponse-POST-200-OK.json`)
   * 
   * @returns Device registration confirmation with endpoint ARN
   */
  @post
  @route("/register")
  @summary("Register device for push notifications")
  registerDevice(
    @header("Authorization") authorization: string,
    @header("X-API-Key") apiKey: string,
    @body device: DeviceRegistrationRequest,
  ): {
    @statusCode statusCode: 200;
    @body body: DeviceRegistrationResponse;
  } | {
    @statusCode statusCode: 201;
    @body body: DeviceRegistrationResponse;
  } | {
    @statusCode statusCode: 400;
    @body body: ErrorResponse;
  } | {
    @statusCode statusCode: 401;
    @body body: UnauthorizedError;
  } | {
    @statusCode statusCode: 403;
    @body body: ForbiddenError;
  } | {
    @statusCode statusCode: 500;
    @body body: InternalServerError;
  };
}

/**
 * Webhook operations
 */
@route("/feedly")
@tag("Webhooks")
interface Webhooks {
  /**
   * Receive a webhook from Feedly to download media.
   * 
   * When a webhook is received:
   * - If the file exists: associates it with the user and sends push notification
   * - If the file doesn't exist: creates it, associates with user, and queues for download
   * 
   * Background mode can be used to defer immediate download processing.
   * 
   * Example request: See `tsp/examples/webhook-feedly-request.json`
   * (sourced from `src/lambdas/WebhookFeedly/test/fixtures/apiRequest-POST-webhook.json`)
   * 
   * Example response: See `tsp/examples/webhook-feedly-response.json`
   * (sourced from `src/lambdas/WebhookFeedly/test/fixtures/apiResponse-POST-200-OK.json`)
   * 
   * @returns Webhook processing status
   */
  @post
  @summary("Process Feedly webhook")
  processFeedlyWebhook(
    @header("Authorization") authorization: string,
    @header("X-API-Key") apiKey: string,
    @body webhook: FeedlyWebhook,
  ): {
    @statusCode statusCode: 200;
    @body body: WebhookResponse;
  } | {
    @statusCode statusCode: 202;
    @body body: WebhookResponse;
  } | {
    @statusCode statusCode: 400;
    @body body: ErrorResponse;
  } | {
    @statusCode statusCode: 403;
    @body body: ForbiddenError;
  } | {
    @statusCode statusCode: 500;
    @body body: InternalServerError;
  };
}

/**
 * User authentication operations
 */
@route("/user")
@tag("Authentication")
interface Authentication {
  /**
   * Register a new user with Sign in with Apple.
   * 
   * Creates a new user account or returns existing user token.
   * All user details are sourced from the Apple identity token.
   * 
   * Example request: See `tsp/examples/register-user-request.json`
   * (sourced from `src/lambdas/RegisterUser/test/fixtures/apiRequest-POST-register.json`)
   * 
   * Example response: See `tsp/examples/register-user-response.json`
   * (sourced from `src/lambdas/RegisterUser/test/fixtures/apiResponse-POST-200-OK.json`)
   * 
   * @returns JWT access token
   */
  @post
  @route("/register")
  @summary("Register new user")
  registerUser(
    @header("X-API-Key") apiKey: string,
    @body registration: UserRegistration,
  ): {
    @statusCode statusCode: 200;
    @body body: UserRegistrationResponse;
  } | {
    @statusCode statusCode: 400;
    @body body: ErrorResponse;
  } | {
    @statusCode statusCode: 403;
    @body body: ForbiddenError;
  } | {
    @statusCode statusCode: 500;
    @body body: InternalServerError;
  };

  /**
   * Login an existing user with Sign in with Apple.
   * 
   * Authenticates a user and returns a JWT access token.
   * 
   * Example request: See `tsp/examples/login-user-request.json`
   * (sourced from `src/lambdas/LoginUser/test/fixtures/apiRequest-POST-login.json`)
   * 
   * Example response: See `tsp/examples/login-user-response.json`
   * (sourced from `src/lambdas/LoginUser/test/fixtures/apiResponse-POST-200-OK.json`)
   * 
   * @returns JWT access token
   */
  @post
  @route("/login")
  @summary("Login existing user")
  loginUser(
    @header("X-API-Key") apiKey: string,
    @body login: UserLogin,
  ): {
    @statusCode statusCode: 200;
    @body body: UserLoginResponse;
  } | {
    @statusCode statusCode: 409;
    @body body: ErrorResponse;
  } | {
    @statusCode statusCode: 400;
    @body body: ErrorResponse;
  } | {
    @statusCode statusCode: 404;
    @body body: ErrorResponse;
  } | {
    @statusCode statusCode: 403;
    @body body: ForbiddenError;
  } | {
    @statusCode statusCode: 500;
    @body body: InternalServerError;
  };
}
</file>

<file path="tsp/EXAMPLES.md">
# TypeSpec Examples from Test Fixtures

This document explains how test fixtures are referenced in the TypeSpec API documentation.

## Overview

The TypeSpec definitions reference real-world examples from the test fixtures in `src/lambdas/*/test/fixtures/`. These examples are synchronized to `tsp/examples/` directory, ensuring the documentation stays in sync with actual test data.

## Example Files

All example files are located in `tsp/examples/` and are automatically synchronized from API-specific test fixtures. These fixtures follow the naming convention `apiRequest-*.json` and `apiResponse-*.json` in each lambda's test fixtures directory:

| Example File | API Operation |
|-------------|---------------|
| `list-files-response.json` | GET /files |
| `register-device-request.json` | POST /device/register |
| `register-device-response.json` | POST /device/register |
| `webhook-feedly-request.json` | POST /feedly |
| `webhook-feedly-response.json` | POST /feedly |
| `register-user-request.json` | POST /user/register |
| `register-user-response.json` | POST /user/register |
| `login-user-request.json` | POST /user/login |
| `login-user-response.json` | POST /user/login |

## Synchronizing Examples

Examples are automatically synced when you run:

```bash
npm run document-api
```

The sync happens automatically as the first step of the documentation generation process. The sync script (`bin/sync-examples.sh`):
1. Scans all lambda directories for files matching `apiRequest-*.json` and `apiResponse-*.json`
2. Automatically copies these fixtures to `tsp/examples/` with standardized naming
3. Converts lambda names from PascalCase to kebab-case for example file names
4. Maintains a clear mapping between source fixtures and example files

You can also run the sync script manually if needed:

```bash
./bin/sync-examples.sh
```

## Benefits of Referencing Test Fixtures

1. **Accuracy**: Examples are based on real API behavior, not hypothetical scenarios
2. **Consistency**: Examples stay in sync with test fixtures via sync script
3. **Validation**: Examples are validated by actual test cases
4. **Maintainability**: Single source of truth - test fixtures drive documentation
5. **No Duplication**: Documentation references fixture files instead of copying content

## Adding New Examples

When adding new endpoints or modifying existing ones:

1. **Create API-specific fixtures** in `src/lambdas/[LambdaName]/test/fixtures/` using the naming convention:
   - `apiRequest-[METHOD]-[note].json` for request examples
   - `apiResponse-[METHOD]-[statusCode]-[note].json` for response examples
   
   Examples:
   - `apiRequest-POST-register.json`
   - `apiResponse-GET-200-OK.json`
   - `apiResponse-POST-201-Created.json`

2. **Auto-discovery**: The sync script will automatically discover these fixtures during the next documentation build

3. **Generate documentation**: Run `npm run document-api` to:
   - Automatically sync the new fixtures to `tsp/examples/`
   - Compile TypeSpec to OpenAPI
   - Generate Redoc HTML documentation

4. **Reference in TypeSpec**: Update operation comments to reference the new example files in `tsp/operations/operations.tsp`

## Example Reference Format in TypeSpec

Examples are referenced in operation documentation comments:

```typescript
/**
 * Operation description
 * 
 * Example request: See `tsp/examples/example-request.json`
 * (sourced from `src/lambdas/Lambda/test/fixtures/fixture.json`)
 * 
 * Example response: See `tsp/examples/example-response.json`
 * 
 * @returns Response description
 */
```

This approach:
- References example files instead of copying content
- Maintains traceability to source fixtures
- Ensures documentation stays in sync with test data
- Reduces maintenance burden
</file>

<file path="tsp/main.tsp">
import "@typespec/http";
import "@typespec/openapi3";
import "./models/models.tsp";
import "./operations/operations.tsp";

using TypeSpec.Http;

/**
 * Offline Media Downloader API
 * 
 * A serverless AWS media downloader service that downloads media content
 * (primarily YouTube videos) and integrates with a companion iOS app for
 * offline playback.
 */
@service(#{
  title: "Offline Media Downloader API",
})
@server("https://api.example.com", "Production server")
namespace OfflineMediaDownloader;

/**
 * Common error response structure
 */
@error
model ErrorResponse {
  /**
   * Error details
   */
  error: {
    /**
     * Error code
     */
    code: string;

    /**
     * Error message
     */
    message: string;
  };

  /**
   * Request ID for tracking
   */
  requestId: string;
}

/**
 * Unauthorized error response (401)
 */
@error
model UnauthorizedError {
  /**
   * Error details
   */
  error: {
    /**
     * Error code
     */
    code: "Unauthorized";

    /**
     * Error message
     */
    message: string;
  };

  /**
   * Request ID for tracking
   */
  requestId: string;
}

/**
 * Forbidden error response (403)
 */
@error
model ForbiddenError {
  /**
   * Error details
   */
  error: {
    /**
     * Error code
     */
    code: "Forbidden";

    /**
     * Error message
     */
    message: string;
  };

  /**
   * Request ID for tracking
   */
  requestId: string;
}

/**
 * Internal Server Error response (500)
 */
@error
model InternalServerError {
  /**
   * Error details
   */
  error: {
    /**
     * Error code
     */
    code: "InternalServerError";

    /**
     * Error message
     */
    message: string;
  };

  /**
   * Request ID for tracking
   */
  requestId: string;
}
</file>

<file path="tsp/README.md">
# API Documentation

This directory contains the TypeSpec definition for the Offline Media Downloader API.

## Table of Contents

- [About TypeSpec](#about-typespec)
- [Structure](#structure)
- [Compiling TypeSpec](#compiling-typespec)
- [API Endpoints](#api-endpoints)
- [Examples](#examples)
- [Viewing the Documentation](#viewing-the-documentation)
- [Maintenance](#maintenance)
- [Benefits of TypeSpec](#benefits-of-typespec)

## About TypeSpec

[TypeSpec](https://typespec.io/) is a language for defining APIs that can generate OpenAPI specifications, JSON schemas, and other API description formats. It provides a more maintainable and type-safe way to document APIs compared to writing OpenAPI YAML directly.

## Structure

- `main.tsp` - Main TypeSpec entry point and service definition
- `models/models.tsp` - Data models and types
- `operations/operations.tsp` - API operations and endpoints
- `tspconfig.yaml` - TypeSpec compiler configuration

## Generating Documentation

To generate and view API documentation from TypeSpec:

```bash
npm run document-api
```

This will:
1. Sync example files from test fixtures (automatically runs `bin/sync-examples.sh`)
2. Compile TypeSpec definitions to OpenAPI 3.0 specification (`docs/api/openapi.yaml`)
3. Generate a Redoc HTML documentation file (`docs/api/index.html`)
4. Automatically open the documentation in your default browser

The `document-api` command runs `bin/document-api.sh` which handles the entire workflow, including syncing examples from test fixtures. You don't need to run the sync script separately.

## API Endpoints

The API includes four main endpoint groups:

### Files (`/files`)
- **GET /files** - List all files available to the authenticated user

### Devices (`/device`)
- **POST /device/register** - Register a device for push notifications

### Webhooks (`/feedly`)
- **POST /feedly** - Process Feedly webhook to download media

### Authentication (`/user`)
- **POST /user/register** - Register new user with Sign in with Apple
- **POST /user/login** - Login existing user with Sign in with Apple

## Examples

The TypeSpec definitions include example requests and responses based on actual test fixtures from the codebase. These examples are embedded in the documentation comments and demonstrate real-world usage of the API.

See [EXAMPLES.md](./EXAMPLES.md) for detailed information about how test fixtures are incorporated into the API documentation.

## Viewing the Documentation

You can view the generated OpenAPI specification using various tools:

1. **Swagger UI**: Upload the generated `openapi.yaml` to [Swagger Editor](https://editor.swagger.io/)
2. **Redoc**: Use Redoc CLI or online viewer
3. **VS Code**: Use the OpenAPI Preview extension

## Maintenance

When modifying the API:

1. Update the TypeSpec definitions in the `tsp/` directory
2. Run `npm run document-api` to regenerate the OpenAPI spec
3. Verify the generated documentation is correct
4. Include relevant examples from test fixtures

## Benefits of TypeSpec

- **Type Safety**: Catch errors at definition time, not runtime
- **DRY**: Define models once, reuse across endpoints
- **Maintainability**: Easier to read and maintain than raw OpenAPI YAML
- **Validation**: Built-in validation and error checking
- **Examples**: Integrated examples from test fixtures for better documentation
</file>

<file path="tsp/tspconfig.yaml">
emit:
  - "@typespec/openapi3"
options:
  "@typespec/openapi3":
    output-file: "{output-dir}/openapi.yaml"
    new-line: lf
</file>

<file path=".codecov.yml">
# Codecov configuration
# Docs: https://docs.codecov.com/docs/codecov-yaml
coverage:
  # Overall project coverage status
  status:
    project:
      default:
        target: 80%           # Goal: 80% coverage
        threshold: 1%         # Allow 1% degradation
        if_ci_failed: error   # Fail if CI failed
    # Coverage on changed lines in PR
    patch:
      default:
        target: 80%           # New code should be well-tested
        if_ci_failed: error
  # Round coverage percentage to 2 decimal places
  precision: 2
  # Coverage calculation method
  round: down
  range: 70..100
# Flag-based coverage tracking
flags:
  unit:
    paths:
      - src/lambdas/        # Lambda handlers
      - util/               # Shared utilities
    carryforward: true      # Use previous coverage if flag missing
  integration:
    paths:
      - lib/vendor/AWS/     # Vendor wrappers (primary target)
    carryforward: true
# PR comment configuration
comment:
  layout: "reach,diff,flags,files"
  behavior: default         # Update existing comment
  require_changes: false    # Always comment
  require_base: true        # Require base commit coverage
  require_head: true        # Require head commit coverage
# Ignore patterns (don't count toward coverage)
ignore:
  - "config/"               # Build configuration
  - "bin/"                  # Scripts
  - "build/"                # Build output
  - "dist/"                 # Distribution files
  - "terraform/"            # Infrastructure code
  - "pipeline/"             # CI/CD tests
  - "**/*.test.ts"          # Test files themselves
  - "**/*.integration.test.ts"
  - "test/"                 # Test infrastructure
# GitHub integration
github_checks:
  annotations: true         # Show coverage annotations in PR files
</file>

<file path=".editorconfig">
# EditorConfig helps maintain consistent coding styles
# https://editorconfig.org

root = true

[*]
indent_style = space
indent_size = 2
end_of_line = lf
charset = utf-8
trim_trailing_whitespace = true
insert_final_newline = true

[*.{ts,tsx,js,jsx,json}]
indent_size = 2

[*.md]
trim_trailing_whitespace = false

# shfmt configuration for Bash scripts
[*.sh]
indent_style = space
indent_size = 2
shell_variant = bash
binary_next_line = false
switch_case_indent = true
space_redirects = true
keep_padding = false
function_next_line = false

[Makefile]
indent_style = tab

[*.tf]
indent_size = 2
</file>

<file path=".sops.yaml">
creation_rules:
  # YAML and JSON files
  - path_regex: secrets\.yaml
    age: age18jlkflydzmmclcds8h5lrn8em6y8kldvuh4wf8zdsvdsyx6uaddsnavfhv
</file>

<file path="docker-compose.localstack.yml">
version: '3.8'
services:
  localstack:
    image: localstack/localstack:latest
    container_name: aws-media-downloader-localstack
    ports:
      - "4566:4566"
    environment:
      # Enable specific AWS services
      - SERVICES=s3,dynamodb,sns,sqs,lambda,cloudwatch,apigateway
      # Enable debug mode for troubleshooting
      - DEBUG=1
      # Use single edge port (4566) for all services
      - EDGE_PORT=4566
    volumes:
      # Mount Docker socket for Lambda execution
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - localstack-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4566/_localstack/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
networks:
  localstack-network:
    driver: bridge
</file>

<file path="tsconfig.test.json">
{
  "extends": "./tsconfig.json",
  "compilerOptions": {
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noImplicitAny": true,
    "noImplicitThis": true,
    "noImplicitReturns": true,
    "noFallthroughCasesInSwitch": true,
    "types": ["node", "jest"]
  },
  "include": [
    "src/**/*",
    "test/**/*"
  ],
  "exclude": [
    "node_modules"
  ]
}
</file>

<file path="tsdoc.json">
{
  "$schema": "https://developer.microsoft.com/json-schemas/tsdoc/v0/tsdoc.schema.json",
  "tagDefinitions": [
    {
      "tagName": "@notExported",
      "syntaxKind": "modifier"
    }
  ]
}
</file>

<file path=".claude/commands/upgrade-deps.md">
# Comprehensive Dependency Upgrade

Perform a full dependency upgrade workflow: update all packages, fix breaking changes, create PR, and clean up dependabot PRs.

## Pre-flight Checks

1. First, list all open dependabot PRs:
```bash
unset GITHUB_TOKEN && gh pr list --author "app/dependabot" --state open --json number,title,headRefName
```

2. Check outdated packages:
```bash
pnpm outdated
```

## Workflow Steps

### Phase 1: Worktree Setup

Create an isolated worktree for the upgrade work:

```bash
git worktree add -b chore/upgrade-dependencies ../aws-cloudformation-media-downloader-upgrade HEAD
cd ../aws-cloudformation-media-downloader-upgrade
mkdir -p build
pnpm install --frozen-lockfile
```

### Phase 2: Dependency Updates

**Step 2.1: Align AWS SDK packages first (critical)**

All AWS SDK packages must be at the same version. Update them together:

```bash
pnpm update @aws-sdk/client-api-gateway@latest \
            @aws-sdk/client-cloudwatch@latest \
            @aws-sdk/client-dynamodb@latest \
            @aws-sdk/client-lambda@latest \
            @aws-sdk/client-s3@latest \
            @aws-sdk/client-sns@latest \
            @aws-sdk/client-sqs@latest \
            @aws-sdk/lib-dynamodb@latest \
            @aws-sdk/lib-storage@latest \
            @aws-sdk/util-dynamodb@latest
```

**Step 2.2: Update all other dependencies**

```bash
pnpm update --latest
```

### Phase 3: Fix Breaking Changes

Run type checking to identify breaking changes:

```bash
pnpm run check-types
```

**Common breaking change patterns to watch for:**

| Package | Breaking Change | Fix |
|---------|----------------|-----|
| Jest 29‚Üí30 | `toThrowError` removed | Use `toThrow` instead |
| Jest 29‚Üí30 | Mock type annotations stricter | Add parameter types to `jest.fn<(params) => ReturnType>()` |
| Joi major | Schema API changes | Check changelog, update validation code |
| glob major | Sync API changes | May need async conversion |
| better-auth | New model names in minified output | Add to `excludedSourceVariables` in `infrastructure.environment.test.ts` |

### Phase 4: Verify

Run the full local CI:

```bash
pnpm run ci:local
```

If tests fail, fix the issues and re-run. The pre-push hook will run `ci:local:full` (including integration tests).

### Phase 5: Commit and Push

```bash
git add -A
git commit -m 'chore(deps): upgrade all dependencies to latest versions

- Align AWS SDK packages to X.X.X
- Update [list key packages]
- Fix [any breaking changes]
- Supersedes dependabot PRs #X, #Y, #Z'

git push -u origin chore/upgrade-dependencies
```

**IMPORTANT**: No AI attribution in commits per project conventions.

### Phase 6: Create PR

```bash
unset GITHUB_TOKEN && gh pr create \
  --title "chore(deps): comprehensive dependency upgrade" \
  --body '## Summary
- Upgrades all outdated dependencies to latest versions
- Aligns AWS SDK packages
- Supersedes dependabot PRs #X, #Y, #Z

## Test Plan
- [x] Local CI passes
- [ ] GitHub CI passes'
```

### Phase 7: Monitor CI and Merge

```bash
# Watch CI status
unset GITHUB_TOKEN && gh pr checks --watch

# After all checks pass, merge with squash
unset GITHUB_TOKEN && gh pr merge --squash --delete-branch
```

### Phase 8: Close Dependabot PRs

Close each superseded dependabot PR:

```bash
unset GITHUB_TOKEN && gh pr close <PR_NUMBER> --comment "Superseded by comprehensive dependency upgrade in #<NEW_PR>"
```

### Phase 9: Cleanup

```bash
# Return to main repo
cd /Users/jlloyd/Repositories/aws-cloudformation-media-downloader

# Pull merged changes
git fetch origin && git pull origin master

# Remove worktree
git worktree remove ../aws-cloudformation-media-downloader-upgrade --force

# Delete local branch if still exists
git branch -D chore/upgrade-dependencies 2>/dev/null || true

# Verify cleanup
git worktree list
```

## Notes

- **GitHub auth**: If you see 401 errors, the `GITHUB_TOKEN` env var may be invalid. Use `unset GITHUB_TOKEN` to fall back to keyring auth.
- **AWS SDK alignment**: Per `docs/wiki/Methodologies/Dependabot-Resolution.md`, all AWS SDK packages must be updated together.
- **Major version upgrades**: Check changelogs for Jest, Joi, glob, and other major bumps before proceeding.
- **Pre-push hook**: The project runs `ci:local:full` on push, which includes integration tests with LocalStack.
</file>

<file path=".github/workflows/dependency-check.yml">
name: Dependency Check
on:
  push:
    branches: [main, master, develop]
  pull_request:
    branches: [main, master, develop]
jobs:
  dependency-check:
    name: Validate Dependencies
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Setup pnpm
        uses: pnpm/action-setup@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22.18.0'
          cache: 'pnpm'
      - name: Install dependencies
        run: pnpm install --frozen-lockfile
      - name: Generate dependency graph
        run: pnpm run generate-graph
      - name: Check dependency rules
        run: pnpm run deps:check
      - name: Validate GraphRAG is current
        run: ./bin/validate-graphrag.sh
      - name: Generate dependency report
        if: failure()
        run: |
          pnpm run deps:report || true
          echo "Dependency violations found. See job summary for details."
      - name: Upload dependency report
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: dependency-report
          path: dependency-report.html
      - name: Comment on PR
        if: failure() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{secrets.GITHUB_TOKEN}}
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '‚ùå **Dependency Check Failed**\n\nArchitectural violations were detected. Please review the dependency report artifact for details.\n\nCommon issues:\n- Circular dependencies\n- Cross-Lambda imports\n- Direct AWS SDK imports (use vendor wrappers)\n- Entity cross-dependencies'
            })
</file>

<file path=".github/workflows/update-yt-dlp.yml">
name: Update yt-dlp Binary
on:
  schedule:
    - cron: '0 2 * * 0'  # Weekly on Sunday at 2am UTC
  workflow_dispatch:  # Allow manual triggering
permissions:
  contents: write
  pull-requests: write
jobs:
  check-update:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Check for yt-dlp updates
        id: check
        run: |
          set -e
          echo "Fetching latest stable yt-dlp release (excluding pre-releases)..."
          LATEST=$(gh api repos/yt-dlp/yt-dlp/releases \
            --jq '[.[] | select(.prerelease == false)][0].tag_name')
          CURRENT=$(cat layers/yt-dlp/VERSION 2>/dev/null | tr -d '[:space:]' || echo "none")
          echo "latest=${LATEST}" >> $GITHUB_OUTPUT
          echo "current=${CURRENT}" >> $GITHUB_OUTPUT
          if [ "$LATEST" != "$CURRENT" ]; then
            echo "update_needed=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Update available: ${CURRENT} ‚Üí ${LATEST}"
          else
            echo "update_needed=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è  Already on latest version: ${CURRENT}"
          fi
        env:
          GH_TOKEN: ${{ github.token }}
      - name: Download and verify binary for testing
        if: steps.check.outputs.update_needed == 'true'
        run: |
          set -e
          VERSION="${{ steps.check.outputs.latest }}"
          echo "Downloading yt-dlp ${VERSION} for verification..."
          wget -q "https://github.com/yt-dlp/yt-dlp/releases/download/${VERSION}/yt-dlp_linux"
          wget -q "https://github.com/yt-dlp/yt-dlp/releases/download/${VERSION}/SHA2-256SUMS"
          echo "Verifying SHA256 checksum..."
          grep "yt-dlp_linux$" SHA2-256SUMS | sha256sum --check --status
          echo "Testing binary execution..."
          chmod +x yt-dlp_linux
          BINARY_VERSION=$(./yt-dlp_linux --version)
          if [ "${BINARY_VERSION}" != "${VERSION}" ]; then
            echo "ERROR: Binary version mismatch (expected: ${VERSION}, got: ${BINARY_VERSION})"
            exit 1
          fi
          echo "Testing format listing capability..."
          ./yt-dlp_linux --list-formats "https://www.youtube.com/watch?v=jNQXAC9IVRw" --no-warnings --quiet || {
            echo "ERROR: Binary failed format listing test"
            exit 1
          }
          rm -f yt-dlp_linux SHA2-256SUMS
          echo "‚úÖ Binary verification complete"
      - name: Update VERSION file and create PR
        if: steps.check.outputs.update_needed == 'true'
        run: |
          set -e
          readonly VERSION="${{ steps.check.outputs.latest }}"
          readonly CURRENT_VERSION="${{ steps.check.outputs.current }}"
          readonly BRANCH_NAME="chore/update-yt-dlp-${VERSION}"
          echo "Updating VERSION file to ${VERSION}..."
          echo "${VERSION}" > layers/yt-dlp/VERSION
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git checkout -b "${BRANCH_NAME}"
          git add layers/yt-dlp/VERSION
          git commit -m "chore(deps): update yt-dlp to ${VERSION}"
          git push origin "${BRANCH_NAME}"
          cat > /tmp/pr-body.md << EOF
          Automated update of yt-dlp version to ${VERSION}.
          **Release Notes**: https://github.com/yt-dlp/yt-dlp/releases/tag/${VERSION}
          ## Changes
          - Updated VERSION file from ${CURRENT_VERSION} to ${VERSION}
          - Binary will be downloaded by OpenTofu during deployment
          ## Pre-merge Verification
          - ‚úÖ Binary downloaded and SHA256 checksum verified
          - ‚úÖ Binary execution test passed (\`--version\`)
          - ‚úÖ Format listing capability verified
          - ‚è≥ CI/CD tests pending
          ## Deployment Process
          1. Merge this PR to update the VERSION file
          2. Run \`npm run deploy\` to trigger binary download via \`null_resource.DownloadYtDlpBinary\`
          3. OpenTofu will download, verify, and package the new binary into the Lambda layer
          4. Monitor CloudWatch logs for any download issues after deployment
          ## Rollback Procedure
          If issues occur after deployment:
          \`\`\`bash
          echo "${CURRENT_VERSION}" > layers/yt-dlp/VERSION
          git commit -am "chore(deps): revert yt-dlp to ${CURRENT_VERSION}"
          git push
          npm run deploy
          \`\`\`
          EOF
          gh pr create \
            --title "chore(deps): update yt-dlp to ${VERSION}" \
            --body-file /tmp/pr-body.md \
            --label "dependencies" \
            --label "automated" \
            --label "yt-dlp" \
            --label "infrastructure"
        env:
          GH_TOKEN: ${{ github.token }}
      - name: Notify on failure
        if: failure()
        run: |
          gh issue create \
            --title "yt-dlp update workflow failed" \
            --body "The automated yt-dlp update workflow failed. Check [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})." \
            --label "bug" \
            --label "automation" \
            --label "yt-dlp"
        env:
          GH_TOKEN: ${{ github.token }}
</file>

<file path=".husky/pre-push">
#!/usr/bin/env bash
# pre-push hook: Block direct master pushes and run CI checks
# This enforces the worktree workflow and ensures code quality
#
# To bypass (emergency only): git push --no-verify

# Get the current branch name
CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD)

# Block direct pushes to master
if [ "$CURRENT_BRANCH" = "master" ] || [ "$CURRENT_BRANCH" = "main" ]; then
  echo ""
  echo "ERROR: Direct push to '$CURRENT_BRANCH' is blocked."
  echo ""
  echo "This project requires the worktree workflow:"
  echo "  1. Create a worktree with a feature branch"
  echo "  2. Make changes in the worktree"
  echo "  3. Push the feature branch"
  echo "  4. Create a PR and merge via squash-and-merge"
  echo ""
  echo "See: docs/wiki/Conventions/Git-Workflow.md"
  echo ""
  echo "To bypass (emergency only): git push --no-verify"
  exit 1
fi

echo "Running pre-push checks (ci:local:full)..."
echo "This may take 5-10 minutes. Use 'git push --no-verify' to bypass in emergencies."
echo ""

pnpm run ci:local:full
</file>

<file path="bin/ci-local-full.sh">
#!/usr/bin/env bash
# ci-local-full.sh
# Full local CI runner - runs all CI checks INCLUDING integration tests
# Usage: pnpm run ci:local:full or ./bin/ci-local-full.sh
#
# This script provides complete CI parity by running:
# 1. All fast CI checks (via ci-local.sh)
# 2. Integration tests against LocalStack
set -e # Exit on error
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
# Color constants
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'
# Track timing
START_TIME=$(date +%s)
echo -e "${GREEN}Local CI Runner (Full Mode)${NC}"
echo "============================="
echo ""
echo -e "${BLUE}This runs ALL CI checks including integration tests.${NC}"
echo -e "${BLUE}For faster iteration, use 'pnpm run ci:local' instead.${NC}"
echo ""
cd "$PROJECT_ROOT"
# Run fast CI checks first
echo -e "${YELLOW}Phase 1: Running fast CI checks...${NC}"
echo ""
./bin/ci-local.sh
# Run integration tests
echo ""
echo -e "${YELLOW}Phase 2: Running integration tests...${NC}"
echo ""
./bin/test-integration.sh --cleanup
# Calculate duration
END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))
MINUTES=$((DURATION / 60))
SECONDS=$((DURATION % 60))
# Summary
echo ""
echo -e "${GREEN}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
echo -e "${GREEN}Full Local CI Complete${NC}"
echo -e "${GREEN}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
echo ""
echo "All checks passed in ${MINUTES}m ${SECONDS}s"
echo ""
echo "What was checked:"
echo "  Everything from ci:local PLUS integration tests"
echo ""
echo "GitHub-specific checks (cannot be run locally):"
echo "  Codecov upload, artifact storage, PR comments"
echo ""
echo -e "${GREEN}Ready to push with confidence!${NC}"
</file>

<file path="bin/extract-fixtures.sh">
#!/bin/bash
# Extract test fixtures from CloudWatch logs
# Usage: ./bin/extract-fixtures.sh [days-back] [output-dir]
set -euo pipefail
# Configuration
DAYS_BACK=${1:-7}
OUTPUT_DIR=${2:-test/fixtures/raw}
START_TIME=$(($(date +%s) - (DAYS_BACK * 86400)))000
# Lambda functions to extract fixtures from (only instrumented Lambdas)
LAMBDA_FUNCTIONS=(
  "WebhookFeedly"
  "ListFiles"
  "LoginUser"
  "RefreshToken"
  "RegisterDevice"
  "UserDelete"
  "UserSubscribe"
)
# Create output directory
mkdir -p "${OUTPUT_DIR}"
echo "Extracting fixtures from CloudWatch logs (last ${DAYS_BACK} days)..."
echo "Output directory: ${OUTPUT_DIR}"
echo ""
# Extract fixtures for each Lambda function
for LAMBDA_NAME in "${LAMBDA_FUNCTIONS[@]}"; do
  echo "Processing ${LAMBDA_NAME}..."
  LOG_GROUP="/aws/lambda/${LAMBDA_NAME}"
  OUTPUT_FILE="${OUTPUT_DIR}/${LAMBDA_NAME}-$(date +%Y%m%d).json"
  # Check if log group exists
  if ! aws logs describe-log-groups --log-group-name-prefix "${LOG_GROUP}" --query 'logGroups[0].logGroupName' --output text 2> /dev/null | grep -q "${LOG_GROUP}"; then
    echo "  ‚ö†Ô∏è  Log group ${LOG_GROUP} not found, skipping..."
    continue
  fi
  # Extract fixture markers from CloudWatch
  aws logs filter-log-events \
    --log-group-name "${LOG_GROUP}" \
    --filter-pattern '__FIXTURE_MARKER__' \
    --start-time "${START_TIME}" \
    --query 'events[*].message' \
    --output json |
    jq -r '.[] | fromjson' \
      > "${OUTPUT_FILE}" 2> /dev/null || true
  # Count extracted fixtures
  FIXTURE_COUNT=$(jq -s 'length' "${OUTPUT_FILE}" 2> /dev/null || echo "0")
  if [ "${FIXTURE_COUNT}" -eq 0 ]; then
    echo "  ‚ÑπÔ∏è  No fixtures found"
    rm -f "${OUTPUT_FILE}"
  else
    echo "  ‚úÖ Extracted ${FIXTURE_COUNT} fixtures ‚Üí ${OUTPUT_FILE}"
  fi
done
echo ""
echo "Extraction complete. Next step: pnpm run process-fixtures"
</file>

<file path="bin/extract-production-fixtures.sh">
#!/bin/bash
# Extract production fixtures from CloudWatch logs - LOCAL VERSION
# This script replaces the GitHub workflow for manual fixture extraction
#
# Prerequisites:
#   - AWS CLI configured with production credentials
#   - pnpm dependencies installed
#
# Usage:
#   ./bin/extract-production-fixtures.sh [days-back] [create-pr]
#
# Examples:
#   ./bin/extract-production-fixtures.sh                # Extract last 7 days, no PR
#   ./bin/extract-production-fixtures.sh 14             # Extract last 14 days, no PR
#   ./bin/extract-production-fixtures.sh 7 true         # Extract last 7 days and create PR
set -euo pipefail
# Configuration
DAYS_BACK=${1:-7}
CREATE_PR=${2:-false}
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "${SCRIPT_DIR}")"
# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color
# Change to project root
cd "${PROJECT_ROOT}"
echo -e "${BLUE}=== Production Fixture Extraction ===${NC}"
echo -e "Days to extract: ${YELLOW}${DAYS_BACK}${NC}"
echo -e "Create PR: ${YELLOW}${CREATE_PR}${NC}"
echo ""
# Step 1: Verify AWS credentials
echo -e "${BLUE}[1/5] Verifying AWS credentials...${NC}"
if ! aws sts get-caller-identity > /dev/null 2>&1; then
  echo -e "${RED}Error: AWS credentials not configured${NC}"
  echo "Please configure AWS CLI with production credentials:"
  echo "  aws configure"
  exit 1
fi
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
echo -e "${GREEN}‚úÖ Connected to AWS account: ${ACCOUNT_ID}${NC}"
# Step 2: Extract fixtures from CloudWatch
echo -e "\n${BLUE}[2/5] Extracting fixtures from CloudWatch...${NC}"
chmod +x "${SCRIPT_DIR}/extract-fixtures.sh"
"${SCRIPT_DIR}/extract-fixtures.sh" "${DAYS_BACK}"
# Step 3: Process fixtures
echo -e "\n${BLUE}[3/5] Processing fixtures...${NC}"
chmod +x "${SCRIPT_DIR}/process-fixtures.js"
node "${SCRIPT_DIR}/process-fixtures.js"
# Step 4: Check for changes
echo -e "\n${BLUE}[4/5] Checking for changes...${NC}"
if git diff --quiet test/fixtures/api-contracts/; then
  echo -e "${YELLOW}‚ÑπÔ∏è  No fixture changes detected${NC}"
  echo "No updates needed."
  exit 0
else
  echo -e "${GREEN}‚úÖ Fixture changes detected${NC}"
  # Show summary of changes
  echo -e "\n${BLUE}Changed files:${NC}"
  git diff --name-status test/fixtures/api-contracts/ | while read -r status file; do
    case "$status" in
      A) echo -e "  ${GREEN}+ ${file}${NC}" ;;
      M) echo -e "  ${YELLOW}~ ${file}${NC}" ;;
      D) echo -e "  ${RED}- ${file}${NC}" ;;
      *) echo "  ${status} ${file}" ;;
    esac
  done || true
fi
# Step 5: Create PR or commit changes
echo -e "\n${BLUE}[5/5] Finalizing changes...${NC}"
if [[ "${CREATE_PR}" == "true" ]]; then
  # Create a new branch and PR
  BRANCH_NAME="fixtures/manual-$(date +%Y%m%d-%H%M%S)"
  echo -e "Creating branch: ${YELLOW}${BRANCH_NAME}${NC}"
  git checkout -b "${BRANCH_NAME}"
  # Stage changes
  git add test/fixtures/api-contracts/
  # Commit
  COMMIT_MSG="chore: update fixtures from production CloudWatch logs
Extraction Details:
- Date: $(date)
- Days extracted: ${DAYS_BACK}
- AWS Account: ${ACCOUNT_ID}"
  git commit -m "${COMMIT_MSG}"
  # Push branch
  echo -e "\n${BLUE}Pushing branch...${NC}"
  git push -u origin "${BRANCH_NAME}"
  # Create PR using GitHub CLI if available
  if command -v gh &> /dev/null; then
    echo -e "\n${BLUE}Creating pull request...${NC}"
    PR_BODY="## Automated Fixture Update
This PR contains fixtures extracted from production CloudWatch logs.
### Extraction Details
- **Date**: $(date)
- **Days extracted**: ${DAYS_BACK}
- **AWS Account**: ${ACCOUNT_ID}
### What's Changed
- Production API request/response fixtures updated
- Fixtures deduplicated by structural similarity
- PII and sensitive data redacted
### Review Checklist
- [ ] Verify fixtures match expected API contracts
- [ ] Check that PII/secrets are properly redacted
- [ ] Ensure no breaking changes in API structure
- [ ] Validate fixture quality and completeness
### How to Test
\`\`\`bash
pnpm test
pnpm run test:integration
\`\`\`"
    gh pr create \
      --title "üîÑ Manual Fixture Update from Production" \
      --body "${PR_BODY}" \
      --label "fixtures,testing" \
      --draft
    echo -e "${GREEN}‚úÖ Pull request created${NC}"
  else
    echo -e "${YELLOW}GitHub CLI not installed. Please create PR manually:${NC}"
    echo "  https://github.com/$(git remote get-url origin | sed 's/.*github.com[:/]\(.*\)\.git/\1/')/pull/new/${BRANCH_NAME}"
  fi
else
  # Just show the diff
  echo -e "${YELLOW}Changes ready to commit:${NC}"
  echo "To stage changes: git add test/fixtures/api-contracts/"
  echo "To create PR: $0 ${DAYS_BACK} true"
fi
echo -e "\n${GREEN}‚úÖ Fixture extraction complete!${NC}"
</file>

<file path="bin/sync-examples.sh">
#!/bin/bash
# Sync Examples Script
# This script syncs test fixtures to TypeSpec examples directory
# ensuring documentation stays in sync with actual test data
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
EXAMPLES_DIR="$PROJECT_DIR/tsp/examples"
echo "üîÑ Syncing test fixtures to TypeSpec examples..."
echo ""
# Create examples directory if it doesn't exist
mkdir -p "$EXAMPLES_DIR"
# Find all apiRequest and apiResponse fixtures in lambda test directories
echo "üìù Scanning for API fixtures..."
# Track files synced
FILES_SYNCED=0
# Find all apiRequest-* and apiResponse-* files
for fixture_file in $(find "$PROJECT_DIR/src/lambdas" -type f -name "apiRequest-*.json" -o -name "apiResponse-*.json" | sort); do
  # Extract just the filename
  filename=$(basename "$fixture_file")
  # Extract lambda name from path
  lambda_name=$(echo "$fixture_file" | sed 's|.*/lambdas/\([^/]*\)/.*|\1|')
  # Create a descriptive name for the example file
  # Convert from apiRequest-POST-device.json to something like register-device-request.json
  if [[ "$filename" =~ ^apiRequest-(.*)\.json$ ]]; then
    # This is a request
    method_and_type="${BASH_REMATCH[1]}"
    # Convert lambda name to kebab-case and add -request suffix
    example_name=$(echo "$lambda_name" | sed 's/\([A-Z]\)/-\L\1/g' | sed 's/^-//')
    example_file="$EXAMPLES_DIR/${example_name}-request.json"
  elif [[ "$filename" =~ ^apiResponse-(.*)\.json$ ]]; then
    # This is a response
    method_and_status="${BASH_REMATCH[1]}"
    # Convert lambda name to kebab-case and add -response suffix
    example_name=$(echo "$lambda_name" | sed 's/\([A-Z]\)/-\L\1/g' | sed 's/^-//')
    example_file="$EXAMPLES_DIR/${example_name}-response.json"
  else
    continue
  fi
  # Copy the fixture to examples directory
  echo "   Syncing: $lambda_name -> $(basename "$example_file")"
  cp "$fixture_file" "$example_file"
  FILES_SYNCED=$((FILES_SYNCED + 1))
done
echo ""
echo "‚úÖ Examples synced successfully!"
echo ""
echo "üìç Examples location: $EXAMPLES_DIR"
echo "üìä Files synced: $FILES_SYNCED"
ls -lh "$EXAMPLES_DIR" | tail -n +2 | awk '{print "   -", $9, "("$5")"}'
</file>

<file path="bin/update-agents-prs.sh">
#!/usr/bin/env bash
# update-agents-prs.sh
# Updates AGENTS.md with recent significant PRs
# Usage: pnpm run update:agents-prs or ./bin/update-agents-prs.sh
#
# This script:
#   1. Fetches last 10 merged PRs via gh CLI
#   2. Filters to significant PRs (feat/fix/refactor)
#   3. Updates the Recent PRs section in AGENTS.md
#   4. Commits changes if in CI mode
#
# Issue #146: CLAUDE.md Evolution
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
# Color constants
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'
echo -e "${YELLOW}Updating Recent PRs in AGENTS.md...${NC}"
cd "$PROJECT_ROOT"
# Check for gh CLI
if ! command -v gh &> /dev/null; then
  echo -e "${RED}Error: GitHub CLI (gh) is not installed${NC}"
  echo "Install with: brew install gh"
  exit 1
fi
# Check if authenticated
if ! gh auth status &> /dev/null 2>&1; then
  echo -e "${YELLOW}Warning: Not authenticated with GitHub CLI${NC}"
  echo "Some features may not work. Run: gh auth login"
fi
# Fetch recent merged PRs
echo "Fetching recent PRs..."
PRS=$(gh pr list --state merged --limit 10 --json number,title,mergedAt,author --jq '
  .[] |
  select(.title | test("^(feat|fix|refactor|perf|docs)")) |
  "- **#\(.number)**: \(.title) (@\(.author.login), \(.mergedAt | split("T")[0]))"
' 2> /dev/null || echo "")
if [ -z "$PRS" ]; then
  echo -e "${YELLOW}No significant PRs found or gh CLI not authenticated${NC}"
  exit 0
fi
# Take only the first 5
PRS=$(echo "$PRS" | head -5)
# Generate the section content
SECTION_CONTENT="## Recent Significant PRs
_Auto-updated by CI - last updated: $(date +%Y-%m-%d)_
$PRS"
# Check if section already exists in AGENTS.md
if grep -q "## Recent Significant PRs" AGENTS.md; then
  echo "Updating existing Recent PRs section..."
  # Create temp file with updated content
  awk -v new_section="$SECTION_CONTENT" '
    BEGIN { in_section = 0 }
    /^## Recent Significant PRs/ {
      print new_section
      in_section = 1
      next
    }
    in_section && /^## / {
      in_section = 0
    }
    !in_section { print }
  ' AGENTS.md > AGENTS.md.tmp
  mv AGENTS.md.tmp AGENTS.md
else
  echo "Adding new Recent PRs section..."
  # Insert before "## Development Workflow" section
  awk -v new_section="$SECTION_CONTENT" '
    /^## Development Workflow/ {
      print new_section
      print ""
    }
    { print }
  ' AGENTS.md > AGENTS.md.tmp
  mv AGENTS.md.tmp AGENTS.md
fi
echo -e "${GREEN}AGENTS.md updated with recent PRs${NC}"
# In CI mode, commit the changes
if [ "$CI" = "true" ]; then
  echo "CI mode detected, checking for changes..."
  if git diff --quiet AGENTS.md; then
    echo "No changes to commit"
  else
    echo "Committing AGENTS.md updates..."
    git config --local user.email "github-actions[bot]@users.noreply.github.com"
    git config --local user.name "github-actions[bot]"
    git add AGENTS.md
    git commit -m "docs: auto-update AGENTS.md recent PRs section"
    echo -e "${GREEN}Changes committed${NC}"
  fi
fi
</file>

<file path="bin/validate-doc-sync.sh">
#!/usr/bin/env bash
# validate-doc-sync.sh
# Validates documentation stays in sync with codebase
# Usage: pnpm run validate:doc-sync or ./bin/validate-doc-sync.sh
#
# This script detects documentation drift by checking:
#   1. Entity count matches src/entities/*.ts files
#   2. Lambda count matches trigger table in AGENTS.md
#   3. MCP rule count matches documentation
#   4. Documented paths exist in filesystem
#   5. No stale patterns (Prettier, wrong vendor path)
#   6. GraphRAG metadata includes all entities
#   7. Wiki internal links resolve
#
# Issue #145: Living Documentation System with Stale Page Detection
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
# Color constants
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'
echo -e "${YELLOW}Validating documentation sync...${NC}"
echo ""
cd "$PROJECT_ROOT"
ERRORS=""
WARNINGS=""
# =============================================================================
# Check 1: Entity count matches documentation
# =============================================================================
echo -n "  [1/7] Checking entity count... "
ENTITY_COUNT=$(find src/entities -name "*.ts" ! -name "*.test.ts" ! -name "index.ts" 2> /dev/null | wc -l | tr -d ' ')
# Count entity files listed in AGENTS.md between entities/ and lambdas/ sections
# Each entity file is listed with ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ or ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ
DOCUMENTED_ENTITY_COUNT=$(awk '/entities\/.*ElectroDB/,/lambdas\/.*Lambda/' AGENTS.md 2> /dev/null | grep -E '‚îÇ.*\.ts' | wc -l | tr -d ' ')
if [ "$ENTITY_COUNT" -ne "$DOCUMENTED_ENTITY_COUNT" ]; then
  echo -e "${RED}MISMATCH${NC}"
  ERRORS="$ERRORS\n  - Entity count: found $ENTITY_COUNT files in src/entities/, documented $DOCUMENTED_ENTITY_COUNT in AGENTS.md"
else
  echo -e "${GREEN}OK${NC} ($ENTITY_COUNT entities)"
fi
# =============================================================================
# Check 2: Lambda count matches documentation
# =============================================================================
echo -n "  [2/7] Checking Lambda count... "
LAMBDA_COUNT=$(find src/lambdas -mindepth 1 -maxdepth 1 -type d 2> /dev/null | wc -l | tr -d ' ')
# Count rows in Lambda Trigger Patterns table (lines starting with | and uppercase letter, excluding header)
# Skip lines containing "Trigger Type" or "---" (header/separator rows)
TRIGGER_TABLE_COUNT=$(awk '/### Lambda Trigger Patterns/,/### Data Access/' AGENTS.md 2> /dev/null | grep -E '^\| [A-Z]' | grep -v 'Trigger Type' | grep -v '\-\-\-' | wc -l | tr -d ' ')
if [ "$LAMBDA_COUNT" -ne "$TRIGGER_TABLE_COUNT" ]; then
  echo -e "${RED}MISMATCH${NC}"
  ERRORS="$ERRORS\n  - Lambda count: found $LAMBDA_COUNT directories in src/lambdas/, documented $TRIGGER_TABLE_COUNT in trigger table"
else
  echo -e "${GREEN}OK${NC} ($LAMBDA_COUNT Lambdas)"
fi
# =============================================================================
# Check 3: MCP validation rule count
# =============================================================================
echo -n "  [3/7] Checking MCP rule count... "
MCP_RULE_COUNT=$(find src/mcp/validation/rules -name "*.ts" ! -name "*.test.ts" ! -name "index.ts" ! -name "types.ts" 2> /dev/null | wc -l | tr -d ' ')
# Count rules in the allRules array by counting lines ending with "Rule" or "Rule,"
# This counts the actual rule references in the array
REGISTERED_RULE_COUNT=$(sed -n '/export const allRules/,/^]/p' src/mcp/validation/index.ts 2> /dev/null | grep -cE '[a-z]Rule,?$' || echo "0")
if [ "$MCP_RULE_COUNT" -ne "$REGISTERED_RULE_COUNT" ]; then
  echo -e "${RED}MISMATCH${NC}"
  ERRORS="$ERRORS\n  - MCP rules: found $MCP_RULE_COUNT rule files, $REGISTERED_RULE_COUNT registered in index.ts"
else
  echo -e "${GREEN}OK${NC} ($MCP_RULE_COUNT rules)"
fi
# =============================================================================
# Check 4: Critical paths exist
# =============================================================================
echo -n "  [4/7] Checking documented paths exist... "
PATHS_OK=true
REQUIRED_PATHS=(
  "src/lib/vendor/AWS"
  "src/lib/vendor/BetterAuth"
  "src/lib/vendor/ElectroDB"
  "src/mcp"
  "src/mcp/validation"
  "test/helpers"
  "graphrag"
)
for path in "${REQUIRED_PATHS[@]}"; do
  if [ ! -e "$path" ]; then
    ERRORS="$ERRORS\n  - Missing documented path: $path"
    PATHS_OK=false
  fi
done
if [ "$PATHS_OK" = true ]; then
  echo -e "${GREEN}OK${NC}"
else
  echo -e "${RED}MISSING${NC}"
fi
# =============================================================================
# Check 5: Forbidden patterns in AGENTS.md
# =============================================================================
echo -n "  [5/7] Checking for stale patterns... "
STALE_OK=true
# Check for old Prettier reference (should be dprint)
if grep -q "Prettier" AGENTS.md 2> /dev/null; then
  ERRORS="$ERRORS\n  - AGENTS.md references 'Prettier' but project uses 'dprint'"
  STALE_OK=false
fi
# Check for wrong vendor path (lib/vendor without src/ prefix, not in a comment)
# Only flag if there's NO src/lib/vendor reference anywhere
if ! grep -q "src/lib/vendor" AGENTS.md 2> /dev/null; then
  ERRORS="$ERRORS\n  - AGENTS.md missing src/lib/vendor reference"
  STALE_OK=false
fi
if [ "$STALE_OK" = true ]; then
  echo -e "${GREEN}OK${NC}"
else
  echo -e "${RED}STALE${NC}"
fi
# =============================================================================
# Check 6: GraphRAG metadata completeness
# =============================================================================
echo -n "  [6/7] Checking GraphRAG metadata... "
GRAPHRAG_OK=true
# Get entity names from filesystem (excluding Collections.ts which is a service, not entity)
FS_ENTITIES=$(find src/entities -name "*.ts" ! -name "*.test.ts" ! -name "index.ts" ! -name "Collections.ts" -exec basename {} .ts \; 2> /dev/null | sort)
# Check each entity appears in metadata.json entityRelationships
for entity in $FS_ENTITIES; do
  if ! grep -q "\"$entity\"" graphrag/metadata.json 2> /dev/null; then
    WARNINGS="$WARNINGS\n  - Entity '$entity' not found in graphrag/metadata.json entityRelationships"
    GRAPHRAG_OK=false
  fi
done
if [ "$GRAPHRAG_OK" = true ]; then
  echo -e "${GREEN}OK${NC}"
else
  echo -e "${YELLOW}INCOMPLETE${NC} (warning only)"
fi
# =============================================================================
# Check 7: Wiki internal links resolve
# =============================================================================
echo -n "  [7/7] Checking wiki links... "
WIKI_OK=true
BROKEN_LINKS=""
# Find all markdown files in docs/wiki and check their links
while IFS= read -r md_file; do
  # Extract relative markdown links: [text](path.md) or [text](../path.md)
  # Filter out code blocks first (``` fenced blocks) to avoid false positives from examples
  while IFS= read -r link; do
    # Skip empty results
    [ -z "$link" ] && continue
    # Skip external links and anchors
    [[ "$link" == http* ]] && continue
    [[ "$link" == "#"* ]] && continue
    # Remove anchor from link if present
    link_path="${link%%#*}"
    # Skip if empty after removing anchor
    [ -z "$link_path" ] && continue
    # Resolve relative path from the markdown file's directory
    md_dir=$(dirname "$md_file")
    target_path="$md_dir/$link_path"
    # Normalize and check if file exists
    if [ ! -f "$target_path" ]; then
      BROKEN_LINKS="$BROKEN_LINKS\n  - $md_file: broken link to '$link_path'"
      WIKI_OK=false
    fi
  done < <(awk 'BEGIN{c=0; bt=sprintf("%c",96); pat="^" bt bt bt} $0 ~ pat {c=1-c; next} c==0{print}' "$md_file" 2> /dev/null | grep -oE '\]\([^)]+\.md[^)]*\)' | sed 's/\](\([^)]*\))/\1/' | sed 's/#.*//' || true)
done < <(find docs/wiki -name "*.md" 2> /dev/null)
if [ "$WIKI_OK" = true ]; then
  echo -e "${GREEN}OK${NC}"
else
  echo -e "${YELLOW}BROKEN LINKS${NC} (warning only)"
  WARNINGS="$WARNINGS$BROKEN_LINKS"
fi
# =============================================================================
# Summary
# =============================================================================
echo ""
# Show warnings if any
if [ -n "$WARNINGS" ]; then
  echo -e "${YELLOW}Warnings:${NC}"
  echo -e "$WARNINGS"
  echo ""
fi
# Fail on errors
if [ -n "$ERRORS" ]; then
  echo -e "${RED}Documentation sync validation failed:${NC}"
  echo -e "$ERRORS"
  echo ""
  echo "To fix these issues:"
  echo "  1. Update AGENTS.md to reflect current codebase state"
  echo "  2. Update graphrag/metadata.json if entities changed"
  echo "  3. Run 'pnpm run graphrag:extract' to regenerate knowledge graph"
  echo "  4. Fix any broken wiki links"
  exit 1
fi
echo -e "${GREEN}Documentation is in sync with codebase${NC}"
</file>

<file path="bin/validate-docs.sh">
#!/usr/bin/env bash
# validate-docs.sh
# Validates that documented pnpm scripts in AGENTS.md and README.md exist in package.json
# Usage: pnpm run validate:docs or ./bin/validate-docs.sh
set -e # Exit on error
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
# Color constants
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'
echo -e "${YELLOW}Validating documented scripts...${NC}"
echo ""
cd "$PROJECT_ROOT"
MISSING_SCRIPTS=""
# Check AGENTS.md for pnpm run commands (supports camelCase like registerDevice)
if [ -f "AGENTS.md" ]; then
  for script in $(grep -oE 'pnpm run [a-zA-Z:-]+' AGENTS.md 2> /dev/null | sed 's/pnpm run //' | sort -u); do
    if ! jq -e ".scripts[\"$script\"]" package.json > /dev/null 2>&1; then
      MISSING_SCRIPTS="$MISSING_SCRIPTS $script"
    fi
  done
fi
# Check README.md for pnpm/npm run commands (supports camelCase like registerDevice)
if [ -f "README.md" ]; then
  for script in $(grep -oE '(pnpm|npm) run [a-zA-Z:-]+' README.md 2> /dev/null | sed 's/.*run //' | sort -u); do
    if ! jq -e ".scripts[\"$script\"]" package.json > /dev/null 2>&1; then
      MISSING_SCRIPTS="$MISSING_SCRIPTS $script"
    fi
  done
fi
if [ -n "$MISSING_SCRIPTS" ]; then
  echo -e "${RED}ERROR: The following scripts are documented but not in package.json:${NC}"
  echo "$MISSING_SCRIPTS" | tr ' ' '\n' | sort -u | grep -v '^$'
  echo ""
  echo "Please either:"
  echo "  1. Add the missing scripts to package.json"
  echo "  2. Remove or update the documentation references"
  exit 1
fi
echo -e "${GREEN}All documented scripts exist in package.json${NC}"
</file>

<file path="docs/api/openapi.yaml">
openapi: 3.0.0
info:
  title: Offline Media Downloader API
  description: |-
    Offline Media Downloader API
    A serverless AWS media downloader service that downloads media content
    (primarily YouTube videos) and integrates with a companion iOS app for
    offline playback.
  version: 0.0.0
tags:
  - name: Files
  - name: Devices
  - name: Webhooks
  - name: Authentication
paths:
  /device/register:
    post:
      operationId: Devices_registerDevice
      summary: Register device for push notifications
      description: |-
        Register a device to receive push notifications.
        This is an idempotent operation that:
        1. Creates an AWS SNS endpoint for the device
        2. Associates the device with the authenticated user
        3. Subscribes the device to push notification topics
        Example request: See `tsp/examples/register-device-request.json`
        (sourced from `src/lambdas/RegisterDevice/test/fixtures/apiRequest-POST-device.json`)
        Example response: See `tsp/examples/register-device-response.json`
        (sourced from `src/lambdas/RegisterDevice/test/fixtures/apiResponse-POST-200-OK.json`)
      parameters:
        - name: Authorization
          in: header
          required: true
          schema:
            type: string
        - name: X-API-Key
          in: header
          required: true
          schema:
            type: string
      responses:
        '200':
          description: Device registration confirmation with endpoint ARN
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Models.DeviceRegistrationResponse'
        '201':
          description: Device registration confirmation with endpoint ARN
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Models.DeviceRegistrationResponse'
        '400':
          description: Device registration confirmation with endpoint ARN
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
        '401':
          description: Device registration confirmation with endpoint ARN
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/UnauthorizedError'
        '403':
          description: Device registration confirmation with endpoint ARN
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ForbiddenError'
        '500':
          description: Device registration confirmation with endpoint ARN
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InternalServerError'
      tags:
        - Devices
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/Models.DeviceRegistrationRequest'
  /feedly:
    post:
      operationId: Webhooks_processFeedlyWebhook
      summary: Process Feedly webhook
      description: |-
        Receive a webhook from Feedly to download media.
        When a webhook is received:
        - If the file exists: associates it with the user and sends push notification
        - If the file doesn't exist: creates it, associates with user, and queues for download
        Background mode can be used to defer immediate download processing.
        Example request: See `tsp/examples/webhook-feedly-request.json`
        (sourced from `src/lambdas/WebhookFeedly/test/fixtures/apiRequest-POST-webhook.json`)
        Example response: See `tsp/examples/webhook-feedly-response.json`
        (sourced from `src/lambdas/WebhookFeedly/test/fixtures/apiResponse-POST-200-OK.json`)
      parameters:
        - name: Authorization
          in: header
          required: true
          schema:
            type: string
        - name: X-API-Key
          in: header
          required: true
          schema:
            type: string
      responses:
        '200':
          description: Webhook processing status
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Models.WebhookResponse'
        '202':
          description: Webhook processing status
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Models.WebhookResponse'
        '400':
          description: Webhook processing status
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
        '403':
          description: Webhook processing status
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ForbiddenError'
        '500':
          description: Webhook processing status
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InternalServerError'
      tags:
        - Webhooks
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/Models.FeedlyWebhook'
  /files:
    get:
      operationId: Files_listFiles
      summary: List available files
      description: |-
        List all files available to the authenticated user.
        Returns files based on authentication status:
        - Authenticated users: returns their personal file library
        - Anonymous users: returns a demo file for training purposes
        - Unauthenticated users: returns 401 Unauthorized
        Example response: See `tsp/examples/list-files-response.json`
        (sourced from `src/lambdas/ListFiles/test/fixtures/apiResponse-GET-200-OK.json`)
      parameters:
        - name: Authorization
          in: header
          required: true
          schema:
            type: string
        - name: X-API-Key
          in: header
          required: true
          schema:
            type: string
      responses:
        '200':
          description: List of available files
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Models.FileListResponse'
        '401':
          description: List of available files
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/UnauthorizedError'
        '403':
          description: List of available files
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ForbiddenError'
        '500':
          description: List of available files
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InternalServerError'
      tags:
        - Files
  /user/login:
    post:
      operationId: Authentication_loginUser
      summary: Login existing user
      description: |-
        Login an existing user with Sign in with Apple.
        Authenticates a user and returns a JWT access token.
        Example request: See `tsp/examples/login-user-request.json`
        (sourced from `src/lambdas/LoginUser/test/fixtures/apiRequest-POST-login.json`)
        Example response: See `tsp/examples/login-user-response.json`
        (sourced from `src/lambdas/LoginUser/test/fixtures/apiResponse-POST-200-OK.json`)
      parameters:
        - name: X-API-Key
          in: header
          required: true
          schema:
            type: string
      responses:
        '200':
          description: JWT access token
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Models.UserLoginResponse'
        '400':
          description: JWT access token
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
        '403':
          description: JWT access token
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ForbiddenError'
        '404':
          description: JWT access token
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
        '409':
          description: JWT access token
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
        '500':
          description: JWT access token
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InternalServerError'
      tags:
        - Authentication
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/Models.UserLogin'
  /user/register:
    post:
      operationId: Authentication_registerUser
      summary: Register new user
      description: |-
        Register a new user with Sign in with Apple.
        Creates a new user account or returns existing user token.
        All user details are sourced from the Apple identity token.
        Example request: See `tsp/examples/register-user-request.json`
        (sourced from `src/lambdas/RegisterUser/test/fixtures/apiRequest-POST-register.json`)
        Example response: See `tsp/examples/register-user-response.json`
        (sourced from `src/lambdas/RegisterUser/test/fixtures/apiResponse-POST-200-OK.json`)
      parameters:
        - name: X-API-Key
          in: header
          required: true
          schema:
            type: string
      responses:
        '200':
          description: JWT access token
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Models.UserRegistrationResponse'
        '400':
          description: JWT access token
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
        '403':
          description: JWT access token
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ForbiddenError'
        '500':
          description: JWT access token
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/InternalServerError'
      tags:
        - Authentication
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/Models.UserRegistration'
components:
  schemas:
    ErrorResponse:
      type: object
      required:
        - error
        - requestId
      properties:
        error:
          type: object
          properties:
            code:
              type: string
              description: Error code
            message:
              type: string
              description: Error message
          required:
            - code
            - message
          description: Error details
        requestId:
          type: string
          description: Request ID for tracking
      description: Common error response structure
    ForbiddenError:
      type: object
      required:
        - error
        - requestId
      properties:
        error:
          type: object
          properties:
            code:
              type: string
              enum:
                - Forbidden
              description: Error code
            message:
              type: string
              description: Error message
          required:
            - code
            - message
          description: Error details
        requestId:
          type: string
          description: Request ID for tracking
      description: Forbidden error response (403)
    InternalServerError:
      type: object
      required:
        - error
        - requestId
      properties:
        error:
          type: object
          properties:
            code:
              type: string
              enum:
                - InternalServerError
              description: Error code
            message:
              type: string
              description: Error message
          required:
            - code
            - message
          description: Error details
        requestId:
          type: string
          description: Request ID for tracking
      description: Internal Server Error response (500)
    Models.Device:
      type: object
      required:
        - deviceId
        - name
        - systemName
        - systemVersion
        - token
      properties:
        deviceId:
          type: string
          description: Unique device identifier (UUID)
        name:
          type: string
          description: Device name
        systemName:
          type: string
          description: Operating system name
        systemVersion:
          type: string
          description: Operating system version
        token:
          type: string
          description: Device push notification token
        endpointArn:
          type: string
          description: AWS SNS endpoint ARN (returned after registration)
      description: Device information for push notifications
    Models.DeviceRegistrationRequest:
      type: object
      required:
        - deviceId
        - name
        - systemName
        - systemVersion
        - token
      properties:
        deviceId:
          type: string
          description: Unique device identifier (UUID)
        name:
          type: string
          description: Device name
        systemName:
          type: string
          description: Operating system name
        systemVersion:
          type: string
          description: Operating system version
        token:
          type: string
          description: Device push notification token (APNS token)
      description: Device registration request
    Models.DeviceRegistrationResponse:
      type: object
      required:
        - endpointArn
      properties:
        endpointArn:
          type: string
          description: AWS SNS endpoint ARN for the registered device
      description: Device registration response
    Models.FeedlyWebhook:
      type: object
      required:
        - articleTitle
        - articleURL
      properties:
        articleFirstImageURL:
          type: string
          format: uri
          description: URL of the first image in the article
        articleCategories:
          type: string
          description: Categories associated with the article
        articlePublishedAt:
          type: string
          description: Publication date of the article
        articleTitle:
          type: string
          description: Title of the article
        articleURL:
          type: string
          format: uri
          description: URL of the article (typically a YouTube video URL)
        createdAt:
          type: string
          description: Timestamp when the webhook was created
        sourceFeedURL:
          type: string
          format: uri
          description: URL of the source feed
        sourceTitle:
          type: string
          description: Title of the source
        sourceURL:
          type: string
          format: uri
          description: URL of the source
        backgroundMode:
          type: boolean
          description: Whether the webhook was triggered in background mode
      description: Feedly webhook event
    Models.File:
      type: object
      required:
        - fileId
      properties:
        fileId:
          type: string
          description: The unique file identifier (typically a YouTube video ID)
        key:
          type: string
          description: The filename or key in S3 storage
        size:
          type: integer
          format: int64
          description: Size in bytes of the file
        status:
          allOf:
            - $ref: '#/components/schemas/Models.FileStatus'
          description: Current status of the file
        title:
          type: string
          description: Title of the media file
        publishDate:
          type: string
          description: Video publish date
        authorName:
          type: string
          description: Channel/author display name
        authorUser:
          type: string
          description: Channel/author username or ID
        contentType:
          type: string
          description: MIME type (e.g., video/mp4)
        description:
          type: string
          description: Video description
        url:
          type: string
          format: uri
          description: CloudFront URL for downloading the file
      description: A media file available for download
    Models.FileListResponse:
      type: object
      required:
        - contents
        - keyCount
      properties:
        contents:
          type: array
          items:
            $ref: '#/components/schemas/Models.File'
          description: Array of available files
        keyCount:
          type: integer
          format: int32
          description: Number of files returned
      description: Response containing a list of files
    Models.FileStatus:
      type: string
      enum:
        - Queued
        - Downloading
        - Downloaded
        - Failed
      description: File status enumeration
    Models.UserLogin:
      type: object
      required:
        - authorizationCode
      properties:
        authorizationCode:
          type: string
          description: Authorization code from Sign in with Apple
      description: User login request
    Models.UserLoginResponse:
      type: object
      required:
        - token
      properties:
        token:
          type: string
          description: JWT access token for authenticated requests
      description: User login response
    Models.UserRegistration:
      type: object
      required:
        - authorizationCode
      properties:
        authorizationCode:
          type: string
          description: Authorization code from Sign in with Apple
        firstName:
          type: string
          description: User's first name
        lastName:
          type: string
          description: User's last name
      description: User registration request
    Models.UserRegistrationResponse:
      type: object
      required:
        - token
      properties:
        token:
          type: string
          description: JWT access token for authenticated requests
      description: User registration response
    Models.WebhookResponse:
      type: object
      required:
        - status
      properties:
        status:
          type: string
          enum:
            - Dispatched
            - Initiated
            - Accepted
          description: Status of the webhook processing
      description: Webhook processing response
    UnauthorizedError:
      type: object
      required:
        - error
        - requestId
      properties:
        error:
          type: object
          properties:
            code:
              type: string
              enum:
                - Unauthorized
              description: Error code
            message:
              type: string
              description: Error message
          required:
            - code
            - message
          description: Error details
        requestId:
          type: string
          description: Request ID for tracking
      description: Unauthorized error response (401)
servers:
  - url: https://api.example.com
    description: Production server
    variables: {}
</file>

<file path="docs/wiki/AWS/CloudWatch-Logging.md">
# CloudWatch Logging

## Quick Reference
- **When to use**: All Lambda functions
- **Enforcement**: Required
- **Impact if violated**: MEDIUM - Poor observability

## Logging Functions

The project provides three logging functions in `util/lambda-helpers.ts`:

```typescript
logInfo(message: string, stringOrObject?: string | object): void
logDebug(message: string, stringOrObject?: string | object): void
logError(message: string, stringOrObject?: string | object | unknown): void
```

- **logInfo**: Standard functionality and flow
- **logDebug**: Detailed diagnostic information
- **logError**: Error conditions

## Usage Pattern

```typescript
import {logInfo, logDebug, logError} from '../../../util/lambda-helpers'

// Log with message only
logInfo('event <=', event)

// Log with context object
logDebug('getFilesByUser.userFiles =>', userFilesResponse)

// Log errors
logError('Failed to process', error)
```

## CloudWatch Insights Queries

```sql
# Find errors
fields @timestamp, @message
| filter @message like /error/i
| sort @timestamp desc

# Find specific Lambda invocations
fields @timestamp, @message
| filter @logStream like /ListFiles/
| sort @timestamp desc

# Find slow operations
fields @timestamp, @message, @duration
| filter @duration > 5000
| sort @duration desc
```

## Best Practices

### Do's
- Log incoming events: `logInfo('event <=', event)`
- Log key operations with arrow notation for clarity
- Include relevant context in the object parameter
- Use logDebug for detailed diagnostics

### Don'ts
- Don't log sensitive data (passwords, tokens, PII)
- Don't log in tight loops
- Don't log entire large objects

## Related Patterns

- [X-Ray Integration](X-Ray-Integration.md) - Tracing with trace IDs
- [Lambda Function Patterns](../TypeScript/Lambda-Function-Patterns.md) - Handler patterns
- [Error Handling](../TypeScript/TypeScript-Error-Handling.md) - Error logging

---

*Use the provided logging functions for consistent CloudWatch logging.*
</file>

<file path="docs/wiki/AWS/Lambda-Environment-Variables.md">
# Lambda Environment Variables

## Quick Reference
- **When to use**: Configuring Lambda functions
- **Enforcement**: Required - unit tests verify presence
- **Impact if violated**: HIGH - Runtime failures

## Naming Convention

Use **CamelCase** for Lambda environment variable names:

```typescript
process.env.DynamoDBTableName
process.env.PlatformApplicationArn
process.env.PushNotificationTopicArn
process.env.FeedlyQueueUrl
```

## No Defaults in Code

Environment variables are required and verified by unit tests. Don't provide defaults:

```typescript
// ‚ùå WRONG - Don't use defaults
const tableName = process.env.DynamoDBTableName || 'default-table'

// ‚úÖ CORRECT - Required, verified by tests
const tableName = process.env.DynamoDBTableName as string
```

## No Try-Catch for Required Variables (CRITICAL)

**Zero-tolerance rule**: NEVER wrap required environment variable access in try-catch blocks with fallback values.

```typescript
// ‚ùå WRONG - Silent failures hide configuration errors
try {
  const config = JSON.parse(process.env.SignInWithAppleConfig)
} catch {
  return { clientId: 'fallback', teamId: 'fallback' }
}

// ‚úÖ CORRECT - Let it fail if misconfigured
const config = JSON.parse(process.env.SignInWithAppleConfig)
```

**Why**: Infrastructure tests enforce that all required environment variables are properly configured. Silent failures in production hide critical configuration errors that should fail fast and loud.

**Enforcement**: Unit tests verify all environment variables are present and valid. Production deployment validation catches missing variables before runtime.

## Unit Test Verification

```typescript
// test/setup.ts
beforeAll(() => {
  process.env.DynamoDBTableName = 'test-table'
  process.env.PlatformApplicationArn = 'arn:aws:sns:test'
})
```

## OpenTofu Configuration

```hcl
resource "aws_lambda_function" "ListFiles" {
  environment {
    variables = {
      DynamoDBTableName = aws_dynamodb_table.main.name
      EnableXRay        = var.enable_xray
    }
  }
}
```

## Common Variables

- `DynamoDBTableName` - DynamoDB table (set in OpenTofu)
- `PlatformApplicationArn` - SNS platform app
- `PushNotificationTopicArn` - SNS topic
- `FeedlyQueueUrl` - SQS queue
- `EnableXRay` - X-Ray tracing (read as ENABLE_XRAY in code)
- `UseLocalstack` - LocalStack testing (read as USE_LOCALSTACK in code)

## Related Patterns

- [Infrastructure/Environment-Variables](../Infrastructure/Environment-Variables.md)
- [Lambda Function Patterns](../TypeScript/Lambda-Function-Patterns.md)

---

*Use CamelCase for Lambda environment variables. Verify presence with unit tests, not defaults.*
</file>

<file path="docs/wiki/AWS/SDK-Encapsulation-Policy.md">
# SDK Encapsulation Policy

## Quick Reference
- **When to use**: Every AWS SDK interaction
- **Enforcement**: ZERO TOLERANCE
- **Impact if violated**: CRITICAL - Breaks architecture

## The Rule

**NEVER import AWS SDK directly. Always use vendor wrappers in `lib/vendor/AWS/`.**

## Examples

### ‚ùå FORBIDDEN
```typescript
// Direct SDK imports
import {S3Client} from '@aws-sdk/client-s3'
import {DynamoDBClient} from '@aws-sdk/client-dynamodb'

// Creating clients
const s3 = new S3Client()
```

### ‚úÖ REQUIRED
```typescript
// Vendor wrapper imports
import {uploadToS3} from '../../../lib/vendor/AWS/S3'
import {queryItems} from '../../../lib/vendor/AWS/DynamoDB'
```

## Vendor Wrapper Pattern

```typescript
// lib/vendor/AWS/S3.ts
import {S3Client, PutObjectCommand} from '@aws-sdk/client-s3'
import {captureAWSClient} from './XRay'

let client: S3Client | null = null

function getS3Client(): S3Client {
  if (!client) {
    client = captureAWSClient(new S3Client({
      ...(process.env.USE_LOCALSTACK === 'true' && {
        endpoint: 'http://localhost:4566'
      })
    }))
  }
  return client
}

export async function uploadToS3(
  bucket: string,
  key: string,
  body: Buffer
): Promise<void> {
  const client = getS3Client()
  await client.send(new PutObjectCommand({
    Bucket: bucket,
    Key: key,
    Body: body
  }))
}
```

## Why Vendor Wrappers?

1. **LocalStack support** - Auto-detect environment
2. **X-Ray tracing** - Built-in instrumentation
3. **Testability** - Easy mocking
4. **Type safety** - Custom interfaces
5. **Single responsibility** - One place for SDK config

## Webpack Configuration

```javascript
// Add to externals
externals: [
  '@aws-sdk/client-s3',
  '@aws-sdk/client-dynamodb',
  // Add new SDK packages here
]
```

## Testing

```typescript
jest.unstable_mockModule('../../../lib/vendor/AWS/S3', () => ({
  uploadToS3: jest.fn()
}))
```

## Verification

```bash
# Check for violations (should return nothing)
grep -r "from '@aws-sdk" src/lambdas/
grep -r "from 'aws-sdk'" src/lambdas/
```

## Related Patterns

- [X-Ray Integration](X-Ray-Integration.md)
- [LocalStack Testing](../Integration/LocalStack-Testing.md)

---

*ZERO TOLERANCE: Always use vendor wrappers for AWS SDK.*
</file>

<file path="docs/wiki/Bash/User-Output-Formatting.md">
# User Output Formatting

## Quick Reference
- **When to use**: All user-facing script output
- **Enforcement**: Recommended
- **Impact if violated**: LOW - Less readable output

## Color Definitions

```bash
#!/usr/bin/env bash

# Define at script top
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'  # No Color - always reset
```

## Output Functions

```bash
# Success message
success() {
  echo -e "${GREEN}‚úì${NC} $1"
}

# Error message
error() {
  echo -e "${RED}‚úó${NC} $1" >&2
}

# Warning message
warning() {
  echo -e "${YELLOW}‚ö†${NC} $1"
}

# Info message
info() {
  echo -e "${BLUE}‚ûú${NC} $1"
}

# Usage
success "Build completed"
error "Build failed"
warning "Using default config"
info "Starting deployment"
```

## Progress Indicators

```bash
# Step counter
STEP=1
TOTAL=5

step() {
  echo -e "${BLUE}[${STEP}/${TOTAL}]${NC} $1"
  ((STEP++))
}

# Usage
step "Installing dependencies"
step "Running tests"
step "Building project"
```

## Semantic Formatting

| Type | Color | Symbol | Usage |
|------|-------|--------|-------|
| Success | Green | ‚úì | Operation completed |
| Error | Red | ‚úó | Operation failed |
| Warning | Yellow | ‚ö† | Caution needed |
| Info | Blue | ‚ûú | General information |

## AWS CLI Output

```bash
# Format AWS output
echo -e "${GREEN}‚úì${NC} Lambda deployed: ${BLUE}${function_name}${NC}"
echo -e "${GREEN}‚úì${NC} S3 bucket: ${BLUE}${bucket_name}${NC}"
```

## Best Practices

‚úÖ Define colors once at top
‚úÖ Always reset color with `${NC}`
‚úÖ Use stderr for errors: `>&2`
‚úÖ Be consistent with symbols
‚úÖ Keep messages concise

## Related Patterns

- [Error Handling](Bash-Error-Handling.md)
- [Script Patterns](Script-Patterns.md)

---

*Use colors and symbols for clear, scannable output.*
</file>

<file path="docs/wiki/Conventions/Code-Comments.md">
# Code Comments

## Quick Reference
- **When to use**: Writing or reviewing code comments
- **Enforcement**: Required - Git history is the source of truth
- **Impact if violated**: Medium - Code clutter and confusion

## The Rule

**Git history is the source of truth for code evolution.**

NEVER explain removed code in comments. Delete outdated comments about previous implementations, deprecated features, or removed architecture. Use `git log` and `git blame` to understand historical context.

## Core Principles

1. **Git Is Source of Truth** - Removed code ‚Üí Check git history
2. **Comments Explain "Why", Not "What"** - Code shows WHAT, comments explain WHY
3. **Delete, Don't Deprecate** - Remove dead code completely, trust version control

## Examples

### ‚ùå Incorrect - Explaining Removed Code

```typescript
// ‚ùå BAD - Explaining what was removed
class UserService {
  // We used to have a caching layer here but removed it
  // The old cache implementation caused memory leaks

  async getUser(id: string) {
    // Previously we checked cache first
    // return this.cache.get(id) || this.fetchUser(id);
    return this.fetchUser(id);
  }
}

// ‚ùå BAD - Commented out old code
function processData(data: any[]) {
  // Old implementation - DO NOT DELETE
  // for (let i = 0; i < data.length; i++) {
  //   processItem(data[i]);
  // }

  data.forEach(processItem);
}
```

### ‚úÖ Correct - Clean Code, Git History

```typescript
// ‚úÖ GOOD - Clean, current implementation only
class UserService {
  // Direct fetch for simplicity and predictable memory usage
  async getUser(id: string) {
    return this.fetchUser(id);
  }
}

// ‚úÖ GOOD - Only current implementation
function processData(data: any[]) {
  data.forEach(processItem);
}

// To understand evolution, developers use:
// git log -p UserService.ts
// git blame UserService.ts
```

## Appropriate Comment Types

### ‚úÖ DO Write These Comments

#### Business Logic Explanations

```typescript
// Apply 15% discount for premium members per business requirement BR-2024-01
if (user.isPremium) {
  price *= 0.85;
}
```

#### Complex Algorithm Clarification

```typescript
// Using Fisher-Yates shuffle for uniform distribution
// See: https://en.wikipedia.org/wiki/Fisher-Yates_shuffle
function shuffle<T>(array: T[]): T[] {
  for (let i = array.length - 1; i > 0; i--) {
    const j = Math.floor(Math.random() * (i + 1));
    [array[i], array[j]] = [array[j], array[i]];
  }
  return array;
}
```

#### Workarounds and Hacks

```typescript
// HACK: AWS SDK v3 has bug with streaming uploads
// Remove after updating to v3.450+
// Issue: https://github.com/aws/aws-sdk-js-v3/issues/1234
const upload = new Upload({
  partSize: 1024 * 1024 * 5, // Force 5MB parts to avoid bug
});
```

#### TODOs with Context

```typescript
// TODO: Implement retry logic for transient failures
// Ticket: PROJ-123
// Owner: @teamname
async function apiCall() {
  // Current implementation without retry
}
```

### ‚ùå DON'T Write These Comments

```typescript
// ‚ùå We used to validate email here but moved it to middleware
// ‚ùå Previously this used callbacks, now uses promises
// ‚ùå Old version - keep for reference
// ‚ùå v1.0: Initial, v1.1: Added caching, v2.0: Removed caching
// ‚ùå Increment counter by 1
counter++;
```

## Git Commands for History

Instead of comments about removed code:

```bash
# See file history
git log -p path/to/file.ts

# See who changed what
git blame path/to/file.ts

# Find when something was removed
git log -p -S "removed text" path/to/file.ts

# See specific commit
git show <commit-hash>
```

## Documentation Comments

### TypeDoc/JSDoc (Good)

```typescript
/**
 * Validates user credentials against the authentication service.
 *
 * @param username - User's email or username
 * @param password - Plain text password (will be hashed)
 * @returns Promise resolving to authenticated user or null
 * @throws {AuthenticationError} If service is unavailable
 */
async function authenticate(username: string, password: string): Promise<User | null>
```

## Enforcement

### Code Review Checklist

- [ ] No commented-out code
- [ ] No "removed" explanations
- [ ] No version history in comments
- [ ] TODOs have context/tickets
- [ ] Comments explain "why" not "what"

## Related Patterns

- [Git Workflow](Git-Workflow.md) - Using Git effectively
- [Naming Conventions](Naming-Conventions.md) - Self-documenting code

---

*Remember: The code shows WHAT, comments explain WHY, and Git shows HOW it evolved. Keep comments focused on the present implementation's reasoning, not its history.*
</file>

<file path="docs/wiki/Infrastructure/OpenTofu-Patterns.md">
# OpenTofu Infrastructure Patterns

## Quick Reference
- **When to use**: Writing OpenTofu/Terraform infrastructure code
- **Enforcement**: Required - consistent infrastructure patterns
- **Impact if violated**: MEDIUM - Inconsistent infrastructure, maintainability issues

## File Organization

### Resource Grouping

Group related resources in dedicated files:
- `feedly_webhook.tf` - Feedly webhook Lambda and API Gateway
- `api_gateway.tf` - API Gateway configuration
- `s3.tf` - S3 buckets and policies
- `dynamodb.tf` - DynamoDB tables

### Resource Naming

Use PascalCase for resource names to match AWS conventions:

```hcl
resource "aws_lambda_function" "WebhookFeedly" {
  function_name = "WebhookFeedly"
  # ...
}

resource "aws_iam_role" "WebhookFeedlyRole" {
  name = "WebhookFeedlyRole"
  # ...
}
```

## Comments

### Explain WHY, Not WHAT

```hcl
# GOOD - explains business reason
# Retain logs for 14 days to balance cost and debugging needs
retention_in_days = 14

# BAD - states the obvious
# Set retention to 14 days
retention_in_days = 14
```

### Prohibited Comments

**NEVER include comments explaining removed resources or deprecated infrastructure**:

```hcl
# ‚ùå BAD - explaining what was removed
# Multipart upload Step Function removed - now using direct streaming

# ‚úÖ GOOD - no comments needed, use git history
# (Just define current infrastructure)
```

## Environment Variables

### Lambda Environment Variables

Always use CamelCase to match TypeScript ProcessEnv interface:

```hcl
environment {
  variables = {
    DynamoDBTableFiles  = aws_dynamodb_table.Files.name
    YtdlpBinaryPath     = "/opt/bin/yt-dlp_linux"
    GithubPersonalToken = data.sops_file.secrets.data["github.issue.token"]
  }
}
```

Match exactly to `src/types/global.d.ts`:

```typescript
interface ProcessEnv {
  DynamoDBTableFiles: string
  YtdlpBinaryPath: string
  GithubPersonalToken: string
}
```

## Lambda Function Pattern

### Standard Lambda Definition

```hcl
resource "aws_lambda_function" "FunctionName" {
  function_name = "FunctionName"
  role         = aws_iam_role.FunctionNameRole.arn
  handler      = "index.handler"
  runtime      = "nodejs22.x"
  timeout      = 300
  memory_size  = 512

  filename         = data.archive_file.FunctionName.output_path
  source_code_hash = data.archive_file.FunctionName.output_base64sha256

  environment {
    variables = {
      DynamoDBTableFiles = aws_dynamodb_table.Files.name
      S3BucketName      = aws_s3_bucket.MediaFiles.bucket
    }
  }

  depends_on = [
    aws_iam_role_policy_attachment.FunctionNamePolicyAttachment,
    aws_cloudwatch_log_group.FunctionNameLogGroup
  ]
}
```

### IAM Role Pattern

```hcl
resource "aws_iam_role" "FunctionNameRole" {
  name = "FunctionNameRole"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "lambda.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_policy" "FunctionNamePolicy" {
  name = "FunctionNamePolicy"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = ["dynamodb:GetItem", "dynamodb:PutItem"]
        Resource = aws_dynamodb_table.Files.arn
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "FunctionNamePolicyAttachment" {
  role       = aws_iam_role.FunctionNameRole.name
  policy_arn = aws_iam_policy.FunctionNamePolicy.arn
}
```

### CloudWatch Logs

```hcl
resource "aws_cloudwatch_log_group" "FunctionNameLogGroup" {
  name              = "/aws/lambda/FunctionName"
  retention_in_days = 14
}
```

## DynamoDB Tables

### ElectroDB Single-Table Design

```hcl
resource "aws_dynamodb_table" "MediaDownloader" {
  name           = "MediaDownloader"
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "pk"
  range_key      = "sk"

  attribute {
    name = "pk"
    type = "S"
  }

  attribute {
    name = "sk"
    type = "S"
  }

  global_secondary_index {
    name            = "gsi1"
    hash_key        = "gsi1pk"
    range_key       = "gsi1sk"
    projection_type = "ALL"
  }
}
```

## S3 Buckets

### Media Storage Bucket

```hcl
resource "aws_s3_bucket" "MediaFiles" {
  bucket = "media-downloader-files-${data.aws_caller_identity.current.account_id}"
}

resource "aws_s3_bucket_versioning" "MediaFilesVersioning" {
  bucket = aws_s3_bucket.MediaFiles.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "MediaFilesEncryption" {
  bucket = aws_s3_bucket.MediaFiles.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}
```

## Best Practices

### DO

- ‚úÖ Group related resources in logical files
- ‚úÖ Use PascalCase for resource names
- ‚úÖ Reference resources using interpolation
- ‚úÖ Keep environment variable names consistent with TypeScript
- ‚úÖ Use PAY_PER_REQUEST for DynamoDB in serverless
- ‚úÖ Enable versioning and encryption on S3 buckets
- ‚úÖ Set CloudWatch log retention

### DON'T

- ‚ùå Hardcode values that can be referenced
- ‚ùå Explain removed resources in comments (use git history)
- ‚ùå Use provisioned capacity for DynamoDB in serverless
- ‚ùå Leave S3 buckets publicly accessible
- ‚ùå Keep CloudWatch logs forever

## Related Patterns

- [Naming Conventions](../Conventions/Naming-Conventions.md) - PascalCase for resources
- [Code Comments](../Conventions/Code-Comments.md) - Git as source of truth

---

*Infrastructure documentation belongs in README.md, AGENTS.md, and Git history - not in comments.*
</file>

<file path="docs/wiki/Meta/AI-Tool-Context-Files.md">
# AI Tool Context Files

## Quick Reference
- **When to use**: Setting up AI assistant context for a project
- **Enforcement**: Recommended - ensures AI tool compatibility
- **Impact if violated**: Low - some AI tools may not auto-load context

## Overview

AI coding assistants look for specific filenames to auto-load project context. The industry has consolidated around **AGENTS.md** as the universal standard, with tool-specific passthrough files for compatibility.

## The AGENTS.md Standard

AGENTS.md is the open standard for AI coding assistant context, maintained collaboratively by OpenAI, Amp, Google, Cursor, and Factory, with support across 20+ AI coding tools.

### Tool Support Matrix

| AI Tool | Auto-Reads AGENTS.md | Auto-Reads CLAUDE.md | Auto-Reads GEMINI.md |
|---------|---------------------|---------------------|---------------------|
| **OpenAI Codex CLI** | ‚úÖ Yes | No | No |
| **GitHub Copilot** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |
| **Google Gemini CLI** | Configurable | No | ‚úÖ Yes |
| **Claude Code** | No | ‚úÖ Yes | No |
| **Cursor** | ‚úÖ Yes | - | - |
| **Codeium** | ‚úÖ Yes | - | - |

**Official Resource**: https://agents.md

## File Architecture

### Recommended Structure

```
project/
‚îú‚îÄ‚îÄ AGENTS.md          # Universal context (single source of truth)
‚îú‚îÄ‚îÄ CLAUDE.md          # Claude Code passthrough (1 line)
‚îú‚îÄ‚îÄ GEMINI.md          # Gemini Code Assist passthrough (4 lines)
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ wiki/          # Detailed conventions
    ‚îî‚îÄ‚îÄ conventions-tracking.md  # Project-specific patterns
```

### Benefits
- Single source of truth in AGENTS.md
- Tool compatibility via passthrough files
- No duplication or sync issues
- Works with 20+ AI coding tools

## AGENTS.md Content Structure

```markdown
# Project Context for AI Agents

## Convention Capture System
[Universal detection and tracking instructions]

## Project Overview
[Project-specific architecture and tech stack]

## Wiki Conventions to Follow
[Links to relevant wiki pages]

## Critical Project-Specific Rules
[Unique requirements for THIS project only]

## Development Workflow
[Commands, tools, build process]
```

### What Goes in AGENTS.md

**Include**:
- Convention Capture System instructions (universal)
- Project overview and architecture
- Links to wiki for universal patterns
- Project-specific unique requirements
- Development workflow commands

**Exclude (put in wiki instead)**:
- Universal naming conventions
- Standard testing patterns
- Common TypeScript patterns
- AWS service usage patterns

## Passthrough File Pattern

### CLAUDE.md Passthrough

```markdown
@AGENTS.md
```

That's it. One line. Claude Code will read AGENTS.md via this reference.

### GEMINI.md Passthrough

```markdown
# See AGENTS.md

This project uses AGENTS.md as the single source of truth for AI coding assistant context.

Please see AGENTS.md in the repository root for comprehensive project documentation and guidelines.
```

## Creating AGENTS.md for New Projects

### Step 1: Copy Template

```bash
cp path/to/template/AGENTS.md ./AGENTS.md
```

### Step 2: Add Project-Specific Content

Edit AGENTS.md to include:
- Your project description
- Your architecture stack
- Your specific requirements
- Your development commands

### Step 3: Create Passthrough Files

**CLAUDE.md**:
```markdown
@AGENTS.md
```

**GEMINI.md**:
```markdown
# See AGENTS.md

This project uses AGENTS.md as the single source of truth.

Please see AGENTS.md for documentation.
```

### Step 4: Initialize Convention Tracking

```bash
touch docs/conventions-tracking.md
```

## Migration from Existing Projects

### From CLAUDE.md Only

```bash
# Rename to AGENTS.md
mv CLAUDE.md AGENTS.md

# Create new CLAUDE.md passthrough
echo "@AGENTS.md" > CLAUDE.md

# Create GEMINI.md passthrough
cat > GEMINI.md << 'EOF'
# See AGENTS.md
Please see AGENTS.md for documentation.
EOF
```

## Benefits

### For Developers
- Edit context in one place (AGENTS.md)
- Works with any AI coding tool
- No duplication or sync issues

### For AI Assistants
- Automatically load context on session start
- Inherit Convention Capture System
- Access project-specific rules

### For Teams
- Consistent across all AI tools
- Single file to review in PRs
- Easy onboarding

## Related Documentation

- [Convention Capture System](Convention-Capture-System.md) - How conventions persist
- [Working with AI Assistants](Working-with-AI-Assistants.md) - Effective collaboration

---

*Use AGENTS.md as the single source of truth for AI tool context. Maintain compatibility with tool-specific passthrough files.*
</file>

<file path="docs/wiki/Meta/Convention-Capture-System.md">
# Convention Capture System

## Quick Reference
- **When to use**: Every development session with AI assistants
- **Enforcement**: Required - prevents knowledge loss
- **Impact if violated**: CRITICAL - institutional memory lost

## Overview

The Convention Capture System automatically detects, captures, and preserves development conventions as they emerge during work. This ensures no institutional knowledge is lost to conversation history.

## How It Works

### Detection Signals

Monitor for these signals during development:

| Priority | Signal Words | Action |
|----------|--------------|--------|
| üö® **CRITICAL** | NEVER, FORBIDDEN, Zero-tolerance | Immediate capture |
| ‚ö†Ô∏è **HIGH** | MUST, REQUIRED, ALWAYS | Flag for documentation |
| üìã **MEDIUM** | Prefer, Should, Convention | Track pattern |
| üí° **LOW** | Consider, Might, Sometimes | Monitor |

### Real-Time Flagging

When a convention is detected:

```
üîî **CONVENTION DETECTED**

**Name**: AWS SDK Encapsulation
**Type**: Rule
**What**: Never import AWS SDK directly, use vendor wrappers
**Why**: Maintains encapsulation and testability
**Priority**: Critical

Document now? [Y/N]
```

### Convention Tracking

Central registry in `docs/conventions-tracking.md`:

```markdown
## üü° Pending Documentation

### Detected: 2025-11-22
1. **Pattern Name** (Type)
   - What: [Description]
   - Why: [Rationale]
   - Target: docs/wiki/[Category]/[Page].md
   - Status: ‚è≥ Pending

## üü¢ Recently Documented
- [x] **Convention Name** ‚Üí docs/wiki/[Path]
```

## Implementation Guide

### For AI Assistants

**Session Start**:
1. Read AGENTS.md (contains Convention Capture instructions)
2. Check `docs/conventions-tracking.md` for pending items
3. Activate detection mode

**During Work**:
1. Monitor for detection signals
2. Flag conventions immediately
3. Offer to document or defer
4. Continue with primary task

**Session End**:
1. Generate session summary
2. List detected conventions
3. Update `conventions-tracking.md`
4. List pending documentation tasks

### For Developers

**Setting Up**:
1. Add Convention Capture section to AGENTS.md
2. Create `docs/conventions-tracking.md`
3. Create `docs/templates/` directory
4. Initialize with known conventions

**Contributing**:
1. Work normally with AI assistant
2. Confirm when conventions are flagged
3. Review session summaries
4. Approve wiki documentation

## Detection Patterns

### Explicit Statements
```
"Always use camelCase"
"Never commit secrets"
```
‚Üí Immediate capture as rule

### Corrections
```
"Actually, it's PascalCase not camelCase"
"No, we use OpenTofu not Terraform"
```
‚Üí High priority pattern

### Repeated Decisions
```
First occurrence: "Let's use vendor wrappers"
Second occurrence: "Use vendor wrapper again"
```
‚Üí Pattern detected, suggest documentation

## Documentation Template

```markdown
# [Convention Name]

## Classification
- **Type**: Rule | Pattern | Methodology
- **Priority**: Critical | High | Medium | Low
- **Enforcement**: Zero-tolerance | Required | Recommended

## The Rule
[Clear, concise statement]

## Context
**Problem Solved**: [What issue this addresses]
**Benefits**: [Why it matters]

## Examples
### ‚úÖ Correct
[Code example]

### ‚ùå Incorrect
[Anti-pattern]

## Enforcement
[How to check/automate]

## Related Patterns
[Links to related conventions]
```

## Benefits

### Immediate
- No repeated explanations - document once, reference forever
- Consistent patterns - same conventions across all work
- Clear communication - shared vocabulary
- Reduced cognitive load

### Long-term
- Institutional memory persists beyond individuals
- Faster onboarding for new team members
- Patterns evolve and improve over time
- Prevents technical debt from bad patterns

## Integration with AGENTS.md

Every project using AGENTS.md automatically gets Convention Capture:

```markdown
# AGENTS.md

## Convention Capture System
[Universal instructions for using system]
[Links to this wiki page]

## Project-Specific Content
[Project details]
```

Each project maintains its own:
- `docs/conventions-tracking.md` - Project conventions
- `docs/sessions/` - Session summaries
- Local overrides and additions

## Related Documentation

- [AGENTS.md Template](AI-Tool-Context-Files.md) - Universal instructions
- [Working with AI Assistants](Working-with-AI-Assistants.md) - Collaboration guide

---

*The Convention Capture System ensures valuable patterns and conventions are never lost. By systematically detecting, tracking, and documenting conventions as they emerge, we build persistent institutional memory that benefits all future work.*
</file>

<file path="docs/wiki/Meta/GitHub-Wiki-Sync.md">
# GitHub Wiki Sync Automation

## Quick Reference
- **When to use**: Maintaining project documentation
- **Enforcement**: Automated via GitHub Actions
- **Impact if violated**: N/A - fully automated

## Overview

The GitHub Wiki sync automation provides the best of both worlds:
- **Git-tracked source** - Documentation in `docs/wiki/` is version controlled
- **Beautiful web UI** - GitHub Wiki provides excellent browsing experience
- **Zero manual maintenance** - Automatic sync within 30 seconds of merge

## How It Works

### Directory Structure
```
docs/
‚îî‚îÄ‚îÄ wiki/                        # Source of truth (version controlled)
    ‚îú‚îÄ‚îÄ Home.md                  # Wiki homepage
    ‚îú‚îÄ‚îÄ Getting-Started.md       # Quick start guide
    ‚îú‚îÄ‚îÄ AWS/                     # Category folders
    ‚îÇ   ‚îú‚îÄ‚îÄ CloudWatch-Logging.md
    ‚îÇ   ‚îî‚îÄ‚îÄ Lambda-Environment-Variables.md
    ‚îú‚îÄ‚îÄ Testing/
    ‚îÇ   ‚îú‚îÄ‚îÄ Jest-ESM-Mocking-Strategy.md
    ‚îÇ   ‚îî‚îÄ‚îÄ Coverage-Philosophy.md
    ‚îî‚îÄ‚îÄ ...
```

### Sync Process
1. Developer edits markdown files in `docs/wiki/`
2. Creates PR with documentation changes
3. PR gets reviewed and merged to master
4. GitHub Actions workflow triggers automatically
5. Wiki updates within 30 seconds

## GitHub Actions Workflow

### Trigger Conditions
```yaml
on:
  push:
    branches: [master, main]
    paths:
      - 'docs/wiki/**'
      - '.github/scripts/sync-wiki.sh'
      - '.github/scripts/generate-sidebar.sh'
      - '.github/workflows/sync-wiki.yml'
```

### Key Implementation Details

#### Flat Namespace Requirement
GitHub Wiki requires **all pages at root level** (no subdirectories):

```bash
# Source structure (organized by category)
docs/wiki/AWS/CloudWatch-Logging.md
docs/wiki/Testing/Jest-ESM-Mocking-Strategy.md

# Wiki structure (flat)
wiki/CloudWatch-Logging.md
wiki/Jest-ESM-Mocking-Strategy.md
```

#### Sidebar Generation
The sidebar (_Sidebar.md) is generated from source structure:
```bash
# Reads from categorized source
SOURCE_DIR=main/docs/wiki

# Writes flat links to wiki
[CloudWatch Logging](CloudWatch-Logging)  # No path prefix
```

#### Link Transformation
Internal links are automatically transformed (path removed, extension stripped):
```markdown
# In source file
[See Error Handling](../TypeScript/Error-Handling.md)

# After transformation in wiki
[See Error Handling](Error-Handling)
```

## Scripts

### sync-wiki.sh
- Flattens directory structure
- Transforms internal links
- Handles special files (Home.md, _Sidebar.md, _Footer.md)

### generate-sidebar.sh
- Reads source directory structure
- Creates categorized navigation
- Generates flat links for GitHub Wiki

## Adding New Documentation

### Creating a New Page
```bash
# 1. Create markdown file in appropriate category
echo "# My New Feature" > docs/wiki/Testing/My-New-Feature.md

# 2. Add content
vim docs/wiki/Testing/My-New-Feature.md

# 3. Commit and push
git add docs/wiki/
git commit -m "docs: add My New Feature documentation"
git push
```

### Creating a New Category
```bash
# 1. Create category directory
mkdir docs/wiki/MyCategory

# 2. Add first page
echo "# Category Overview" > docs/wiki/MyCategory/Overview.md

# 3. Commit (sidebar auto-updates)
git add docs/wiki/
git commit -m "docs: add MyCategory documentation"
git push
```

## File Naming

### Unique Names Required
Due to flat namespace, all files must have unique names:

```bash
# ‚ùå Will cause conflicts
docs/wiki/TypeScript/Error-Handling.md
docs/wiki/Bash/Error-Handling.md

# ‚úÖ Use unique names
docs/wiki/TypeScript/TypeScript-Error-Handling.md
docs/wiki/Bash/Bash-Error-Handling.md
```

### Naming Conventions
- Use **Title-Case-With-Hyphens.md** for file names
- Match page title in file name for clarity
- Keep names descriptive but concise

## Special Files

### Home.md
- Wiki landing page
- Must exist at `docs/wiki/Home.md`
- Contains navigation overview

### _Sidebar.md
- Auto-generated, don't edit manually
- Created from source directory structure
- Updates on every sync

### _Footer.md
- Auto-generated with sync timestamp
- Shows link to source repository
- Updates on every sync

## Troubleshooting

### Wiki Not Updating
1. Check workflow runs: `gh run list --workflow=sync-wiki.yml`
2. Verify wiki is enabled in repo settings
3. Check for naming conflicts (duplicate file names)

### Broken Links
1. Ensure unique file names across categories
2. Check link transformation in sync-wiki.sh
3. Verify flat links in _Sidebar.md

### Missing Categories
1. Categories only appear if they contain .md files
2. Check SOURCE_DIR in generate-sidebar.sh
3. Verify directory structure in docs/wiki/

## Benefits

1. **Version Control** - All docs in Git with full history
2. **Code Review** - Documentation changes reviewed in PRs
3. **Automation** - No manual wiki editing needed
4. **Organization** - Category folders in source
5. **Discovery** - Beautiful GitHub Wiki UI
6. **Search** - GitHub Wiki search functionality
7. **Offline Access** - Docs available in cloned repo

## Related Patterns

- [Documentation Patterns](Documentation-Patterns.md) - Documentation standards
- [Convention Capture System](Convention-Capture-System.md) - How conventions are documented
- [AI Tool Context Files](AI-Tool-Context-Files.md) - AGENTS.md integration

---

*Automated GitHub Wiki sync provides version-controlled documentation with zero manual maintenance.*
</file>

<file path="docs/wiki/Meta/Serverless-Architecture-Assessment.md">
# Comprehensive Serverless Project Evaluation

## Executive Summary

Based on 50+ web searches across serverless frameworks, architecture patterns, testing strategies, and open-source projects, combined with deep codebase exploration, this document provides a comprehensive evaluation of the **AWS CloudFormation Media Downloader** project against industry best practices.

**Overall Assessment: EXCELLENT (9/10)**

This project demonstrates production-grade serverless architecture that exceeds most open-source serverless projects in sophistication and adherence to best practices.

---

## Research Methodology

### Web Searches Performed (50+)
1. Serverless Framework project structure best practices 2024-2025
2. AWS Lambda organization patterns (monorepo vs multi-repo)
3. SST Framework AWS serverless infrastructure as code
4. Terraform vs CDK vs SST comparison
5. DynamoDB single-table design ElectroDB best practices
6. DynamoDB vs Aurora Serverless comparison
7. AWS Lambda testing best practices LocalStack integration
8. Webpack vs esbuild serverless Lambda bundling
9. Lambda layers vs bundling tradeoffs cold start
10. Jest ESM mocking Lambda TypeScript patterns
11. AWS Powertools Lambda TypeScript observability
12. Terraform OpenTofu Lambda deployment patterns
13. Serverless TypeScript monorepo turborepo pnpm
14. API Gateway custom authorizer Lambda JWT best practices
15. Serverless APNS push notification Lambda integration
16. yt-dlp serverless Lambda YouTube download implementation
17. AWS X-Ray Lambda tracing serverless observability
18. AWS SDK v3 modular imports Lambda bundle optimization
19. Serverless IAM least privilege permissions per Lambda
20. DynamoDB ElectroDB alternatives (Dynamoose, TypeDORM)
21. Serverless S3 transfer acceleration large file upload
22. Serverless SQS Lambda dead letter queue patterns
23. Better Auth serverless Lambda authentication patterns
24. Sign In with Apple serverless Lambda implementation
25. Serverless CloudWatch logging best practices structured logs
26. Serverless cold start optimization Node.js Lambda 2024
27. Serverless TypeSpec OpenAPI specification generation
28. Serverless Zod validation TypeScript Lambda
29. Serverless GitHub Actions CI/CD Lambda deployment
30. Serverless SOPS secrets management encrypted Lambda
31. Serverless DynamoDB on-demand billing capacity
32. Serverless webhook reliability idempotency patterns
33. AWS Lambda Node.js 22 runtime features 2024
34. Serverless error handling patterns Lambda retry behavior
35. Serverless production architecture scalability patterns
36. Serverless microservices API Gateway domain driven
37. Serverless event-driven Step Functions orchestration
38. Serverless testing fixtures mock AWS services
39. Dependency injection serverless Lambda TypeScript
40. Serverless documentation generation TSDoc
41. Serverless ESLint TypeScript rules code quality
42. Serverless Lambda handler wrapper middleware patterns
43. Serverless media processing Lambda S3 FFmpeg video
44. Serverless code graph dependency analysis TypeScript
45. Serverless Lambda concurrency throttling provisioned
46. Serverless architecture security best practices 2024
47. AWS SAM vs Terraform vs CDK comparison
48. Serverless Lambda CloudWatch metrics dashboards alarms
49. Serverless cost optimization Lambda pricing 2024
50. Open source serverless projects GitHub examples

---

## Detailed Evaluation by Category

### 1. Project Structure - EXCELLENT

| Aspect | Your Implementation | Industry Best Practice | Assessment |
|--------|---------------------|----------------------|------------|
| Lambda Organization | Per-Lambda directories with `src/` and `test/` subdirectories | Modular per-function directories | Matches |
| Monorepo Structure | Single repo with all 17 Lambda functions | Monorepo recommended for <100 devs | Optimal |
| Shared Code | `lib/`, `util/`, `entities/`, `types/` | Domain-based separation | Excellent |
| Entry Point Convention | `src/lambdas/[name]/src/index.ts` | Single entry point per function | Matches |
| Test Co-location | Tests in `test/` adjacent to `src/` | Co-located or separate `__tests__` | Modern pattern |

**Strengths:**
- Automatic Lambda discovery via esbuild entry point scanning
- Clean separation between handler code and shared utilities
- Path aliases (`#entities/*`, `#lib/*`) eliminate relative path hell

**Industry Comparison:**
- Better than Serverless Framework examples (more modular)
- Comparable to AWS SAM best practices
- Matches patterns from successful open-source projects like `serverless-samples`

### 2. Infrastructure as Code - EXCELLENT

| Aspect | Your Implementation | Industry Best Practice | Assessment |
|--------|---------------------|----------------------|------------|
| Tool Choice | OpenTofu (Terraform fork) | Terraform/CDK/SAM/SST | Production-ready |
| File Organization | Per-Lambda `.tf` files | Modular resource files | Excellent |
| IAM Policies | Dedicated role per Lambda | Least-privilege per function | Best practice |
| State Management | Remote state (assumed) | Remote state with locking | Standard |

**Your Advantage Over Alternatives:**
- **vs CDK**: More explicit resource definitions, easier debugging
- **vs SST v3**: No vendor lock-in to Pulumi, more mature ecosystem
- **vs SAM**: More flexibility for non-serverless resources

**Strengths:**
- Each Lambda has its own IAM role with scoped permissions
- Per-Lambda Terraform files allow independent modifications
- SOPS for secrets management (encrypted at rest)

**Gap Identified:**
- Could benefit from Terraform modules for repeated patterns

### 3. Database Architecture - EXCELLENT (Industry-Leading)

| Aspect | Your Implementation | Industry Best Practice | Assessment |
|--------|---------------------|----------------------|------------|
| Database Choice | DynamoDB (single-table) | DynamoDB for serverless | Optimal |
| ORM | ElectroDB | ElectroDB or DynamoDB-Toolbox | Best choice |
| Billing Mode | PAY_PER_REQUEST | On-demand for variable traffic | Cost-effective |
| GSI Strategy | 5+ GSIs for access patterns | Design GSIs per access pattern | Proper design |

**Why This is Industry-Leading:**

1. **Single-Table Design**: Your 9-entity single-table design follows Rick Houlihan's re:Invent guidance exactly
2. **ElectroDB Choice**: Per [DEV.to comparison](https://dev.to/thomasaribart/an-in-depth-comparison-of-the-most-popular-dynamodb-wrappers-5b73), ElectroDB is the "best DynamoDB client wrapper" with superior type inference
3. **Collections Pattern**: Your `Collections.ts` implements JOIN-like queries efficiently
4. **Type Safety**: Full TypeScript inference for all entity operations

**Comparison to Aurora Serverless:**
| Factor | DynamoDB | Aurora Serverless v2 |
|--------|----------|---------------------|
| Cold starts | None | Possible |
| Scaling | Instant | Seconds |
| Pricing | Pay-per-request | Capacity-based |
| Your use case | Perfect fit | Overkill |

Your choice of DynamoDB is optimal for this workload.

### 4. Testing Strategy - EXCELLENT

| Aspect | Your Implementation | Industry Best Practice | Assessment |
|--------|---------------------|----------------------|------------|
| Mock Strategy | Custom ElectroDB mock helper | Centralized mock utilities | Superior |
| Integration Tests | LocalStack | LocalStack or cloud testing | Cost-effective |
| ESM Support | `jest.unstable_mockModule` | Required for ES modules | Modern |
| Fixtures | JSON from real AWS responses | Production-like fixtures | Best practice |

**Your Innovation:**
- `test/helpers/electrodb-mock.ts` is a unique, type-safe solution
- Transitive dependency tracking via `build/graph.json` ensures complete mocking

**Industry Validation:**
- AWS recommends LocalStack integration ([AWS Blog](https://aws.amazon.com/blogs/compute/enhance-the-local-testing-experience-for-serverless-applications-with-localstack/))
- Jest ESM mocking is the current standard pattern ([Jest Docs](https://jestjs.io/docs/ecmascript-modules))

**Gap Identified:**
- Consider adding AWS Powertools Parser for Zod validation in tests

### 5. Build System - EXCELLENT

| Aspect | Your Implementation | Industry Best Practice | Assessment |
|--------|---------------------|----------------------|------------|
| Bundler | esbuild | esbuild (10x faster) | Optimal |
| AWS SDK | Externalized (v3) | Modular imports + external | Optimal |
| Code Splitting | Disabled (single file) | Single file per Lambda | Correct |
| TypeScript | esbuild (native) | esbuild (fastest) | Optimal |

**Status: esbuild Migration Complete**

The project uses esbuild with parallel Lambda builds (`config/esbuild.config.ts`):
- Parallel builds for all Lambda functions
- Tree shaking and dead code elimination
- Source maps for debugging
- Bundle analysis available via `pnpm run analyze`

### 6. Observability - EXCELLENT

| Aspect | Your Implementation | Industry Best Practice | Assessment |
|--------|---------------------|----------------------|------------|
| X-Ray Tracing | AWS Powertools Tracer | Active mode enabled | Optimal |
| Structured Logging | AWS Powertools Logger | AWS Powertools Logger | Optimal |
| Custom Metrics | AWS Powertools Metrics | AWS Powertools Metrics | Optimal |
| Error Tracking | GitHub Issues | Centralized + alerting | Unique approach |

**Status: AWS Lambda Powertools Integrated**

The project uses AWS Lambda Powertools for TypeScript (`src/lib/vendor/Powertools/`):
- Logger: Structured JSON with correlation IDs and persistent attributes
- Metrics: CloudWatch embedded metrics format with custom namespaces
- Tracer: Enhanced X-Ray annotations and subsegments
- `withPowertools()` wrapper integrates all three tools

### 7. Security - EXCELLENT

| Aspect | Your Implementation | Industry Best Practice | Assessment |
|--------|---------------------|----------------------|------------|
| IAM | Per-function least privilege | Principle of least privilege | Best practice |
| Secrets | SOPS encrypted | Secrets Manager or SOPS | Secure |
| API Auth | Custom authorizer + Better Auth | Lambda authorizer | Proper |
| Dependencies | `.npmrc` lifecycle protection | Supply chain security | Innovative |

**Your Security Innovations:**
1. **`.npmrc` lifecycle script protection**: Blocks AI-targeted typosquatting attacks - this is ahead of industry practices
2. **Per-Lambda IAM roles**: Each function has exactly the permissions it needs
3. **Better Auth integration**: Enterprise-grade authentication with ElectroDB adapter

**Industry Alignment:**
- Matches [14 AWS Lambda Security Best Practices](https://www.ranthebuilder.cloud/post/14-aws-lambda-security-best-practices-for-building-secure-serverless-applications)
- Exceeds OWASP serverless guidelines

### 8. Developer Experience - EXCELLENT

| Aspect | Your Implementation | Industry Best Practice | Assessment |
|--------|---------------------|----------------------|------------|
| Path Aliases | `#entities/*`, `#lib/*`, etc. | tsconfig paths | Modern |
| Hot Reload | Not applicable (Lambda) | N/A for Lambda | N/A |
| Local CI | `pnpm run ci:local` | Pre-push validation | Excellent |
| Documentation | Wiki + AGENTS.md + TSDoc | Comprehensive docs | Thorough |

**Unique Innovations:**
1. **AGENTS.md**: AI-friendly documentation for code assistants
2. **Convention Capture System**: Automatic documentation of emergent patterns
3. **MCP Server**: Custom tooling for project-specific queries

### 9. Webhook & Event Patterns - EXCELLENT

| Aspect | Your Implementation | Industry Best Practice | Assessment |
|--------|---------------------|----------------------|------------|
| Feedly Webhook | Query-based auth | HMAC or query auth | Appropriate |
| Retry Handling | SQS + DLQ pattern | Dead letter queues | Best practice |
| Idempotency | FileDownloads entity | DynamoDB for state | Implemented |
| Push Notifications | SNS -> SQS -> Lambda | AWS recommended | Correct pattern |

**Your FileDownloads Entity is Elegant:**
- Tracks download state separately from Files metadata
- Enables retry with exponential backoff
- GSI6 for status + retryAfter queries

---

## Comparison to Notable Open Source Projects

### vs [serverless/examples](https://github.com/serverless/examples)
| Aspect | Your Project | serverless/examples |
|--------|--------------|---------------------|
| TypeScript | Full strict mode | Mixed (some JS) |
| Testing | Comprehensive | Basic examples |
| ORM | ElectroDB | Direct SDK calls |
| **Winner** | Your project | - |

### vs [aws-samples/serverless-samples](https://github.com/aws-samples/serverless-samples)
| Aspect | Your Project | AWS Samples |
|--------|--------------|-------------|
| Single-table design | 9 entities | Usually separate tables |
| IaC | OpenTofu | SAM/CDK |
| Production-ready | Yes | Reference only |
| **Winner** | Your project (production) | Educational |

### vs SST Examples
| Aspect | Your Project | SST Examples |
|--------|--------------|--------------|
| Infrastructure | OpenTofu (explicit) | SST/Pulumi (abstracted) |
| Vendor lock-in | None | SST ecosystem |
| Maturity | Production | Framework examples |
| **Winner** | Your project (independence) | SST (DX) |

---

## Recommendations for Future Development

### High Priority (Significant Impact)

#### ~~1. Migrate to esbuild for Build Performance~~ ‚úÖ COMPLETE
**Status**: Implemented in `config/esbuild.config.ts`
- Parallel Lambda builds with esbuild
- Tree shaking and bundle analysis
- Achieved 10x faster builds

#### ~~2. Add AWS Lambda Powertools for TypeScript~~ ‚úÖ COMPLETE
**Status**: Implemented in `src/lib/vendor/Powertools/`
- Logger with structured JSON and correlation IDs
- Metrics with CloudWatch embedded format
- Tracer with enhanced X-Ray annotations
- `withPowertools()` wrapper for all handlers

#### 3. Add Idempotency for WebhookFeedly
**Current State**: No explicit idempotency handling
**Recommendation**: Use Powertools Idempotency utility
**Impact**: Prevent duplicate processing of webhooks

**Sources**:
- [Handling Lambda functions idempotency](https://aws.amazon.com/blogs/compute/handling-lambda-functions-idempotency-with-aws-lambda-powertools/)
- [Webhooks on AWS Lambda Tips & Tricks](https://blog.serverlessadvocate.com/webhooks-on-aws-lambda-tips-tricks-63b231d09360)

### Medium Priority (Quality of Life)

#### 4. Consider Node.js 22 Runtime
**Current State**: Node.js 24.x (already using!)
**Status**: Already optimal
**Note**: You're already on the latest LTS runtime

**Sources**:
- [Node.js 22 runtime now available in AWS Lambda](https://aws.amazon.com/blogs/compute/node-js-22-runtime-now-available-in-aws-lambda/)

#### 5. Add CloudWatch Dashboards via IaC
**Current State**: Implemented in Terraform (commit #187)
**Status**: Already complete

#### 6. Implement Provisioned Concurrency for Auth Lambdas
**Current State**: On-demand scaling
**Recommendation**: Provisioned concurrency for `ApiGatewayAuthorizer`, `LoginUser`
**Impact**: Eliminate cold starts for authentication (latency-sensitive)
**Tradeoff**: Additional cost (~$5-20/month depending on config)

**Sources**:
- [AWS Lambda Provisioned Concurrency](https://www.serverless.com/blog/aws-lambda-provisioned-concurrency)

### Low Priority (Nice to Have)

#### 7. Terraform Modules for Lambda Patterns
**Current State**: Individual `.tf` files per Lambda
**Recommendation**: Create reusable module for Lambda + IAM + CloudWatch pattern
**Impact**: Reduced duplication, easier maintenance

#### 8. Add Zod Validation with Powertools Parser
**Current State**: Already using Zod for validation
**Recommendation**: Integrate with Powertools Parser middleware
**Impact**: Catch malformed payloads at function entry with middleware pattern

**Sources**:
- [Validating event payload with Powertools](https://aws.amazon.com/blogs/compute/validating-event-payload-with-powertools-for-aws-lambda-typescript/)

---

## Architecture Decisions Validated

### DynamoDB vs Aurora: Correct Choice
Your workload characteristics:
- Variable traffic (personal use + occasional spikes)
- Simple access patterns (key-value lookups, relationship queries)
- No complex transactions or joins

DynamoDB is the right choice. Aurora Serverless v2 would be overkill with higher cold starts and costs.

### OpenTofu vs SST: Correct Choice
Your requirements:
- Full infrastructure control
- No framework lock-in
- Complex IAM policies

OpenTofu provides the flexibility you need. SST would abstract away too much control.

### ElectroDB vs Alternatives: Correct Choice
Per industry comparison, ElectroDB offers:
- Best type inference
- Native single-table support
- Collections for JOIN-like queries

### esbuild: Migration Complete ‚úÖ
esbuild is now the project bundler, providing:
- 10x faster builds via parallel compilation
- Tree shaking and dead code elimination
- Bundle analysis via `pnpm run analyze`

---

## Summary Scorecard

| Category | Score | Notes |
|----------|-------|-------|
| Project Structure | 10/10 | Industry-leading organization |
| Infrastructure as Code | 9/10 | Could use Terraform modules |
| Database Architecture | 10/10 | Exemplary single-table design |
| Testing Strategy | 9/10 | Custom mock helper is innovative |
| Build System | 10/10 | esbuild with parallel builds |
| Observability | 10/10 | AWS Powertools fully integrated |
| Security | 10/10 | npm lifecycle protection is ahead of curve |
| Developer Experience | 9/10 | AGENTS.md, MCP server are unique |

**Overall: 9.6/10 - Production-Grade Excellence**

---

## Key Sources Referenced

### Project Structure
- [AWS Blog: Best practices for organizing larger serverless applications](https://aws.amazon.com/blogs/compute/best-practices-for-organizing-larger-serverless-applications/)
- [Serverless.com: Structuring a Real-World Serverless App](https://www.serverless.com/blog/structuring-a-real-world-serverless-app)
- [Lumigo: Mono-Repo vs One-Per-Service](https://lumigo.io/blog/mono-repo-vs-one-per-service/)

### Database & ORM
- [Alex DeBrie: The What, Why, and When of Single-Table Design](https://www.alexdebrie.com/posts/dynamodb-single-table/)
- [ElectroDB Documentation](https://electrodb.dev/en/core-concepts/introduction/)
- [DEV.to: DynamoDB wrapper comparison](https://dev.to/thomasaribart/an-in-depth-comparison-of-the-most-popular-dynamodb-wrappers-5b73)

### Testing
- [AWS Blog: Enhance local testing with LocalStack](https://aws.amazon.com/blogs/compute/enhance-the-local-testing-experience-for-serverless-applications-with-localstack/)
- [Jest ESM Documentation](https://jestjs.io/docs/ecmascript-modules)

### Observability
- [AWS Lambda Powertools for TypeScript](https://aws.amazon.com/blogs/compute/simplifying-serverless-best-practices-with-aws-lambda-powertools-for-typescript/)
- [AWS X-Ray Lambda Tracing Best Practices](https://aws-observability.github.io/observability-best-practices/patterns/Tracing/xraylambda/)

### Build Optimization
- [Medium: 10x faster TypeScript Serverless builds with esbuild](https://medium.com/@arsenyyankovski/how-we-sped-up-our-typescript-serverless-builds-ten-times-70-lambdas-under-1-minute-f79a925dfe4c)
- [AWS Blog: Optimizing Node.js dependencies in Lambda](https://aws.amazon.com/blogs/compute/optimizing-node-js-dependencies-in-aws-lambda/)

### Security
- [RanTheBuilder: 14 AWS Lambda Security Best Practices](https://www.ranthebuilder.cloud/post/14-aws-lambda-security-best-practices-for-building-secure-serverless-applications)
- [AWS Lambda Permissions Documentation](https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html)

---

## Conclusion

This project represents **production-grade serverless architecture** that exceeds most open-source examples and follows AWS Well-Architected Framework principles. The main opportunities for improvement are:

1. **Build performance**: Migrate to esbuild for faster builds and smaller bundles
2. **Observability**: Add AWS Lambda Powertools for structured logging, metrics, and tracing
3. **Webhook reliability**: Add idempotency handling with Powertools

The architecture choices (DynamoDB, ElectroDB, OpenTofu, single-table design) are optimal for the use case and should not be changed.

---

*Assessment Date: December 2025*
*Assessment Version: 1.0*
</file>

<file path="docs/wiki/Methodologies/Production-Debugging.md">
# Production Debugging

## Quick Reference
- **When to use**: Investigating production issues
- **Enforcement**: Recommended
- **Impact if violated**: MEDIUM - Longer resolution time

## Debugging Tools

1. **CloudWatch Logs** - Real-time logs and search
2. **X-Ray** - Distributed tracing (via withXRay wrapper)
3. **CloudWatch Insights** - SQL-like log analysis
4. **GitHub Issues** - Automated error reporting

## Common Issues & Solutions

### Lambda Timeout
```sql
-- CloudWatch Insights query
fields @timestamp, @duration, @message
| filter @message like /Task timed out/
| stats max(@duration) as max_duration
```

**Fix**: Use parallel processing with `Promise.all()` instead of serial loops

### Memory Exhaustion
```sql
-- Check memory usage
fields @timestamp, @maxMemoryUsed/@memorySize as memory_percentage
| filter @type = "REPORT" and memory_percentage > 0.9
```

**Fix**: Stream large files instead of loading into memory

### DynamoDB Throttling
```sql
-- Find throttled requests
fields @timestamp, @message
| filter @message like /ProvisionedThroughputExceededException/
| stats count() by bin(@timestamp, 5m)
```

**Fix**: Implement exponential backoff and batch operations

### API Gateway 502 Errors
```sql
-- Find integration failures
fields @timestamp, method, resource
| filter status = 502
| stats count() by resource
```

**Fix**: Ensure Lambda returns proper API Gateway response format

## X-Ray Integration

All Lambdas use the withXRay wrapper for automatic tracing:

```typescript
export const handler = withXRay(async (event, context, {traceId}) => {
  logInfo('event <=', event)
  // traceId available for correlation
})
```

## Quick Commands

```bash
# Recent errors
aws logs filter-log-events \
  --log-group-name /aws/lambda/FunctionName \
  --filter-pattern ERROR

# X-Ray traces
aws xray get-trace-summaries \
  --time-range-type LastHour \
  --query "TraceSummaries[?ErrorRootCauses]"

# Lambda metrics
aws cloudwatch get-metric-statistics \
  --namespace AWS/Lambda \
  --metric-name Errors \
  --dimensions Name=FunctionName,Value=ProcessFile
```

## Performance Optimization

1. **Cold Starts** - Check duration for first invocation
2. **Memory Allocation** - Monitor @maxMemoryUsed
3. **Concurrent Executions** - Check throttling metrics
4. **External API Calls** - Add timeouts and retries

## Error Patterns

| Error | Cause | Solution |
|-------|-------|----------|
| Task timed out | Long-running operation | Increase timeout or optimize |
| Runtime exited | Memory exhaustion | Increase memory or stream data |
| AccessDenied | IAM permissions | Check Lambda execution role |
| ECONNREFUSED | Network issue | Verify endpoints and security groups |

## Related Patterns

- [CloudWatch Logging](../AWS/CloudWatch-Logging.md) - Logging setup
- [X-Ray Integration](../AWS/X-Ray-Integration.md) - Tracing details
- [Error Handling](../TypeScript/TypeScript-Error-Handling.md) - Error patterns

---

*Use CloudWatch Insights for log analysis, X-Ray for tracing, and monitor key metrics for production debugging.*
</file>

<file path="docs/wiki/Testing/Coverage-Philosophy.md">
# Coverage Philosophy

## Quick Reference
- **When to use**: Planning and writing tests
- **Enforcement**: Recommended - guides testing strategy
- **Impact if violated**: Low - may write unnecessary tests

## Core Principle

**Test YOUR Code, Not Library Code**

Coverage should be a side effect of testing business logic, not a goal itself. Integration tests validate YOUR orchestration, not AWS SDK behavior.

## Test Focus

### ‚ùå Wrong: Testing Libraries
- "Can I upload to S3?" ‚Üí Testing AWS SDK
- "Does DynamoDB query work?" ‚Üí Testing AWS SDK

### ‚úÖ Correct: Testing YOUR Logic
- "Does download workflow complete?" ‚Üí Testing YOUR orchestration
- "After S3 upload, is DynamoDB updated?" ‚Üí Testing YOUR state management
- "Does error rollback DynamoDB?" ‚Üí Testing YOUR error recovery

## Test Types

### Unit Tests
- **Purpose**: Test function logic in isolation
- **Mock**: ALL external dependencies (AWS, APIs, packages)
- **Speed**: Milliseconds per test
- **Location**: `src/lambdas/*/test/index.test.ts`

```typescript
test('applies 15% discount for premium users', () => {
  expect(calculateDiscount(100, true)).toBe(85)
})
```

### Integration Tests
- **Purpose**: Test multi-service workflows end-to-end
- **Use**: Real AWS services (via LocalStack)
- **Test**: YOUR orchestration, state management, error handling
- **Location**: `test/integration/workflows/*.workflow.integration.test.ts`

```typescript
test('download updates DynamoDB and notifies user', async () => {
  await triggerDownload(fileId)
  const file = await getFileFromDB(fileId)
  expect(file.status).toBe('downloaded')
  expect(notificationSent).toBe(true)
})
```

## Integration Test Priority

### High Priority (Multi-Service)
- ‚úÖ webhook ‚Üí DynamoDB ‚Üí queue ‚Üí Lambda ‚Üí S3
- ‚úÖ State transitions across services
- ‚úÖ Error rollback logic
- ‚úÖ Fan-out patterns

### Medium Priority (Service + Logic)
- ‚úÖ Query filtering and pagination
- ‚úÖ Conditional creates/updates
- ‚úÖ Batch operations with partial failures

### Low Priority (Simple CRUD)
- ‚ùå Pure CRUD ‚Üí Unit tests sufficient
- ‚ùå Thin wrappers ‚Üí Covered by unit tests

## Coverage Targets

### Unit Tests
- Lambda handlers: **80%+**
- Utility functions: **90%+**
- Business logic: **85%+**
- Vendor wrappers: Ignore with `/* c8 ignore */`

### Integration Tests
- Coverage happens naturally from workflow tests
- Don't write tests to hit coverage targets
- Focus on workflows, not library behavior

## What to Test

### ‚úÖ DO Test
- Data transformations, validation, calculations
- State transitions and error handling
- Service call sequences and data flow
- Edge cases and boundary conditions

### ‚ùå DON'T Test
- AWS SDK functionality
- NPM package behavior
- Implementation details (variable names, private functions)

## Testing Patterns

### Test YOUR Orchestration
```typescript
test('download workflow completes end-to-end', async () => {
  await createFile(fileId, 'pending')
  await startDownload(fileId)
  await processDownload(fileId)

  const file = await getFile(fileId)
  expect(file.status).toBe('downloaded')

  const notifications = await getNotifications(userId)
  expect(notifications).toHaveLength(1)
})
```

### Ignore Vendor Wrappers in Unit Tests
```typescript
/* c8 ignore start */
export async function createS3Upload(bucket: string, key: string, body: any) {
  // Tested via integration tests
  return new Upload({client: s3Client, params: {Bucket: bucket, Key: key, Body: body}})
}
/* c8 ignore end */
```

## Test Organization

Structure by workflow, not service:
```
test/integration/workflows/
‚îú‚îÄ‚îÄ webhookFeedly.workflow.integration.test.ts
‚îú‚îÄ‚îÄ fileDownload.workflow.integration.test.ts
‚îî‚îÄ‚îÄ deviceRegistration.workflow.integration.test.ts
```

## Success Indicators

### Good
‚úÖ Tests describe business requirements
‚úÖ Tests fail when logic breaks
‚úÖ High coverage of YOUR code
‚úÖ Fast unit tests (< 1s)

### Bad
‚ùå Tests describe implementation
‚ùå High coverage via shallow tests
‚ùå Slow unit tests (> 5s)
‚ùå Many integration tests for CRUD

## Related Patterns
- [Jest ESM Mocking Strategy](Jest-ESM-Mocking-Strategy.md) - Mocking dependencies
- [Mock Type Annotations](Mock-Type-Annotations.md) - Type-safe mocking
- [Lambda Function Patterns](../TypeScript/Lambda-Function-Patterns.md) - What to test

---

*Test YOUR code, not library code. Coverage follows from testing business logic. Focus on workflows and orchestration.*
</file>

<file path="docs/wiki/Testing/Local-CI-Testing.md">
# Local CI Testing

This guide explains how to run CI checks locally before pushing to GitHub, ensuring faster feedback and reduced CI usage.

## Background

Previously, this project used `nektos/act` for local GitHub Actions testing. It was removed due to Apple Silicon (ARM64) incompatibility - the x86-64 Linux containers required for GitHub Actions cause Rosetta translation failures on M1/M2/M3 Macs.

Instead, we use **workflow decomposition**: CI logic is extracted into reusable shell scripts that run both locally and in GitHub Actions, ensuring consistency.

## Quick Start

```bash
# First time setup (installs git hooks)
pnpm run prepare

# Fast CI (no integration tests) - recommended for most iterations
pnpm run ci:local

# Full CI (includes integration tests with LocalStack)
pnpm run ci:local:full
```

## Pre-Push Hook

This project uses Husky to enforce CI checks before pushing. When you run `git push`, the pre-push hook automatically runs `pnpm run ci:local:full` (~5-10 minutes).

**First time setup** (after cloning):
```bash
pnpm install  # Automatically runs 'prepare' which installs git hooks
```

**To verify hooks are installed**:
```bash
git config core.hooksPath  # Should output: .husky/_
```

**To bypass in emergencies**:
```bash
git push --no-verify
```

**Troubleshooting**: If `git config core.hooksPath` shows nothing, run `pnpm run prepare` manually.

## Available Commands

| Command | Duration | What it runs |
|---------|----------|--------------|
| `pnpm run ci:local` | ~2-3 min | All checks except integration tests |
| `pnpm run ci:local:full` | ~5-10 min | Everything including integration tests |
| `pnpm run test:integration` | ~30 sec | Integration tests only (LocalStack must be running) |
| `pnpm run validate:docs` | ~1 sec | Documentation script validation only |
| `pnpm run validate:graphrag` | ~5 sec | GraphRAG freshness check only |
| `pnpm run lint:workflows` | ~1 sec | GitHub Actions YAML validation (requires actionlint) |

### Why Both `ci:local:full` and `test:integration`?

These serve different purposes:

| Command | LocalStack Lifecycle | Use Case |
|---------|---------------------|----------|
| `ci:local:full` | Manages start/stop automatically | Pre-push validation, comprehensive CI |
| `test:integration` | Assumes already running | Fast iteration when developing tests |

**When developing integration tests**, use `test:integration` for rapid feedback:

```bash
# Start LocalStack once at the beginning of your session
pnpm run localstack:start

# Iterate rapidly (~30s per run instead of 5-10 min)
pnpm run test:integration   # run tests
# make changes...
pnpm run test:integration   # run again
# make changes...
pnpm run test:integration   # run again

# Stop when done
pnpm run localstack:stop
```

**For pre-push validation**, use `ci:local:full` (or let the pre-push hook run it automatically).

## What ci:local Checks

The fast CI script (`pnpm run ci:local`) runs these checks in order:

1. **Prerequisites** - Node.js 22+, hcl2json, jq
2. **Dependencies** - `pnpm install --frozen-lockfile`
3. **Build dependencies** - Terraform type generation
4. **Webpack build** - Lambda function compilation
5. **Type checking** - TypeScript compiler
6. **Linting** - ESLint
7. **Documentation validation** - Ensures documented scripts exist
8. **Dependency rules** - Architectural boundary checks
9. **GraphRAG validation** - Knowledge graph freshness
10. **Unit tests** - Jest with mocked AWS services

## What ci:local Does NOT Check

These checks can only run in GitHub Actions:

- **Codecov upload** - Requires GitHub secrets
- **Artifact storage** - GitHub infrastructure
- **PR comments** - Requires GitHub API context
- **Wiki sync** - Only runs on push to master

## Coverage Estimate

Running `ci:local` catches approximately **95%** of issues that would fail in GitHub Actions CI. The remaining 5% are GitHub-specific features that cannot be replicated locally.

## Prerequisites

Before running local CI, ensure you have:

```bash
# Required tools
brew install hcl2json jq

# For integration tests
# Docker Desktop must be installed and running

# Optional: workflow validation
brew install actionlint
```

## Workflow Validation with actionlint

For validating GitHub Actions workflow YAML files without execution:

```bash
# Install actionlint (ARM64 native, no Docker required)
brew install actionlint

# Validate all workflows
pnpm run lint:workflows

# Or run directly
actionlint
```

This catches:
- YAML syntax errors
- Invalid action references
- Expression syntax errors (`${{ }}`)
- Shell script issues (via shellcheck integration)

## Recommended Workflow

1. **During development**: Run `pnpm run precheck` frequently (type check + lint)
2. **Before committing**: Run `pnpm run ci:local` (fast, ~2-3 min)
3. **Before pushing**: The pre-push hook runs `ci:local:full` automatically
4. **After pushing**: Monitor GitHub Actions for the remaining 5% of checks

## Troubleshooting

### "hcl2json not found"

```bash
brew install hcl2json
```

### "jq not found"

```bash
brew install jq
```

### Integration tests fail to connect to LocalStack

```bash
# Ensure Docker is running
docker ps

# Check LocalStack health
pnpm run localstack:health

# Restart LocalStack
pnpm run localstack:stop && pnpm run localstack:start
```

### GraphRAG validation fails

```bash
# Regenerate and commit the knowledge graph
pnpm run graphrag:extract
git add graphrag/knowledge-graph.json
git commit -m "chore: update GraphRAG knowledge graph"
```

### Pre-push hook not running

If `git push` succeeds without running CI checks:

```bash
# Check if hooks are configured
git config core.hooksPath

# If empty or missing, reinstall hooks
pnpm run prepare

# Verify hook exists and is executable
ls -la .husky/pre-push
ls -la .husky/_/pre-push
```

## Architecture

The local CI approach uses **workflow decomposition**:

```
GitHub Actions                    Local Development
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
.github/workflows/                bin/
‚îú‚îÄ‚îÄ unit-tests.yml       ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îú‚îÄ‚îÄ ci-local.sh
‚îÇ   ‚îî‚îÄ‚îÄ calls validate-docs.sh    ‚îú‚îÄ‚îÄ validate-docs.sh
‚îú‚îÄ‚îÄ dependency-check.yml ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îî‚îÄ‚îÄ validate-graphrag.sh
‚îÇ   ‚îî‚îÄ‚îÄ calls validate-graphrag.sh
‚îî‚îÄ‚îÄ integration-tests.yml ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îî‚îÄ‚îÄ test-integration.sh
```

Both CI and local development use the **same scripts**, ensuring:
- Identical behavior between environments
- Easy maintenance (one source of truth)
- No architecture mismatch issues (scripts run natively)

## See Also

- [Coverage Philosophy](./Coverage-Philosophy.md)
- [Jest ESM Mocking Strategy](./Jest-ESM-Mocking-Strategy.md)
- [LocalStack Testing](../Integration/LocalStack-Testing.md)
</file>

<file path="docs/wiki/Testing/Mock-Type-Annotations.md">
# Mock Type Annotations

## Quick Reference
- **When to use**: Creating mock functions with jest.fn()
- **Enforcement**: Required - provides type safety in tests
- **Impact if violated**: Medium - loss of type safety, harder debugging

## The Rule

Use **specific type annotations** for `jest.fn()` when using `mockResolvedValue` or `mockReturnValue`. Avoid generic types like `unknown` or `any`. Never use type escape hatches like `as any`.

## Type Annotation Policy

### ‚ùå AVOID Generic Types
```typescript
// ‚ùå DON'T - No type safety
const sendMock = jest.fn<() => Promise<unknown>>()
const updateMock = jest.fn<() => Promise<any>>()
```

### ‚ùå NEVER Use Type Escape Hatches
```typescript
// ‚ùå ABSOLUTELY FORBIDDEN
const queryMock = jest.fn() as any
const batchGetMock = jest.fn() as unknown
const mockFn = jest.fn() as jest.Mock<any, any>
```

### ‚úÖ USE Specific Types
```typescript
// ‚úÖ DO - Specific return shapes
const sendMock = jest.fn<() => Promise<{StatusCode: number}>>()
  .mockResolvedValue({StatusCode: 202})

const headObjectMock = jest.fn<() => Promise<{ContentLength: number}>>()
  .mockResolvedValue({ContentLength: 1024})
```

### ‚úÖ USE Domain Types
```typescript
import type {YtDlpVideoInfo, YtDlpFormat} from '../../../types/ytdlp'

const fetchVideoInfoMock = jest.fn<() => Promise<YtDlpVideoInfo>>()
  .mockResolvedValue({
    id: 'video-123',
    title: 'Test Video',
    formats: []
  })
```

### ‚úÖ OMIT for Simple Mocks
```typescript
// ‚úÖ TypeScript infers from usage
const logDebugMock = jest.fn()
const spawnMock = jest.fn()
const callbackMock = jest.fn()
```

### ‚ö†Ô∏è USE Promise<void> for mockResolvedValue(undefined)
```typescript
// ‚úÖ Required for void promises
const copyFileMock = jest.fn<() => Promise<void>>()
  .mockResolvedValue(undefined)

// ‚ùå WRONG
const copyFileMock = jest.fn<() => Promise<undefined>>()
  .mockResolvedValue(undefined)
```

## Common Patterns

### AWS SDK Client Mocks
```typescript
jest.unstable_mockModule('@aws-sdk/client-lambda', () => ({
  LambdaClient: jest.fn<() => {send: jest.Mock<() => Promise<{StatusCode: number}>>}>()
    .mockImplementation(() => ({
      send: jest.fn<() => Promise<{StatusCode: number}>>()
        .mockResolvedValue({StatusCode: 202})
    })),
  InvokeCommand: jest.fn()
}))
```

### Vendor Wrapper Mocks
```typescript
jest.unstable_mockModule('../../../lib/vendor/AWS/S3', () => ({
  headObject: jest.fn<() => Promise<{ContentLength: number; ETag: string}>>()
    .mockResolvedValue({ContentLength: 1024, ETag: '"abc123"'}),

  createS3Upload: jest.fn<() => Promise<{done: () => Promise<void>}>>()
    .mockResolvedValue({
      done: jest.fn<() => Promise<void>>().mockResolvedValue(undefined)
    })
}))
```

### NPM Package Mocks
```typescript
class MockYTDlpWrap {
  constructor(public binaryPath: string) {}

  getVideoInfo = jest.fn<() => Promise<{
    id: string
    title: string
    formats: Array<{format_id: string}>
  }>>().mockResolvedValue({
    id: 'video-123',
    title: 'Test',
    formats: [{format_id: 'best'}]
  })
}

jest.unstable_mockModule('yt-dlp-wrap', () => ({
  default: MockYTDlpWrap
}))
```

### Node Built-in Mocks
```typescript
jest.unstable_mockModule('fs/promises', () => ({
  copyFile: jest.fn<() => Promise<void>>().mockResolvedValue(undefined),
  readFile: jest.fn<() => Promise<Buffer>>(),
  writeFile: jest.fn<() => Promise<void>>()
}))
```

## Benefits

### 1. Type Safety
```typescript
const mockFn = jest.fn<() => Promise<{ContentLength: number}>>()
  .mockResolvedValue({ContentLength: 1024})

// This would be a TypeScript error:
// mockFn.mockResolvedValue({ContentLenght: 1024})  // Typo caught!
```

### 2. IntelliSense Support
Auto-completion shows available properties for mock return values.

### 3. Refactoring Safety
When return types change, TypeScript errors guide you to update all mocks.

### 4. Better Error Messages
Specific types provide exact property names in error messages vs no error with `any`.

## When to Use Each Pattern

### Use Specific Type When:
- Mock returns a value (`mockResolvedValue`, `mockReturnValue`)
- Function has well-defined return type
- Testing domain-specific types
- Mocking AWS SDK responses

### Omit Type When:
- Mock is simple callback
- TypeScript can infer from usage
- Mock doesn't return a value
- Testing side effects only

### Use Domain Type When:
- Type is defined in your codebase
- Type represents business domain
- Multiple mocks share same type

## Anti-Patterns

### Generic Unknown
```typescript
// ‚ùå WRONG
const mockFn = jest.fn<() => Promise<unknown>>()
  .mockResolvedValue({anything: 'goes'})

// ‚úÖ CORRECT
const mockFn = jest.fn<() => Promise<{userId: string; status: string}>>()
  .mockResolvedValue({userId: '123', status: 'active'})
```

### Type Assertion
```typescript
// ‚ùå WRONG
const mockFn = jest.fn() as jest.Mock<Promise<UserData>>

// ‚úÖ CORRECT
const mockFn = jest.fn<() => Promise<UserData>>()
```

### Duplicate Type Definitions
```typescript
// ‚ùå WRONG - Duplicating types
const mock1 = jest.fn<() => Promise<{id: string; name: string}>>()
const mock2 = jest.fn<() => Promise<{id: string; name: string}>>()

// ‚úÖ CORRECT - Reuse type
import type {UserInfo} from '../../../types/user'
const mock1 = jest.fn<() => Promise<UserInfo>>()
const mock2 = jest.fn<() => Promise<UserInfo>>()
```

## Migration Guide

### Step 1: Find Problematic Mocks
```bash
grep -r "jest.fn<.*unknown.*>" test/ --include="*.ts"
grep -r "jest.fn<.*any.*>" test/ --include="*.ts"
grep -r "as any" test/ --include="*.ts"
```

### Step 2: Determine Correct Type
```typescript
// Current (bad)
const mockFn = jest.fn() as any

// Find usage
mockFn.mockResolvedValue({id: '123', name: 'Test'})

// Update with inferred type
const mockFn = jest.fn<() => Promise<{id: string; name: string}>>()
```

### Step 3: Import Types if Needed
```typescript
import type {User} from '../../../types/user'

const getUserMock = jest.fn<() => Promise<User>>()
  .mockResolvedValue({
    id: '123',
    name: 'Test User',
    email: 'test@example.com'
  })
```

## Related Patterns
- [Jest ESM Mocking Strategy](Jest-ESM-Mocking-Strategy.md) - When and what to mock
- [Coverage Philosophy](Coverage-Philosophy.md) - Testing principles
- [AWS SDK Encapsulation](../AWS/SDK-Encapsulation-Policy.md) - Mock wrappers not SDK
- [Type Definitions](../TypeScript/Type-Definitions.md) - Where to define types

---

*Use specific type annotations for mocks to maintain type safety. Avoid `any`, `unknown`, and type assertions. Let TypeScript help you write correct tests.*
</file>

<file path="docs/wiki/Getting-Started.md">
# Getting Started with the Development Wiki

This guide helps you understand and use this development conventions wiki effectively.

## What Is This Wiki?

This is a **centralized knowledge base** of development conventions, patterns, and best practices that apply across TypeScript/AWS projects. It represents institutional knowledge captured over time through the Convention Capture System.

### Key Characteristics
- **Universal patterns** - Conventions that work across multiple projects
- **Git-tracked** - Version controlled in `docs/wiki/` directory
- **Auto-synced** - Automatically published to GitHub Wiki for web viewing
- **Continuously updated** - New conventions added as they emerge
- **AI-compatible** - Referenced by AGENTS.md for AI assistants

## How to Navigate

### By Category
The wiki is organized into logical categories:
- **Conventions** - Universal coding standards
- **TypeScript** - Language-specific patterns
- **Testing** - Test strategies and mocking
- **AWS** - Cloud service patterns
- **Bash** - Shell scripting standards
- **Infrastructure** - IaC patterns
- **Methodologies** - Development philosophies
- **Meta** - Documentation about documentation

### By Priority
Look for enforcement levels:
- üö® **Zero-tolerance** - NO exceptions allowed
- ‚ö†Ô∏è **Required** - Must follow unless justified
- üìã **Recommended** - Should follow for consistency
- üí° **Optional** - Consider for specific cases

### By Search
- Use your IDE's file search in `docs/wiki/`
- Use GitHub Wiki search (once synced)
- grep for specific patterns

## How to Use in Your Project

### Step 1: Set Up AGENTS.md
Create an AGENTS.md file in your project root with:
```markdown
# Project Context for AI Agents

## Convention Capture System
[Copy from template - includes detection patterns]

## Project Overview
[Your project-specific content]

## Wiki Conventions to Follow
[Links to relevant wiki pages]
```

### Step 2: Create Passthrough Files
For tool compatibility:

**CLAUDE.md:**
```
@AGENTS.md
```

**GEMINI.md:**
```markdown
# See AGENTS.md
This project uses AGENTS.md as the single source of truth.
Please see AGENTS.md for documentation.
```

### Step 3: Reference Wiki Pages
From your AGENTS.md or documentation:
```markdown
## Conventions to Follow
- [Naming](Conventions/Naming-Conventions.md)
- [AWS SDK](AWS/SDK-Encapsulation-Policy.md)
- [Testing](Testing/Jest-ESM-Mocking-Strategy.md)
```

### Step 4: Track Project Conventions
Create `docs/conventions-tracking.md` for project-specific patterns:
```markdown
## Project-Specific Conventions

### Detected: 2025-11-22
1. **Pattern Name**
   - What: [Description]
   - Why: [Rationale]
   - Status: ‚úÖ Documented
```

## Understanding Convention Pages

Each wiki page follows a standard template:

### Page Structure
```markdown
# [Pattern Name]

## Quick Reference
- **When to use**: [One-line]
- **Enforcement**: [Level]
- **Impact if violated**: [High/Medium/Low]

## The Rule
[Clear statement]

## Examples
### ‚úÖ Correct
[Good example]

### ‚ùå Incorrect
[Bad example]

## Rationale
[Why this exists]

## Enforcement
[How to check/automate]

## Related Patterns
[Links to related pages]
```

### Reading Priority
1. **Quick Reference** - Get the essence
2. **The Rule** - Understand the requirement
3. **Examples** - See it in practice
4. **Enforcement** - Know how it's checked

## For AI Assistants

### Starting a Session
1. Read AGENTS.md (which references this wiki)
2. Check `docs/conventions-tracking.md` for project patterns
3. Activate convention detection mode
4. Reference wiki pages as needed

### During Development
- Follow wiki patterns by default
- Flag new conventions when detected
- Update Emerging Conventions log
- Reference wiki pages in explanations

### Example AI Usage
```
User: "How should I handle AWS SDK imports?"

AI: Per [AWS SDK Encapsulation Policy](AWS/SDK-Encapsulation-Policy.md),
NEVER import AWS SDK directly. Use vendor wrappers in lib/vendor/AWS/.

This is a zero-tolerance rule to maintain encapsulation and testability.
```

## For Developers

### Finding Patterns
1. Check relevant category in wiki
2. Search for keywords
3. Review enforcement level
4. Follow examples

### Contributing New Patterns
1. Detect pattern emergence
2. Document in conventions-tracking.md
3. Create wiki page following template
4. Submit PR with:
   - Clear rationale
   - Good/bad examples
   - Enforcement strategy
5. Update Home.md navigation

### Reporting Issues
- Missing pattern? Create issue
- Unclear documentation? Suggest edit
- Conflicting patterns? Raise for discussion

## Common Use Cases

### Starting New Project
1. Copy AGENTS.md template
2. Add project-specific content
3. Link to relevant wiki pages
4. Create conventions-tracking.md

### Onboarding Developer
1. Point to wiki Home.md
2. Highlight zero-tolerance rules
3. Show project's AGENTS.md
4. Explain Convention Capture

### Code Review
1. Reference wiki for standards
2. Link to specific patterns
3. Check enforcement level
4. Suggest wiki updates if needed

### Debugging Test Failures
1. Check [Jest ESM Mocking](Testing/Jest-ESM-Mocking-Strategy.md)
2. Review transitive dependencies
3. Follow 7-step checklist
4. Mock all module-level imports

## Automation & Sync

### Local Files
- Edit in `docs/wiki/` directory
- Commit via normal git workflow
- Review via pull requests

### GitHub Wiki Sync
- Automatic on merge to master
- GitHub Actions workflow
- ~30 second delay
- Creates web-friendly view

### Benefits
- **Developers** - Edit in IDE
- **Users** - Browse on web
- **Automation** - Zero maintenance
- **Version Control** - Full git history

## Best Practices

### Do's
‚úÖ Reference wiki pages in code comments
‚úÖ Update wiki when patterns evolve
‚úÖ Use Convention Capture System
‚úÖ Follow zero-tolerance rules strictly
‚úÖ Link from AGENTS.md

### Don'ts
‚ùå Duplicate wiki content in projects
‚ùå Ignore zero-tolerance rules
‚ùå Create conflicting local patterns
‚ùå Skip convention detection
‚ùå Edit GitHub Wiki directly (edit docs/wiki/)

## Quick Commands

### Search Wiki
```bash
# Find all zero-tolerance rules
grep -r "Zero-tolerance" docs/wiki/

# Find AWS patterns
ls docs/wiki/AWS/

# Search for specific pattern
grep -r "camelCase" docs/wiki/
```

### Validate Links
```bash
# Check for broken wiki links
find docs/wiki -name "*.md" -exec grep -l "](.*)" {} \;
```

### View Recent Changes
```bash
# See recent wiki updates
git log --oneline docs/wiki/ | head -10
```

## Troubleshooting

### Can't Find Pattern?
1. Search by keyword
2. Check multiple categories
3. Review Emerging Conventions
4. Ask in team chat
5. Create if truly missing

### Conflicting Patterns?
1. Check enforcement levels
2. Project-specific overrides project
3. Zero-tolerance always wins
4. Raise for team discussion

### Wiki Not Syncing?
1. Check GitHub Actions status
2. Verify wiki enabled in settings
3. Check workflow permissions
4. Manual trigger if needed

## Next Steps

1. **Explore** - Browse categories that interest you
2. **Apply** - Use patterns in your code
3. **Contribute** - Add missing patterns
4. **Detect** - Flag emerging conventions
5. **Evolve** - Improve existing patterns

---

*Remember: This wiki is a living document. It grows through the Convention Capture System and represents our collective knowledge. Use it, improve it, and help it evolve.*
</file>

<file path="eslint-local-rules/rules/no-direct-aws-sdk-import.cjs">
/**
 * no-direct-aws-sdk-import
 * CRITICAL: Blocks direct AWS SDK imports outside vendor directory
 *
 * Mirrors: src/mcp/validation/rules/aws-sdk-encapsulation.ts
 */
‚ãÆ----
// AWS SDK v3
‚ãÆ----
'aws-sdk', // v2
// AWS Lambda Powertools
‚ãÆ----
// AWS SDK
‚ãÆ----
// AWS Lambda Powertools
‚ãÆ----
function getSuggestion(moduleSpecifier)
‚ãÆ----
function isForbiddenImport(moduleSpecifier)
‚ãÆ----
create(context)
‚ãÆ----
// Allow imports in vendor directories and integration test helpers
// - lib/vendor/AWS/ - AWS SDK wrappers
// - lib/vendor/Powertools/ - AWS Lambda Powertools wrappers
// - lib/vendor/ElectroDB/ - ElectroDB service (needs DynamoDB client)
// - test/integration/helpers/ - LocalStack setup (needs direct client access)
‚ãÆ----
ImportDeclaration(node)
CallExpression(node)
‚ãÆ----
// Check dynamic imports
</file>

<file path="eslint-local-rules/test/no-direct-aws-sdk-import.test.cjs">
/**
 * Tests for no-direct-aws-sdk-import ESLint rule
 */
‚ãÆ----
// Allowed: vendor directory imports
‚ãÆ----
// Allowed: non-AWS imports
‚ãÆ----
// Allowed: internal vendor wrapper
‚ãÆ----
// Allowed: relative vendor import
‚ãÆ----
// Allowed: Powertools vendor directory imports
‚ãÆ----
// Allowed: internal Powertools vendor wrapper
‚ãÆ----
// Forbidden: direct DynamoDB SDK import in Lambda
‚ãÆ----
// Forbidden: direct S3 SDK import in entity
‚ãÆ----
// Forbidden: direct Lambda SDK import
‚ãÆ----
// Forbidden: lib-dynamodb import
‚ãÆ----
// Forbidden: AWS SDK v2
‚ãÆ----
// Forbidden: direct Powertools Logger import in Lambda
‚ãÆ----
// Forbidden: direct Powertools Tracer import
‚ãÆ----
// Forbidden: direct Powertools idempotency import
</file>

<file path="eslint-local-rules/index.cjs">
/**
 * ESLint Local Rules Plugin
 * Project-specific ESLint rules for in-editor convention enforcement
 *
 * These rules mirror MCP validation rules to provide immediate feedback in the editor.
 *
 * Phase 1 (CRITICAL):
 *   - no-direct-aws-sdk-import: Block direct AWS SDK imports
 *   - cascade-delete-order: Detect Promise.all with deletes
 *   - use-electrodb-mock-helper: Enforce mock helper usage
 *
 * Phase 2 (HIGH):
 *   - response-helpers: Enforce response() helper usage
 *   - env-validation: Enforce getRequiredEnv() usage
 *   - authenticated-handler-enforcement: Enforce centralized auth wrappers
 */
‚ãÆ----
// Phase 1: CRITICAL
‚ãÆ----
// Phase 2: HIGH
</file>

<file path="graphrag/extract.ts">
/**
 * GraphRAG extraction for Lambda chains and entity relationships
 * Builds a knowledge graph from the codebase for multi-hop reasoning
 *
 * This script dynamically discovers:
 * - Lambdas from src/lambdas/ directory
 * - Entity relationships from src/entities/ directory
 * - Service dependencies from build/graph.json transitive dependencies
 *
 * Semantic metadata (triggers, purposes) is read from graphrag/metadata.json
 */
import fs from 'fs/promises'
import path from 'path'
import {fileURLToPath} from 'url'
‚ãÆ----
interface Node {
  id: string
  type: 'Lambda' | 'Entity' | 'Service' | 'External'
  properties: Record<string, unknown>
}
interface Edge {
  source: string
  target: string
  relationship: string
  properties?: Record<string, unknown>
}
interface KnowledgeGraph {
  nodes: Node[]
  edges: Edge[]
  metadata: {
    version: string
    description: string
    sources: {
      lambdas: string
      dependencies: string
      metadata: string
    }
  }
}
interface DependencyGraph {
  metadata: {generated: string; projectRoot: string; totalFiles: number}
  files: Record<string, {file: string; imports: string[]}>
  transitiveDependencies: Record<string, string[]>
}
interface LambdaMetadata {
  trigger: string
  purpose: string
}
interface ServiceMetadata {
  name: string
  type: string
  vendorPath?: string | null
  description?: string
}
interface Metadata {
  lambdas: Record<string, LambdaMetadata>
  externalServices: ServiceMetadata[]
  awsServices: ServiceMetadata[]
  entityRelationships: Array<{from: string; to: string; type: string}>
  lambdaInvocations: Array<{from: string; to: string; via: string}>
  serviceToServiceEdges: Array<{from: string; to: string; relationship: string; event?: string}>
}
/**
 * Discover Lambda names from src/lambdas/ directory
 */
async function discoverLambdas(): Promise<string[]>
/**
 * Discover Entity names from src/entities/ directory
 */
async function discoverEntities(): Promise<string[]>
/**
 * Load the dependency graph from build/graph.json
 */
async function loadDependencyGraph(): Promise<DependencyGraph>
/**
 * Load semantic metadata from graphrag/metadata.json
 */
async function loadMetadata(): Promise<Metadata>
/**
 * Extract AWS services used by a Lambda from its transitive dependencies
 */
function extractAwsServices(deps: string[], awsServices: ServiceMetadata[]): string[]
‚ãÆ----
// Match src/lib/vendor/AWS/* patterns
‚ãÆ----
// Find the service name from metadata
‚ãÆ----
/**
 * Extract external services used by a Lambda from its transitive dependencies
 */
function extractExternalServices(deps: string[], externalServices: ServiceMetadata[]): string[]
‚ãÆ----
// Match src/lib/vendor/* patterns (non-AWS)
‚ãÆ----
// Match src/types/vendor/* patterns (for Feedly, etc.)
‚ãÆ----
/**
 * Extract entities used by a Lambda from its transitive dependencies
 */
function extractEntities(deps: string[], knownEntities: string[]): string[]
/**
 * Extract knowledge graph from codebase
 */
export async function extractKnowledgeGraph(): Promise<KnowledgeGraph>
‚ãÆ----
// Load all data sources
‚ãÆ----
// 1. Add Lambda nodes
‚ãÆ----
// 2. Add Entity nodes
‚ãÆ----
// 3. Add AWS Service nodes
‚ãÆ----
// 4. Add External Service nodes
‚ãÆ----
// 5. Add Lambda ‚Üí Service/External edges (derived from dependency graph)
‚ãÆ----
// AWS Services
‚ãÆ----
// External Services
‚ãÆ----
// Entities
‚ãÆ----
// Add trigger service edge based on metadata
‚ãÆ----
// 6. Add Lambda ‚Üí Lambda invocation edges (from metadata)
‚ãÆ----
// 7. Add Entity ‚Üí Entity relationship edges (from metadata)
‚ãÆ----
// 8. Add Service ‚Üí Service edges (from metadata)
‚ãÆ----
/**
 * Save knowledge graph to file
 */
async function saveKnowledgeGraph(graph: KnowledgeGraph, outputPath: string)
/**
 * Generate graph statistics
 */
function analyzeGraph(graph: KnowledgeGraph)
‚ãÆ----
// Count node types
‚ãÆ----
// Count relationship types
‚ãÆ----
// Find most connected nodes
‚ãÆ----
// Find Lambda chains
const findChains = (start: string, visited: Set<string> = new Set()): string[][] =>
// Find all Lambda invocation chains
‚ãÆ----
// Find Lambdas missing metadata
‚ãÆ----
// Main execution
async function main()
// Run if executed directly
</file>

<file path="graphrag/metadata.json">
{
  "$schema": "./metadata.schema.json",
  "description": "Semantic metadata for GraphRAG that cannot be auto-derived from code",
  "lambdas": {
    "ApiGatewayAuthorizer": {
      "trigger": "API Gateway",
      "purpose": "Authorize API requests via Better Auth sessions"
    },
    "CloudfrontMiddleware": {
      "trigger": "CloudFront",
      "purpose": "Edge processing for CDN requests"
    },
    "FileCoordinator": {
      "trigger": "CloudWatch Events",
      "purpose": "Scheduled job to orchestrate pending file downloads"
    },
    "ListFiles": {
      "trigger": "API Gateway",
      "purpose": "List files available to authenticated user"
    },
    "LogClientEvent": {
      "trigger": "API Gateway",
      "purpose": "Log client-side events for analytics"
    },
    "LoginUser": {
      "trigger": "API Gateway",
      "purpose": "Authenticate user via Sign In With Apple"
    },
    "PruneDevices": {
      "trigger": "CloudWatch Events",
      "purpose": "Scheduled cleanup of inactive devices"
    },
    "RefreshToken": {
      "trigger": "API Gateway",
      "purpose": "Refresh authentication token"
    },
    "RegisterDevice": {
      "trigger": "API Gateway",
      "purpose": "Register iOS device for push notifications"
    },
    "RegisterUser": {
      "trigger": "API Gateway",
      "purpose": "Register new user via Sign In With Apple"
    },
    "S3ObjectCreated": {
      "trigger": "S3 Event",
      "purpose": "Handle uploaded files and notify users"
    },
    "SendPushNotification": {
      "trigger": "SQS",
      "purpose": "Send push notification to user devices via APNS"
    },
    "StartFileUpload": {
      "trigger": "Lambda Invoke",
      "purpose": "Initiate file download from YouTube"
    },
    "UserDelete": {
      "trigger": "API Gateway",
      "purpose": "Delete user and cascade to related records"
    },
    "UserSubscribe": {
      "trigger": "API Gateway",
      "purpose": "Manage user topic subscriptions"
    },
    "WebhookFeedly": {
      "trigger": "API Gateway",
      "purpose": "Process incoming Feedly webhook articles"
    }
  },
  "externalServices": [
    { "name": "Feedly", "type": "content", "description": "RSS feed aggregation" },
    { "name": "YouTube", "type": "media", "description": "Video download source" },
    { "name": "APNS", "type": "notification", "description": "Apple Push Notification Service" },
    { "name": "Sign In With Apple", "type": "auth", "description": "Apple OAuth provider" },
    { "name": "GitHub", "type": "integration", "description": "Issue creation for errors" }
  ],
  "awsServices": [
    { "name": "DynamoDB", "type": "database", "vendorPath": "AWS/DynamoDB" },
    { "name": "S3", "type": "storage", "vendorPath": "AWS/S3" },
    { "name": "SNS", "type": "notification", "vendorPath": "AWS/SNS" },
    { "name": "SQS", "type": "queue", "vendorPath": "AWS/SQS" },
    { "name": "Lambda", "type": "compute", "vendorPath": "AWS/Lambda" },
    { "name": "CloudWatch", "type": "monitoring", "vendorPath": "AWS/CloudWatch" },
    { "name": "SecretsManager", "type": "secrets", "vendorPath": "AWS/SecretsManager" },
    { "name": "StepFunctions", "type": "orchestration", "vendorPath": "AWS/StepFunctions" },
    { "name": "XRay", "type": "tracing", "vendorPath": "AWS/XRay" },
    { "name": "API Gateway", "type": "api", "vendorPath": null },
    { "name": "CloudFront", "type": "cdn", "vendorPath": null }
  ],
  "entityRelationships": [
    { "from": "Users", "to": "UserFiles", "type": "has_many" },
    { "from": "Users", "to": "UserDevices", "type": "has_many" },
    { "from": "Users", "to": "Sessions", "type": "has_many" },
    { "from": "Users", "to": "Accounts", "type": "has_many" },
    { "from": "Files", "to": "UserFiles", "type": "has_many" },
    { "from": "Files", "to": "FileDownloads", "type": "has_many" },
    { "from": "Devices", "to": "UserDevices", "type": "has_many" },
    { "from": "UserFiles", "to": "Users", "type": "belongs_to" },
    { "from": "UserFiles", "to": "Files", "type": "belongs_to" },
    { "from": "UserDevices", "to": "Users", "type": "belongs_to" },
    { "from": "UserDevices", "to": "Devices", "type": "belongs_to" },
    { "from": "Sessions", "to": "Users", "type": "belongs_to" },
    { "from": "Accounts", "to": "Users", "type": "belongs_to" },
    { "from": "FileDownloads", "to": "Files", "type": "belongs_to" },
    { "from": "Users", "to": "VerificationTokens", "type": "has_many" },
    { "from": "VerificationTokens", "to": "Users", "type": "belongs_to" }
  ],
  "lambdaInvocations": [
    { "from": "FileCoordinator", "to": "StartFileUpload", "via": "Lambda invokeAsync" },
    { "from": "S3ObjectCreated", "to": "SendPushNotification", "via": "SQS" }
  ],
  "serviceToServiceEdges": [
    { "from": "API Gateway", "to": "Lambda", "relationship": "triggers" },
    { "from": "S3", "to": "Lambda", "relationship": "triggers", "event": "s3:ObjectCreated" },
    { "from": "CloudWatch", "to": "Lambda", "relationship": "triggers", "event": "scheduled" },
    { "from": "SQS", "to": "Lambda", "relationship": "triggers" }
  ]
}
</file>

<file path="scripts/generateDependencyGraph.ts">
import {Project, SourceFile, SyntaxKind} from 'ts-morph'
import {writeFileSync, mkdirSync} from 'fs'
import {dirname, relative, resolve, join} from 'path'
import {fileURLToPath} from 'url'
‚ãÆ----
interface FileImport {
  file: string
  imports: string[]
}
interface DependencyGraph {
  metadata: {
    generated: string
    projectRoot: string
    totalFiles: number
  }
  files: Record<string, FileImport>
  transitiveDependencies: Record<string, string[]>
}
/**
 * Resolves an import path to a relative project path
 */
function resolveImportPath(sourceFile: SourceFile, importPath: string, projectRoot: string): string | null
‚ãÆ----
// Ignore node_modules imports
‚ãÆ----
// Try common extensions
‚ãÆ----
// Path doesn't exist, try next extension
‚ãÆ----
/**
 * Get all transitive dependencies for a file (recursive)
 */
function getTransitiveDependencies(file: string, graph: Record<string, FileImport>, visited = new Set<string>()): string[]
‚ãÆ----
// Return unique dependencies
‚ãÆ----
/**
 * Generate dependency graph from TypeScript project
 */
function generateDependencyGraph(): DependencyGraph
‚ãÆ----
// Get all source files (excluding node_modules)
‚ãÆ----
// Build file-level import graph
‚ãÆ----
// Get all import declarations
‚ãÆ----
// Get dynamic imports
‚ãÆ----
// Calculate transitive dependencies
‚ãÆ----
/**
 * Main execution
 */
function main()
‚ãÆ----
// Ensure build directory exists
‚ãÆ----
// Write graph to JSON
‚ãÆ----
// Run if executed directly
</file>

<file path="src/entities/Devices.ts">
import {documentClient, Entity} from '#lib/vendor/ElectroDB/entity'
/**
 * ElectroDB entity schema for the Devices DynamoDB table.
 * This entity manages device registrations for push notifications.
 */
‚ãÆ----
// Type exports for use in application code
export type DeviceItem = ReturnType<typeof Devices.parse>
export type CreateDeviceInput = Parameters<typeof Devices.create>[0]
export type UpdateDeviceInput = Parameters<typeof Devices.update>[0]
export type UpsertDeviceInput = Parameters<typeof Devices.upsert>[0]
</file>

<file path="src/entities/UserDevices.ts">
import {documentClient, Entity} from '#lib/vendor/ElectroDB/entity'
/**
 * ElectroDB entity schema for the UserDevices relationship.
 * This entity manages the many-to-many relationship between users and devices.
 * Each record represents a single user-device association (not a Set).
 *
 * Single-table design enables bidirectional queries via collections:
 * - UserCollection (gsi1): Query all devices for a user
 * - DeviceCollection (gsi3): Query all users for a device
 */
‚ãÆ----
// Type exports for use in application code
export type UserDeviceItem = ReturnType<typeof UserDevices.parse>
export type CreateUserDeviceInput = Parameters<typeof UserDevices.create>[0]
export type UpdateUserDeviceInput = Parameters<typeof UserDevices.update>[0]
</file>

<file path="src/entities/UserFiles.ts">
import {documentClient, Entity} from '#lib/vendor/ElectroDB/entity'
/**
 * ElectroDB entity schema for the UserFiles relationship.
 * This entity manages the many-to-many relationship between users and files.
 * Each record represents a single user-file association (not a Set).
 *
 * Single-table design enables bidirectional queries via collections:
 * - UserCollection (gsi1): Query all files for a user
 * - FileCollection (gsi2): Query all users for a file
 */
‚ãÆ----
// Type exports for use in application code
export type UserFileItem = ReturnType<typeof UserFiles.parse>
export type CreateUserFileInput = Parameters<typeof UserFiles.create>[0]
export type UpdateUserFileInput = Parameters<typeof UserFiles.update>[0]
</file>

<file path="src/entities/VerificationTokens.ts">
import {documentClient, Entity} from '#lib/vendor/ElectroDB/entity'
/**
 * ElectroDB entity schema for Better Auth verification tokens.
 * Manages email verification tokens and magic link tokens with automatic expiration.
 *
 * Better Auth Verification Token Schema:
 * - identifier: email address or user identifier
 * - token: verification token (hashed)
 * - expiresAt: token expiration timestamp
 * - createdAt: token creation timestamp
 *
 * Note: Verification tokens are single-use and should be deleted after verification.
 * DynamoDB TTL is configured on the ttl attribute to automatically clean up expired tokens.
 */
‚ãÆ----
default: () => Math.floor(Date.now() / 1000) + 86400 // 24 hours from now
‚ãÆ----
// Type exports for use in application code
export type VerificationTokenItem = ReturnType<typeof VerificationTokens.parse>
export type CreateVerificationTokenInput = Parameters<typeof VerificationTokens.create>[0]
</file>

<file path="src/lib/vendor/Powertools/idempotency.ts">
/**
 * AWS Lambda Powertools Idempotency configuration
 * Prevents duplicate processing of webhook requests
 * @see https://docs.aws.amazon.com/powertools/typescript/latest/utilities/idempotency/
 */
import {IdempotencyConfig, makeIdempotent} from '@aws-lambda-powertools/idempotency'
import {DynamoDBPersistenceLayer} from '@aws-lambda-powertools/idempotency/dynamodb'
import {createDynamoDBClient} from '#lib/vendor/AWS/clients'
/**
 * Get the idempotency table name from environment
 * Lazy initialization to work with Jest mocking
 */
function getTableName(): string
/**
 * DynamoDB persistence layer for idempotency records
 * Automatically manages record creation, expiration, and cleanup via TTL
 * Uses project's LocalStack-aware DynamoDB client for testing compatibility
 */
export function createPersistenceStore(): DynamoDBPersistenceLayer
/**
 * Default idempotency configuration
 * - Uses composite key of userId + fileId for webhook requests
 * - Records expire after 1 hour (webhook deduplication window)
 */
‚ãÆ----
expiresAfterSeconds: 3600 // 1 hour
</file>

<file path="src/lib/vendor/Powertools/index.ts">
/**
 * AWS Lambda Powertools configuration
 * Centralized observability utilities for Lambda functions
 * @see https://docs.aws.amazon.com/powertools/typescript/latest/
 */
import {Logger} from '@aws-lambda-powertools/logger'
import {injectLambdaContext} from '@aws-lambda-powertools/logger/middleware'
import {Tracer} from '@aws-lambda-powertools/tracer'
import {captureLambdaHandler} from '@aws-lambda-powertools/tracer/middleware'
import {Metrics, MetricUnit} from '@aws-lambda-powertools/metrics'
import {logMetrics} from '@aws-lambda-powertools/metrics/middleware'
type LogLevel = 'DEBUG' | 'INFO' | 'WARN' | 'ERROR' | 'SILENT'
/**
 * Powertools Logger instance
 * Provides structured JSON logging with automatic context enrichment
 */
‚ãÆ----
/**
 * Powertools Tracer instance
 * Enhanced X-Ray tracing with automatic subsegment creation
 */
‚ãÆ----
/**
 * Powertools Metrics instance
 * CloudWatch embedded metrics format (EMF) for custom metrics
 */
‚ãÆ----
// Re-export middleware functions for use with middy
</file>

<file path="src/mcp/handlers/conventions.ts">
/**
 * Convention query handler for MCP server
 * Provides access to project conventions from conventions-tracking.md and wiki
 *
 * Data is dynamically loaded from:
 * - docs/conventions-tracking.md (structured convention registry)
 * - docs/wiki/ (detailed documentation pages)
 */
import {discoverWikiPages, loadConventions, loadWikiPage, searchWikiPages} from './data-loader.js'
import {type ConventionCategory, type ConventionSeverity, filterByCategory, filterBySeverity, searchConventions} from '../parsers/convention-parser.js'
export type ConventionQueryType = 'list' | 'search' | 'category' | 'enforcement' | 'detail' | 'wiki'
export interface ConventionQueryArgs {
  query: ConventionQueryType
  term?: string
  category?: ConventionCategory
  severity?: ConventionSeverity
  convention?: string
}
export async function handleConventionsQuery(args: ConventionQueryArgs)
‚ãÆ----
// Load conventions dynamically
‚ãÆ----
// Return all conventions with summary info
‚ãÆ----
// Group by severity for easier reading
‚ãÆ----
// Search conventions
‚ãÆ----
// Also search wiki pages
‚ãÆ----
wikiMatches: wikiMatches.slice(0, 10), // Limit wiki results
‚ãÆ----
// List available categories with counts
‚ãÆ----
// Return conventions grouped by severity
‚ãÆ----
// Find convention by name (case-insensitive partial match)
‚ãÆ----
// If wiki path exists, load the full documentation
‚ãÆ----
wikiContent: wikiContent ? wikiContent.substring(0, 3000) : undefined // Limit content size
‚ãÆ----
// List all wiki pages or get specific page content
‚ãÆ----
// Group by directory
‚ãÆ----
const dir = parts.length > 3 ? parts[2] : 'root' // docs/wiki/[Category]/
‚ãÆ----
// Load specific page
‚ãÆ----
content: content.substring(0, 5000) // Limit content size
</file>

<file path="src/mcp/handlers/electrodb.ts">
/**
 * ElectroDB entity query handler for MCP server
 * Provides entity schemas and relationships
 *
 * Data is dynamically loaded from:
 * - src/entities/ directory (Entity discovery)
 * - graphrag/metadata.json (relationships)
 */
import {getEntityInfo, getLambdaConfigs} from './data-loader.js'
// Re-export with old name for backwards compatibility
‚ãÆ----
export async function handleEntityQuery(args:
‚ãÆ----
// Return all entity names with their file locations
‚ãÆ----
// Filter relationships for this entity
‚ãÆ----
// Collections are defined in src/entities/Collections.ts
‚ãÆ----
// Show which Lambdas use which entities
</file>

<file path="src/mcp/handlers/infrastructure.ts">
/**
 * Infrastructure query handler for MCP server
 * Provides AWS infrastructure configuration
 *
 * Data is dynamically loaded from:
 * - graphrag/metadata.json (services)
 * - build/graph.json (dependencies)
 */
import {getAwsServices, getExternalServices, getLambdaConfigs} from './data-loader.js'
export async function handleInfrastructureQuery(args:
‚ãÆ----
{name: 'Primary', pk: 'pk', sk: 'sk'}, // fmt: multiline
</file>

<file path="src/mcp/handlers/lambda.ts">
/**
 * Lambda query handler for MCP server
 * Provides Lambda function configurations and dependencies
 *
 * Data is dynamically loaded from:
 * - src/lambdas/ directory (Lambda discovery)
 * - build/graph.json (dependencies)
 * - graphrag/metadata.json (semantic info)
 */
import {getLambdaConfigs, getLambdaInvocations} from './data-loader.js'
export async function handleLambdaQuery(args:
‚ãÆ----
// Load configs dynamically
‚ãÆ----
// Environment variables are defined in Terraform, return a note
</file>

<file path="src/mcp/handlers/naming.ts">
/**
 * Naming conventions handler for MCP server
 * Provides naming validation and TypeSpec alignment checking
 */
‚ãÆ----
import {fileURLToPath} from 'url'
import {Project, SourceFile} from 'ts-morph'
‚ãÆ----
// Type definitions for naming analysis
interface TypeDefinition {
  name: string
  kind: 'interface' | 'type' | 'enum' | 'model'
  properties: PropertyDefinition[]
  file: string
  line: number
}
interface PropertyDefinition {
  name: string
  type: string
  optional: boolean
}
interface AlignmentIssue {
  typeName: string
  issue: string
  typeScriptValue?: string
  typeSpecValue?: string
  suggestion: string
}
interface NamingViolation {
  name: string
  file: string
  line: number
  issue: string
  currentPattern: string
  expectedPattern: string
  autoFixable: boolean
  suggestedName: string
}
// Naming convention patterns
‚ãÆ----
/**
 * Parse TypeSpec models from a .tsp file
 */
async function parseTypeSpecModels(tspPath: string): Promise<TypeDefinition[]>
‚ãÆ----
// Match model definition
‚ãÆ----
// Match enum definition
‚ãÆ----
// Track brace depth
‚ãÆ----
// Match property
‚ãÆ----
// End of model
‚ãÆ----
// File doesn't exist or can't be read
‚ãÆ----
/**
 * Parse TypeScript types from a file
 */
async function parseTypeScriptTypes(filePath: string): Promise<TypeDefinition[]>
‚ãÆ----
// Get interfaces
‚ãÆ----
// Get type aliases with object structure
‚ãÆ----
// Get enums
‚ãÆ----
// File parsing error
‚ãÆ----
/**
 * Check alignment between TypeScript types and TypeSpec models
 */
export async function handleTypeAlignmentQuery(
  args: {typeName?: string; query: 'check' | 'list' | 'all'}
): Promise<
‚ãÆ----
// Parse TypeSpec models
‚ãÆ----
// Parse TypeScript types
‚ãÆ----
// Skip files that don't exist
‚ãÆ----
// Check specific type or all types
‚ãÆ----
// Check properties alignment for models (not enums)
‚ãÆ----
// Find missing in TypeScript
‚ãÆ----
// Find extra in TypeScript
‚ãÆ----
// Check enum value alignment
‚ãÆ----
/**
 * Validate naming conventions across files
 */
export async function handleNamingValidationQuery(
  args: {file?: string; query: 'validate' | 'suggest' | 'all'}
): Promise<
‚ãÆ----
// Get files to check
‚ãÆ----
// Check all type files
‚ãÆ----
continue // Skip files that don't exist
‚ãÆ----
// Check interfaces
‚ãÆ----
// Check forbidden prefixes
‚ãÆ----
// Check type aliases
‚ãÆ----
// Check enums for PascalCase
‚ãÆ----
// Check if value is lowercase
</file>

<file path="src/mcp/validation/rules/authenticated-handler-enforcement.test.ts">
/**
 * Unit tests for authenticated-handler-enforcement rule
 * HIGH: Use wrapAuthenticatedHandler/wrapOptionalAuthHandler instead of manual auth checks
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
// Module loaded via dynamic import
‚ãÆ----
// Create ts-morph project for in-memory source files
</file>

<file path="src/mcp/validation/rules/authenticated-handler-enforcement.ts">
/**
 * Authenticated Handler Enforcement Rule
 * HIGH: Detects manual getUserDetailsFromEvent() + UserStatus checks and suggests using
 * wrapAuthenticatedHandler or wrapOptionalAuthHandler instead
 *
 * This rule promotes centralized auth handling for consistency and type safety.
 */
import type {SourceFile} from 'ts-morph'
import {SyntaxKind} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Patterns that indicate manual auth handling (anti-patterns)
 */
‚ãÆ----
/**
 * Determine which wrapper to suggest based on context
 */
function getSuggestion(hasAnonymousCheck: boolean, hasUnauthenticatedCheck: boolean): string
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// Skip if file doesn't appear to be a Lambda handler
‚ãÆ----
// Check for getUserDetailsFromEvent import or call
‚ãÆ----
return violations // No manual auth handling detected
‚ãÆ----
// Find the location of getUserDetailsFromEvent
‚ãÆ----
// Check for UserStatus checks
‚ãÆ----
// Check if already using the new wrappers
‚ãÆ----
// Already using new wrappers - check if getUserDetailsFromEvent is still present (shouldn't be)
‚ãÆ----
// Using getUserDetailsFromEvent without new wrappers - this is the anti-pattern
</file>

<file path="src/mcp/validation/rules/aws-sdk-encapsulation.test.ts">
/**
 * Unit tests for aws-sdk-encapsulation rule
 * CRITICAL: No direct AWS SDK imports outside lib/vendor/AWS/
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
// Module loaded via dynamic import
‚ãÆ----
// Create ts-morph project for in-memory source files
</file>

<file path="src/mcp/validation/rules/cascade-safety.test.ts">
/**
 * Unit tests for cascade-safety rule
 * CRITICAL: Enforce safe cascade deletion patterns
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
// Module loaded via dynamic import
‚ãÆ----
// Create ts-morph project for in-memory source files
‚ãÆ----
// Should not flag Promise.allSettled
‚ãÆ----
// Sequential deletes in correct order should not flag Promise.all violations
‚ãÆ----
// This test verifies correct order doesn't flag Promise.all violations
// Order detection is a best-effort heuristic and may have edge cases
</file>

<file path="src/mcp/validation/rules/config-enforcement.test.ts">
/**
 * Unit tests for config-enforcement rule
 * CRITICAL: Detects configuration drift that weakens enforcement
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
</file>

<file path="src/mcp/validation/rules/doc-sync.ts">
/**
 * Documentation Sync Rule
 * HIGH: Validates that code patterns match documented expectations
 *
 * This rule helps detect when source code drifts from documentation:
 * - Entity file count in src/entities matches AGENTS.md project structure
 * - Lambda directories match trigger table
 * - MCP rule count matches registered rules
 *
 * Note: This is complementary to bin/validate-doc-sync.sh which performs
 * filesystem-level validation. This rule validates TypeScript files for
 * patterns that indicate documentation may be stale.
 *
 * @see docs/doc-code-mapping.json
 * @see Issue #145: Living Documentation System
 */
import type {SourceFile} from 'ts-morph'
import {SyntaxKind} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Expected counts from documentation (updated when docs change)
 * These are validated by bin/validate-doc-sync.sh against filesystem
 */
‚ãÆ----
// Entity files in src/entities (excluding index.ts and test files)
‚ãÆ----
// Lambda directories in src/lambdas
‚ãÆ----
// MCP validation rules in src/mcp/validation/rules (including this rule)
‚ãÆ----
/**
 * Required vendor paths that must exist (documented in AGENTS.md)
 */
‚ãÆ----
/**
 * Patterns that should not appear in source code (stale references)
 */
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// For TypeScript files, check for stale import patterns
‚ãÆ----
// For the MCP validation index, verify rule count matches
‚ãÆ----
/**
 * Validate TypeScript files for stale patterns
 */
function validateTypeScriptPatterns(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// filePath available for context if needed
‚ãÆ----
// Check for stale import patterns
‚ãÆ----
// Check for imports from non-existent vendor paths
‚ãÆ----
// Check if it's a vendor import
‚ãÆ----
// Validate the vendor path exists
‚ãÆ----
// Check if any required vendor path matches
‚ãÆ----
/**
 * Validate MCP rule count in index.ts
 */
function validateMcpRuleCount(sourceFile: SourceFile): Violation[]
‚ãÆ----
// Count rule exports in allRules array
‚ãÆ----
/**
 * Find line number for a regex pattern match
 */
function findLineNumber(text: string, pattern: RegExp): number
/**
 * Auto-fix suggestion generator
 * Returns shell commands to help diagnose and fix issues
 */
export function generateAutoFixSuggestions(violations: Violation[]): string[]
</file>

<file path="src/mcp/validation/rules/electrodb-mocking.ts">
/**
 * ElectroDB Mocking Rule
 * CRITICAL: Test files must use createElectroDBEntityMock() helper
 *
 * This ensures consistent mocking patterns and proper type safety.
 */
import type {SourceFile} from 'ts-morph'
import {SyntaxKind} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Entity names that should be mocked with the helper
 */
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// Only check test files
‚ãÆ----
// Skip the helper file itself
‚ãÆ----
// Check if file imports any entities
‚ãÆ----
// Check if file mocks entities
‚ãÆ----
return violations // No entity usage, rule doesn't apply
‚ãÆ----
// Check if createElectroDBEntityMock is imported
‚ãÆ----
// Check for manual entity mocking patterns
‚ãÆ----
// Check for jest.unstable_mockModule with entity paths
‚ãÆ----
// Check if mocking an entity
‚ãÆ----
// Check if the mock implementation uses createElectroDBEntityMock
‚ãÆ----
// If file mocks entities but doesn't import the helper
‚ãÆ----
// Check if they're using the mock helper correctly
</file>

<file path="src/mcp/validation/rules/env-validation.test.ts">
/**
 * Unit tests for env-validation rule
 * CRITICAL: No direct process.env access without getRequiredEnv() wrapper
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
// Module loaded via dynamic import
‚ãÆ----
// Create ts-morph project for in-memory source files
</file>

<file path="src/mcp/validation/rules/env-validation.ts">
/**
 * Environment Variable Validation Rule
 * CRITICAL: Detect direct process.env access without getRequiredEnv() wrapper
 *
 * This enforces the convention that all environment variable access should use
 * the validated helpers from util/env-validation.ts to ensure proper error
 * handling and avoid silent failures from missing environment variables.
 */
import type {SourceFile} from 'ts-morph'
import {SyntaxKind} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Allowed helper functions for environment variable access
 */
‚ãÆ----
/**
 * Known environment variables that might be accessed directly
 * (used for better error messages)
 */
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// Skip the env-validation utility itself
‚ãÆ----
// Find all property access expressions
‚ãÆ----
// Check for process.env.X pattern
‚ãÆ----
// This is process.env.SOMETHING
‚ãÆ----
// Check if it's inside a getRequiredEnv call (allowed)
‚ãÆ----
// Also check for process.env['X'] pattern (element access)
‚ãÆ----
// Also check element access expressions for process.env['X']
</file>

<file path="src/mcp/validation/rules/import-order.test.ts">
/**
 * Unit tests for import-order rule
 * MEDIUM: Imports should be grouped and ordered consistently
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
// Module loaded via dynamic import
‚ãÆ----
// Create ts-morph project for in-memory source files
‚ãÆ----
// Should detect both out-of-order and non-grouped
</file>

<file path="src/mcp/validation/rules/import-order.ts">
/**
 * Import Order Rule
 * MEDIUM: Imports should be grouped and ordered consistently
 *
 * Order: aws-lambda types ‚Üí entities ‚Üí vendor ‚Üí types ‚Üí utilities
 */
import type {SourceFile} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Import categories in expected order
 */
‚ãÆ----
function categorizeImport(moduleSpecifier: string): string
function getCategoryIndex(category: string): number
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// Only check Lambda handler files
‚ãÆ----
return violations // Not enough imports to check order
‚ãÆ----
// Analyze import order
‚ãÆ----
// Check if this category comes before a previously seen category
‚ãÆ----
// Check for mixed categories (same category appearing non-consecutively)
</file>

<file path="src/mcp/validation/rules/mock-formatting.test.ts">
/**
 * Unit tests for mock-formatting rule
 * MEDIUM: Enforce separate statements for mock return values
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
</file>

<file path="src/mcp/validation/rules/response-helpers.test.ts">
/**
 * Unit tests for response-helpers rule
 * HIGH: Lambda handlers must use response() helper, not raw objects
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
// Module loaded via dynamic import
‚ãÆ----
// Create ts-morph project for in-memory source files
‚ãÆ----
// May have 2 violations: raw response + missing import warning
‚ãÆ----
// May have 2 violations: raw response + missing import warning
</file>

<file path="src/mcp/validation/rules/scan-pagination.test.ts">
/**
 * Unit tests for scan-pagination rule
 * HIGH: Enforce pagination handling for scan operations
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
‚ãÆ----
// The rule looks for .scan property access followed by .go()
// This test verifies the rule can identify scan patterns
‚ãÆ----
// Rule may not catch all patterns - this is a best-effort heuristic
// The important thing is it doesn't false-positive on valid patterns
</file>

<file path="src/mcp/validation/rules/types-location.test.ts">
/**
 * Unit tests for types-location rule
 * HIGH: Exported type definitions should be in src/types/
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {Project} from 'ts-morph'
// Module loaded via dynamic import
‚ãÆ----
// Create ts-morph project for in-memory source files
</file>

<file path="src/mcp/README.md">
# Model Context Protocol (MCP) Server

This MCP server provides queryable interfaces for the Media Downloader codebase, allowing Claude and other AI assistants to understand the project structure without reading files.

## Installation

```bash
# Install dependencies
pnpm install

# Make the server executable
chmod +x src/mcp/server.ts
```

## Configuration

### Claude Code (CLI)

Add this to your project's `.claude/settings.local.json`:

```json
{
  "mcpServers": {
    "media-downloader": {
      "command": "node",
      "args": ["--loader", "ts-node/esm", "src/mcp/server.ts"]
    }
  }
}
```

### Claude Desktop

Add this to your Claude Desktop configuration (`~/Library/Application Support/Claude/claude_desktop_config.json`):

```json
{
  "mcpServers": {
    "media-downloader": {
      "command": "node",
      "args": ["--loader", "ts-node/esm", "/path/to/project/src/mcp/server.ts"]
    }
  }
}
```

## Available Tools

### 1. query_entities
Query ElectroDB entity schemas and relationships.

```typescript
// Examples:
query_entities({ query: "schema" })                    // Get all schemas
query_entities({ entity: "Users", query: "schema" })   // Get Users schema
query_entities({ query: "relationships" })             // Get all relationships
query_entities({ query: "collections" })               // Get collection queries
```

### 2. query_lambda
Query Lambda function configurations and dependencies.

```typescript
// Examples:
query_lambda({ query: "list" })                        // List all Lambda functions
query_lambda({ lambda: "ListFiles", query: "config" }) // Get ListFiles config
query_lambda({ query: "triggers" })                    // Get all Lambda triggers
query_lambda({ query: "dependencies" })                // Get dependency summary
query_lambda({ lambda: "ListFiles", query: "env" })    // Get env variables
```

### 3. query_infrastructure
Query AWS infrastructure configuration.

```typescript
// Examples:
query_infrastructure({ resource: "dynamodb", query: "config" })     // DynamoDB config
query_infrastructure({ resource: "s3", query: "usage" })            // S3 usage patterns
query_infrastructure({ resource: "all", query: "dependencies" })    // All dependencies
query_infrastructure({ resource: "apigateway", query: "config" })   // API Gateway config
```

### 4. query_dependencies
Query code dependencies from graph.json.

```typescript
// Examples:
query_dependencies({ query: "circular" })              // Find circular dependencies
query_dependencies({
  file: "src/lambdas/ListFiles/src/index.ts",
  query: "imports"
})                                                      // Get file imports
query_dependencies({
  file: "src/entities/Users.ts",
  query: "dependents"
})                                                      // Find who imports this file
query_dependencies({
  file: "src/lambdas/ListFiles/src/index.ts",
  query: "transitive"
})                                                      // Get all transitive dependencies
```

### 5. query_conventions
Search project conventions and wiki documentation.

```typescript
// Examples:
query_conventions({ query: "list" })                    // List all conventions by severity
query_conventions({ query: "search", term: "mock" })    // Search conventions and wiki
query_conventions({ query: "category", category: "testing" }) // Filter by category
query_conventions({ query: "enforcement", severity: "CRITICAL" }) // Get critical rules
query_conventions({ query: "detail", convention: "AWS SDK" }) // Get full convention details
query_conventions({ query: "wiki" })                    // List all wiki pages
```

### 6. validate_pattern
Validate code against project conventions using AST analysis.

```typescript
// Examples:
validate_pattern({ query: "rules" })                    // List all validation rules
validate_pattern({ file: "src/lambdas/ListFiles/src/index.ts", query: "all" }) // Full validation
validate_pattern({ file: "src/lambdas/ListFiles/src/index.ts", query: "aws-sdk" }) // Check SDK encapsulation
validate_pattern({ file: "src/lambdas/ListFiles/test/index.test.ts", query: "electrodb" }) // Check ElectroDB mocking
validate_pattern({ file: "src/lambdas/ListFiles/src/index.ts", query: "summary" }) // Concise summary
```

### 7. check_coverage
Analyze which dependencies need mocking for Jest tests.

```typescript
// Examples:
check_coverage({ file: "src/lambdas/ListFiles/src/index.ts", query: "required" }) // List mocks needed
check_coverage({ file: "src/lambdas/ListFiles/src/index.ts", query: "missing" })  // Compare to existing test
check_coverage({ file: "src/lambdas/ListFiles/src/index.ts", query: "all" })      // Full analysis
check_coverage({ file: "src/lambdas/ListFiles/src/index.ts", query: "summary" })  // Quick summary
```

### 8. lambda_impact
Show what's affected by changing a file.

```typescript
// Examples:
lambda_impact({ file: "src/entities/Files.ts", query: "dependents" })  // Direct dependents
lambda_impact({ file: "src/entities/Files.ts", query: "cascade" })     // Full cascade
lambda_impact({ file: "src/entities/Files.ts", query: "tests" })       // Tests to update
lambda_impact({ file: "src/entities/Files.ts", query: "infrastructure" }) // Terraform files
lambda_impact({ file: "src/util/lambda-helpers.ts", query: "all" })    // Comprehensive analysis
```

### 9. suggest_tests
Generate test file scaffolding with all required mocks.

```typescript
// Examples:
suggest_tests({ file: "src/lambdas/ListFiles/src/index.ts", query: "scaffold" }) // Complete test file
suggest_tests({ file: "src/lambdas/ListFiles/src/index.ts", query: "mocks" })    // Just mock setup
suggest_tests({ file: "src/lambdas/ListFiles/src/index.ts", query: "fixtures" }) // Suggested fixtures
suggest_tests({ file: "src/lambdas/ListFiles/src/index.ts", query: "structure" }) // Test structure outline
```

## Usage Examples

### Understanding Entity Relationships
```
Q: "What entities are in the database?"
Use: query_entities({ query: "schema" })

Q: "How are Users and Files related?"
Use: query_entities({ query: "relationships" })

Q: "How do I query all files for a user?"
Use: query_entities({ query: "collections" })
```

### Lambda Function Analysis
```
Q: "Which Lambda functions use DynamoDB?"
Use: query_lambda({ query: "dependencies" })

Q: "What triggers the FileCoordinator Lambda?"
Use: query_lambda({ lambda: "FileCoordinator", query: "triggers" })

Q: "What environment variables does LoginUser need?"
Use: query_lambda({ lambda: "LoginUser", query: "env" })
```

### Infrastructure Queries
```
Q: "What's the DynamoDB table structure?"
Use: query_infrastructure({ resource: "dynamodb", query: "config" })

Q: "Which Lambdas use S3?"
Use: query_infrastructure({ resource: "s3", query: "usage" })

Q: "What are the API endpoints?"
Use: query_infrastructure({ resource: "apigateway", query: "config" })
```

### Dependency Analysis
```
Q: "What files does ListFiles Lambda depend on?"
Use: query_dependencies({
  file: "src/lambdas/ListFiles/src/index.ts",
  query: "transitive"
})

Q: "Which files import the Users entity?"
Use: query_dependencies({
  file: "src/entities/Users.ts",
  query: "dependents"
})

Q: "Are there any circular dependencies?"
Use: query_dependencies({ query: "circular" })
```

## Development

### Running Locally

```bash
# Run the server directly
node --loader ts-node/esm src/mcp/server.ts

# Or compile and run
pnpm run build
node dist/mcp/server.js
```

### Testing with MCP Inspector

```bash
npx @modelcontextprotocol/inspector src/mcp/server.ts
```

### Extending the Server

To add new query capabilities:

1. Add tool definition in `server.ts` `ListToolsRequestSchema` handler
2. Add case in `CallToolRequestSchema` handler
3. Create handler in `src/mcp/handlers/`
4. Update this README with examples

## Architecture Benefits

- **No File Reading**: AI can understand structure without consuming context on file contents
- **Consistent Responses**: Structured data instead of parsing code
- **Performance**: Fast queries vs. reading multiple files
- **Maintainable**: Single source of truth for architecture information
- **Extensible**: Easy to add new query types as needed

## Integration with Claude

When using Claude with this MCP server, you can ask questions like:

- "Query the MCP server to show me all Lambda functions and their triggers"
- "Use MCP to find which entities have relationships with Users"
- "Query infrastructure to understand the DynamoDB access patterns"
- "Check for circular dependencies in the codebase"

Claude will automatically use the appropriate MCP tool to get structured information about the codebase.
</file>

<file path="src/templates/github-issues/cookie-expiration.md">
## YouTube Cookie Expiration

YouTube has detected the cookies as expired or is blocking requests with bot detection.

**Triggered By**:
- **File ID**: ${fileId}
- **Video URL**: ${fileUrl}
- **Error Message**: ${error.message}
- **Timestamp**: ${new Date().toISOString()}

---

## Required Action: Refresh YouTube Cookies

### Step 1: Extract New Cookies
\`\`\`bash
# Ensure you're logged into YouTube in Chrome
# Then run the cookie update script
pnpm run update-cookies
\`\`\`

This will:
1. Extract cookies from Chrome browser
2. Filter to YouTube/Google domains only
3. Copy filtered cookies to Lambda layer directory

### Step 2: Build and Deploy
\`\`\`bash
pnpm run build
pnpm run deploy
\`\`\`

### Step 3: Verify Fix
\`\`\`bash
# Trigger a test download
/opt/homebrew/bin/aws lambda invoke \\
  --function-name FileCoordinator \\
  --region us-west-2 \\
  --payload '{}' \\
  /dev/null

# Monitor logs for authentication success
/opt/homebrew/bin/aws logs tail /aws/lambda/StartFileUpload \\
  --region us-west-2 \\
  --follow \\
  --format short
\`\`\`

---

## Background

YouTube cookies typically last 30-60 days. This error indicates:
- Cookies have expired naturally
- YouTube has invalidated the session
- Bot detection triggered (datacenter IP + stale cookies)

**Cookie Location**: \`layers/yt-dlp/cookies/youtube-cookies.txt\` (18KB, filtered)

**Documentation**: See \`docs/YT-DLP-MIGRATION-STRATEGY.md\` Phase 2 for details.

---

## Stack Trace

\`\`\`
${error.stack || 'No stack trace available'}
\`\`\`

---

This issue was automatically created by the cookie monitoring system.
</file>

<file path="src/types/vendor/IFTTT/Feedly/Webhook.d.ts">
export interface Webhook {
  readonly articleFirstImageURL: string
  readonly articlePublishedAt: string
  readonly articleTitle: string
  readonly articleURL: string
  backgroundMode?: boolean
}
</file>

<file path="src/util/test/env-validation.test.ts">
import {afterEach, beforeEach, describe, expect, it} from '@jest/globals'
import {getOptionalEnv, getRequiredEnv, getRequiredEnvNumber, MissingEnvVarError} from '../env-validation'
</file>

<file path="src/util/test/pagination.test.ts">
import {beforeEach, describe, expect, it, jest} from '@jest/globals'
import {scanAllPages} from '../pagination'
type ScanFn<T> = (cursor?: string) => Promise<{data: T[]; cursor: string | null}>
‚ãÆ----
interface Device {
      deviceId: string
      token: string
    }
</file>

<file path="src/util/test/retry.test.ts">
import {beforeEach, describe, expect, it, jest} from '@jest/globals'
import {retryUnprocessed, retryUnprocessedDelete} from '../retry'
type RetryOpFn = () => Promise<{data: string[]; unprocessed: unknown[]}>
type DeleteOpFn = () => Promise<{unprocessed: unknown[]}>
‚ãÆ----
expect(mockOperation).toHaveBeenCalledTimes(3) // Initial + 2 retries
</file>

<file path="src/util/constants-runtime.ts">
/**
 * Runtime Constants
 *
 * Centralized configuration values used across Lambda functions.
 * These are runtime values, not build-time constants.
 */
/** Default retry configuration for DynamoDB batch operations */
‚ãÆ----
/** Batch processing configuration */
‚ãÆ----
/** Maximum items per batch for DynamoDB operations */
‚ãÆ----
/** Delay between batches in milliseconds */
‚ãÆ----
/** Session expiration times */
‚ãÆ----
/** Session token expiration (30 days in milliseconds) */
‚ãÆ----
/** Push notification configuration */
‚ãÆ----
/** Maximum description length for APNS payload limits */
‚ãÆ----
/** HTTP status codes for consistent API responses */
</file>

<file path="src/util/env-validation.ts">
/**
 * Environment variable validation utilities
 * Provides fail-fast validation at Lambda cold start
 */
export class MissingEnvVarError extends Error
‚ãÆ----
constructor(name: string)
‚ãÆ----
/**
 * Gets a required environment variable or throws an error
 * Use this at cold start to fail fast if configuration is missing
 * @param name - Environment variable name
 * @returns The environment variable value
 * @throws MissingEnvVarError if the variable is not set
 */
export function getRequiredEnv(name: string): string
/**
 * Gets an optional environment variable with a default value
 * @param name - Environment variable name
 * @param defaultValue - Default value if not set
 * @returns The environment variable value or default
 */
export function getOptionalEnv(name: string, defaultValue: string): string
/**
 * Gets a required numeric environment variable
 * @param name - Environment variable name
 * @returns The parsed number
 * @throws MissingEnvVarError if the variable is not set
 * @throws Error if the value is not a valid number
 */
export function getRequiredEnvNumber(name: string): number
/**
 * Gets an optional numeric environment variable with a default value
 * @param name - Environment variable name
 * @param defaultValue - Default value if not set or invalid
 * @returns The parsed number or default value
 */
export function getOptionalEnvNumber(name: string, defaultValue: number): number
</file>

<file path="src/util/jest-setup.ts">
import {afterAll, beforeAll, jest} from '@jest/globals'
‚ãÆ----
// Randomly generated key; not actually used anywhere (safe)
// openssl ecparam -name prime256v1 -genkey -noout -out private.ec.key
// openssl ec -in private.ec.key -pubout -out public.ec.key
</file>

<file path="src/util/logging.ts">
/**
 * Logging utilities powered by AWS Lambda Powertools
 * Provides structured JSON logging with automatic context enrichment
 *
 * Backwards-compatible with existing logInfo/logDebug/logError calls
 */
import {logger} from '#lib/vendor/Powertools'
/**
 * Log an informational message with optional structured data
 * @param message - The log message
 * @param data - Optional structured data or string to include
 */
export function logInfo(message: string, data?: string | object): void
/**
 * Log a debug message with optional structured data
 * @param message - The log message
 * @param data - Optional structured data or string to include
 */
export function logDebug(message: string, data?: string | object): void
/**
 * Log an error message with optional structured data or Error object
 * @param message - The log message
 * @param data - Optional structured data, string, or Error object
 */
export function logError(message: string, data?: string | object | Error | unknown): void
// Re-export logger for direct access when needed
</file>

<file path="src/util/pagination.ts">
import {logDebug} from './logging'
/**
 * Scans all pages of a DynamoDB table using ElectroDB pagination
 * Handles the cursor-based pagination automatically
 * @param scanFn - Function that performs a scan with optional cursor
 * @returns All items from all pages combined
 */
export async function scanAllPages<T>(scanFn: (cursor?: string) => Promise<
</file>

<file path="src/util/template-helpers.ts">
import {readFileSync} from 'fs'
import {dirname, join} from 'path'
import {fileURLToPath} from 'url'
// ES module equivalent of __dirname
‚ãÆ----
/**
 * Render a GitHub issue template with variable interpolation
 * Uses native JavaScript template literals for full expression support
 * @param templateName - Name of template file (without .md extension)
 * @param data - Variables to inject into the template
 * @returns Rendered template string
 */
export function renderGithubIssueTemplate(templateName: string, data: Record<string, unknown>): string
‚ãÆ----
// Use Function constructor for safe template evaluation
// This allows full JavaScript expressions in templates
</file>

<file path="src/util/video-error-classifier.ts">
import {CookieExpirationError} from './errors'
import type {SchedulingVideoInfo, VideoErrorCategory, VideoErrorClassification} from '#types/video'
/** Patterns indicating permanent failures that should not be retried */
‚ãÆ----
/** Patterns indicating transient/network errors that should be retried with backoff */
‚ãÆ----
/** Patterns indicating scheduled content (used as hints alongside release_timestamp) */
‚ãÆ----
/** Default retry buffer in seconds (added to release_timestamp) */
const RETRY_BUFFER_SECONDS = 300 // 5 minutes
/** Default max retries for different categories */
‚ãÆ----
scheduled: 3, // Scheduled videos should succeed quickly after release
livestream_upcoming: 10, // Livestreams may have variable start times
transient: 5, // Standard exponential backoff
cookie_expired: 0, // Requires manual intervention
permanent: 0 // Never retry
‚ãÆ----
/** Exponential backoff delays in seconds */
‚ãÆ----
15 * 60, // 15 minutes
30 * 60, // 30 minutes
60 * 60, // 1 hour
2 * 60 * 60, // 2 hours
4 * 60 * 60 // 4 hours
‚ãÆ----
/**
 * Calculate exponential backoff delay for retries
 * @param retryCount - Number of previous retry attempts (0-indexed)
 * @param baseDelaySeconds - Base delay in seconds (default: 15 minutes)
 * @param maxDelaySeconds - Maximum delay cap in seconds (default: 4 hours)
 * @returns Unix timestamp (seconds) for when to retry
 */
export function calculateExponentialBackoff(retryCount: number, baseDelaySeconds = 15 * 60, maxDelaySeconds = 4 * 60 * 60): number
‚ãÆ----
// Use predefined delays if available, otherwise calculate exponentially
‚ãÆ----
// Exponential backoff: base * 2^retryCount, capped at max
‚ãÆ----
/**
 * Check if error message matches any pattern in a list (case-insensitive)
 */
function matchesPattern(message: string, patterns: string[]): boolean
/**
 * Check if the video has a future release timestamp indicating scheduled content
 */
function isScheduledContent(videoInfo?: SchedulingVideoInfo): boolean
/**
 * Check if the video is an upcoming livestream
 */
function isUpcomingLivestream(videoInfo?: SchedulingVideoInfo): boolean
/**
 * Classify a video download error to determine retry strategy
 *
 * Classification priority:
 * 1. Cookie expiration (requires manual intervention)
 * 2. Scheduled content (retry at release_timestamp)
 * 3. Upcoming livestream (retry at release or with backoff)
 * 4. Transient errors (exponential backoff)
 * 5. Permanent failures (no retry)
 *
 * @param error - The error that occurred during download
 * @param videoInfo - Optional video info that may contain scheduling metadata
 * @param retryCount - Current retry count (for backoff calculation)
 * @returns Classification with retry strategy
 */
export function classifyVideoError(error: Error, videoInfo?: SchedulingVideoInfo, retryCount = 0): VideoErrorClassification
‚ãÆ----
// Priority 1: Cookie expiration (requires manual intervention)
‚ãÆ----
// Priority 2: Upcoming livestream (check before scheduled to handle livestreams with release_timestamp)
‚ãÆ----
// Priority 3: Scheduled content with known release time
‚ãÆ----
// Priority 4: Check error message for scheduled content hints
// (when videoInfo doesn't have release_timestamp but error suggests scheduling)
‚ãÆ----
// Priority 5: Transient/network errors
‚ãÆ----
// Priority 6: Permanent failures
‚ãÆ----
// Default: Treat unknown errors as transient (give benefit of the doubt)
// This allows recovery from unexpected temporary issues
‚ãÆ----
/**
 * Check if a file should be permanently failed based on retry exhaustion
 * @param retryCount - Current retry count
 * @param maxRetries - Maximum allowed retries
 * @returns true if retries are exhausted
 */
export function isRetryExhausted(retryCount: number, maxRetries: number): boolean
</file>

<file path="test/integration/helpers/lambda-context.ts">
/**
 * Lambda Context Mock Helper
 *
 * Creates mock AWS Lambda Context objects for integration tests
 */
import type {Context} from 'aws-lambda'
/**
 * Create a mock Lambda context for testing
 */
export function createMockContext(overrides?: Partial<Context>): Context
‚ãÆ----
getRemainingTimeInMillis: () => 30000, // 30 seconds remaining
</file>

<file path="tsp/models/models.tsp">
import "@typespec/http";

using TypeSpec.Http;

namespace OfflineMediaDownloader.Models;

/**
 * File status enumeration
 */
enum FileStatus {
  /**
   * File is queued for download
   */
  Queued: "Queued",

  /**
   * File is currently being downloaded
   */
  Downloading: "Downloading",

  /**
   * File has been successfully downloaded
   */
  Downloaded: "Downloaded",

  /**
   * File download failed
   */
  Failed: "Failed",
}

/**
 * A media file available for download
 */
model File {
  /**
   * The unique file identifier (typically a YouTube video ID)
   * @example "4TfEp8oG5gM"
   */
  fileId: string;

  /**
   * The filename or key in S3 storage
   */
  key?: string;

  /**
   * Size in bytes of the file
   */
  size?: int64;

  /**
   * Current status of the file
   */
  status?: FileStatus;

  /**
   * Title of the media file
   */
  title?: string;

  /**
   * Video publish date
   */
  publishDate?: string;

  /**
   * Channel/author display name
   */
  authorName?: string;

  /**
   * Channel/author username or ID
   */
  authorUser?: string;

  /**
   * MIME type (e.g., video/mp4)
   */
  contentType?: string;

  /**
   * Video description
   */
  description?: string;

  /**
   * CloudFront URL for downloading the file
   */
  url?: url;
}

/**
 * Response containing a list of files
 */
model FileListResponse {
  /**
   * Array of available files
   */
  contents: File[];

  /**
   * Number of files returned
   */
  keyCount: int32;
}

/**
 * Device information for push notifications
 */
model Device {
  /**
   * Unique device identifier (UUID)
   * @example "C49BA68B-E21A-4AEE-8D22-D99A2689B56A"
   */
  deviceId: string;

  /**
   * Device name
   * @example "Programmer's iPhone"
   */
  name: string;

  /**
   * Operating system name
   * @example "iOS"
   */
  systemName: string;

  /**
   * Operating system version
   * @example "14.4"
   */
  systemVersion: string;

  /**
   * Device push notification token
   */
  token: string;

  /**
   * AWS SNS endpoint ARN (returned after registration)
   */
  endpointArn?: string;
}

/**
 * Device registration request
 */
model DeviceRegistrationRequest {
  /**
   * Unique device identifier (UUID)
   * @example "C49BA68B-E21A-4AEE-8D22-D99A2689B56A"
   */
  deviceId: string;

  /**
   * Device name
   * @example "Programmer's iPhone"
   */
  name: string;

  /**
   * Operating system name
   * @example "iOS"
   */
  systemName: string;

  /**
   * Operating system version
   * @example "14.4"
   */
  systemVersion: string;

  /**
   * Device push notification token (APNS token)
   */
  token: string;
}

/**
 * Device registration response
 */
model DeviceRegistrationResponse {
  /**
   * AWS SNS endpoint ARN for the registered device
   */
  endpointArn: string;
}

/**
 * Feedly webhook event
 */
model FeedlyWebhook {
  /**
   * URL of the first image in the article
   * @example "https://i.ytimg.com/vi/7jEzw5WLiMI/maxresdefault.jpg"
   */
  articleFirstImageURL?: url;

  /**
   * Categories associated with the article
   * @example "YouTube"
   */
  articleCategories?: string;

  /**
   * Publication date of the article
   * @example "April 27, 2020 at 04:10PM"
   */
  articlePublishedAt?: string;

  /**
   * Title of the article
   */
  articleTitle: string;

  /**
   * URL of the article (typically a YouTube video URL)
   * @example "https://www.youtube.com/watch?v=wRG7lAGdRII"
   */
  articleURL: url;

  /**
   * Timestamp when the webhook was created
   * @example "April 27, 2020 at 04:10PM"
   */
  createdAt?: string;

  /**
   * URL of the source feed
   * @example "https://www.youtube.com/playlist?list=UUlFSU9_bUb4Rc6OYfTt5SPw"
   */
  sourceFeedURL?: url;

  /**
   * Title of the source
   * @example "Philip DeFranco (uploads) on YouTube"
   */
  sourceTitle?: string;

  /**
   * URL of the source
   * @example "https://youtube.com/playlist?list=UUlFSU9_bUb4Rc6OYfTt5SPw"
   */
  sourceURL?: url;

  /**
   * Whether the webhook was triggered in background mode
   */
  backgroundMode?: boolean;
}

/**
 * Webhook processing response
 */
model WebhookResponse {
  /**
   * Status of the webhook processing
   */
  status: "Dispatched" | "Initiated" | "Accepted";
}

/**
 * User login request
 */
model UserLogin {
  /**
   * Authorization code from Sign in with Apple
   */
  authorizationCode: string;
}

/**
 * User login response
 */
model UserLoginResponse {
  /**
   * JWT access token for authenticated requests
   */
  token: string;
}

/**
 * User registration request
 */
model UserRegistration {
  /**
   * Authorization code from Sign in with Apple
   */
  authorizationCode: string;

  /**
   * User's first name
   */
  firstName?: string;

  /**
   * User's last name
   */
  lastName?: string;
}

/**
 * User registration response
 */
model UserRegistrationResponse {
  /**
   * JWT access token for authenticated requests
   */
  token: string;
}
</file>

<file path=".dependency-cruiser.cjs">
/**
 * Dependency Cruiser configuration for Media Downloader
 * Enforces architectural rules and prevents unwanted dependencies
 */
‚ãÆ----
/* Rules that should never be broken */
‚ãÆ----
/* Rules that explicitly allow certain patterns */
‚ãÆ----
/* Configuration options */
</file>

<file path=".mcp.json">
{
  "mcpServers": {
    "media-downloader": {
      "type": "stdio",
      "command": "npx",
      "args": ["tsx", "src/mcp/server.ts"]
    }
  }
}
</file>

<file path=".npmrc">
# ============================================================================
# PNPM-SPECIFIC CONFIGURATION
# ============================================================================
# This project uses pnpm exclusively (enforced via packageManager in package.json)
# Settings below are pnpm-specific - npm warnings about "unknown project config"
# can be safely ignored if npm is accidentally invoked
#
# ============================================================================
# SECURITY HARDENING: Lifecycle Script Protection
# ============================================================================
# Disable all lifecycle scripts by default (pnpm v10+ feature)
# This prevents malicious packages from executing code during installation
# Protects against: Typosquatting, supply chain attacks, AI-hallucinated packages
enable-pre-post-scripts=false

# Explicitly allowlist packages requiring scripts (AUDIT BEFORE ADDING)
# Expected: NONE for this project (all pure JS/TS dependencies)
# If needed in future:
# pnpm.onlyBuiltDependencies[]=package-name

# ============================================================================
# PERFORMANCE OPTIMIZATION
# ============================================================================
# Use hard links (faster, disk-efficient)
package-import-method=hardlink

# Strict peer dependencies (catch compatibility issues)
strict-peer-dependencies=false

# Hoist pattern (compatibility with some tools)
shamefully-hoist=false

# ============================================================================
# FUTURE: WORKSPACE CONFIGURATION
# ============================================================================
# Enable when migrating to monorepo architecture
# recursive-install=false
# link-workspace-packages=true
</file>

<file path="dprint.json">
{
  "$schema": "https://dprint.dev/schemas/v0.json",

  "lineWidth": 157,
  "indentWidth": 2,
  "useTabs": false,
  "newLineKind": "lf",

  "typescript": {
    "quoteStyle": "preferSingle",
    "jsx.quoteStyle": "preferSingle",
    "quoteProps": "asNeeded",

    "semiColons": "asi",

    "trailingCommas": "never",

    "bracePosition": "sameLine",
    "nextControlFlowPosition": "sameLine",
    "singleBodyPosition": "nextLine",
    "useBraces": "whenNotSingleLine",

    "operatorPosition": "sameLine",
    "binaryExpression.operatorPosition": "sameLine",
    "binaryExpression.preferSingleLine": true,
    "conditionalExpression.operatorPosition": "nextLine",

    "arrowFunction.useParentheses": "force",

    "objectExpression.preferSingleLine": true,
    "objectExpression.trailingCommas": "never",

    "arrayExpression.preferSingleLine": false,
    "arrayExpression.trailingCommas": "never",

    "arguments.preferSingleLine": true,
    "arguments.preferHanging": "always",
    "arguments.trailingCommas": "never",
    "parameters.preferSingleLine": true,
    "parameters.trailingCommas": "never",

    "preferHanging": false,

    "memberExpression.linePerExpression": false,
    "memberExpression.preferSingleLine": true,

    "importDeclaration.forceMultiLine": "never",
    "importDeclaration.sortNamedImports": "caseInsensitive",
    "importDeclaration.spaceSurroundingNamedImports": false,
    "module.sortImportDeclarations": "maintain",
    "module.sortExportDeclarations": "maintain",

    "typeLiteral.separatorKind": "semiColon",
    "typeLiteral.preferSingleLine": true,
    "typeAnnotation.spaceBeforeColon": false,

    "spaceSurroundingProperties": false,
    "binaryExpression.spaceSurroundingBitwiseAndArithmeticOperator": true,

    "ifStatement.useBraces": "whenNotSingleLine",
    "whileStatement.bracePosition": "sameLine",
    "forStatement.useBraces": "whenNotSingleLine"
  },

  "json": {
    "indentWidth": 2,
    "trailingCommas": "never"
  },

  "includes": [
    "src/**/*.{ts,tsx,js,jsx}",
    "test/**/*.{ts,tsx,js,jsx}",
    "config/**/*.{ts,js,json}",
    "*.json",
    "*.mjs"
  ],

  "excludes": [
    "**/node_modules",
    "**/dist",
    "**/build",
    "**/coverage",
    "**/.git",
    "terraform/**",
    "src/mcp/test/fixtures/**/config-*.fixture.ts"
  ],

  "plugins": [
    "https://plugins.dprint.dev/typescript-0.93.4.wasm",
    "https://plugins.dprint.dev/json-0.19.4.wasm"
  ]
}
</file>

<file path=".github/agents/aws-media-downloader.agent.md">
---
name: aws-media-downloader
description: Expert agent for the AWS CloudFormation Media Downloader project with ElectroDB ORM, enforcing AWS SDK wrapper usage, commit rules, convention capture, and comprehensive test patterns.
tools: ['read', 'search', 'edit', 'git']
target: github-copilot
---

# AWS Media Downloader Agent

<!--
üîÑ SYNCHRONIZATION NOTICE:
This file must be kept in sync with:
- AGENTS.md (primary source of truth)
- CLAUDE.md (Claude AI configuration)
- GEMINI.md (Google Gemini configuration)

When updating project rules, conventions, or critical policies,
ensure all files are updated to maintain consistency across AI assistants.
-->

You are an expert in developing serverless AWS applications, specifically for the AWS CloudFormation Media Downloader project - a serverless media download service built with OpenTofu and TypeScript that integrates with iOS for offline playback.

## Convention Capture System (CRITICAL)

This project uses an automated system to capture emergent conventions during development:

### Monitor for Signals:
- üö® **CRITICAL**: "NEVER", "FORBIDDEN", "Zero-tolerance"
- ‚ö†Ô∏è **HIGH**: "MUST", "REQUIRED", "ALWAYS", corrections
- üìã **MEDIUM**: "Prefer X over Y", repeated decisions (2+ times)
- üí° **LOW**: Suggestions to monitor

### When Convention Detected:
Flag it with: "üîî CONVENTION DETECTED" and document the pattern

### Reference:
- **Tracking**: `docs/conventions-tracking.md` - Current conventions
- **Guide**: `docs/CONVENTION-CAPTURE-GUIDE.md` - Methodology
- **Templates**: `docs/templates/` - Convention documentation

## Project Context

This is a production AWS serverless application with:
- **Infrastructure**: OpenTofu (IaC)
- **Runtime**: AWS Lambda (Node.js 22.x) with TypeScript
- **Storage**: S3 for media files
- **Database**: DynamoDB with ElectroDB ORM (single-table design)
- **Testing**: Jest with LocalStack for integration tests
- **CI/CD**: GitHub Actions with automated testing

## Critical Rules (MUST FOLLOW)

### AWS SDK Encapsulation Policy (ZERO TOLERANCE)
- **NEVER** import AWS SDK packages directly (`@aws-sdk/*`)
- **ALWAYS** use vendor wrappers in `lib/vendor/AWS/*`
- Example: Use `import {createS3Upload} from '../../../lib/vendor/AWS/S3'` NOT `import {S3Client} from '@aws-sdk/client-s3'`

### Commit Message Rules (ABSOLUTE)
- **NEVER** include AI references in commits (no "Claude", "Generated", "Co-Authored-By", emojis)
- **ALWAYS** use commitlint syntax: `feat:`, `fix:`, `refactor:`, `test:`, `docs:`, `chore:`
- Keep messages professional and technical

### Code Style Requirements
- **ALWAYS** read wiki guides before writing code:
  - Lambda: `docs/wiki/TypeScript/Lambda-Function-Patterns.md`
  - Tests: `docs/wiki/Testing/Jest-ESM-Mocking-Strategy.md`
  - Bash: `docs/wiki/Bash/Script-Patterns.md`
  - OpenTofu: `docs/wiki/Infrastructure/OpenTofu-Patterns.md`
- Use camelCase for variables/functions/files
- Use PascalCase for TypeScript types/interfaces/classes
- **NEVER** explain removed code in comments - git history is the source of truth

### Testing Requirements
- Mock ALL transitive dependencies using `jest.unstable_mockModule`
- Mock vendor wrappers (`lib/vendor/AWS/*`), never `@aws-sdk/*` directly
- **ALWAYS** use `test/helpers/electrodb-mock.ts` for mocking ElectroDB entities
- Use specific type annotations for `jest.fn()` when using `mockResolvedValue`
- Run integration tests against LocalStack for AWS service changes

## Project Structure

```
src/
‚îú‚îÄ‚îÄ entities/              # ElectroDB entity definitions (single-table design)
‚îÇ   ‚îú‚îÄ‚îÄ Collections.ts     # Service combining entities for JOIN-like queries
‚îÇ   ‚îú‚îÄ‚îÄ Files.ts          # File entity
‚îÇ   ‚îú‚îÄ‚îÄ Users.ts          # User entity
‚îÇ   ‚îú‚îÄ‚îÄ Devices.ts        # Device entity
‚îÇ   ‚îú‚îÄ‚îÄ UserFiles.ts      # User-File relationships
‚îÇ   ‚îî‚îÄ‚îÄ UserDevices.ts    # User-Device relationships
‚îú‚îÄ‚îÄ lambdas/[name]/
‚îÇ   ‚îú‚îÄ‚îÄ src/index.ts      # Lambda handler with TypeDoc
‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ       ‚îú‚îÄ‚îÄ index.test.ts # Unit tests
‚îÇ       ‚îî‚îÄ‚îÄ fixtures/     # Test data
lib/vendor/
‚îú‚îÄ‚îÄ AWS/                  # AWS SDK wrappers (S3, DynamoDB, Lambda, etc.)
‚îî‚îÄ‚îÄ ElectroDB/           # ElectroDB configuration & service
test/
‚îú‚îÄ‚îÄ helpers/              # Test utilities
‚îÇ   ‚îî‚îÄ‚îÄ electrodb-mock.ts # ElectroDB mock helper
‚îî‚îÄ‚îÄ integration/          # LocalStack integration tests
util/                     # Shared utilities
terraform/                # OpenTofu infrastructure
build/graph.json         # Code dependency graph - READ THIS FIRST
```

## ElectroDB Architecture (CRITICAL)

This project uses ElectroDB as the DynamoDB ORM for type-safe database operations:

### Key Features
- **Single-table design**: All entities in one DynamoDB table
- **Type-safe queries**: Full TypeScript type inference
- **Collections**: JOIN-like queries across entities (see `src/entities/Collections.ts`)
- **Batch operations**: Efficient bulk reads/writes

### Entity Relationships
- **Users** ‚Üî **Files**: Many-to-many via UserFiles
- **Users** ‚Üî **Devices**: Many-to-many via UserDevices
- **Collections.userResources**: Query all files & devices for a user
- **Collections.fileUsers**: Get all users with a file (for notifications)

### Testing with ElectroDB
- **ALWAYS** use `test/helpers/electrodb-mock.ts` for mocking
- **NEVER** create manual mocks for ElectroDB entities
- See wiki testing guides for patterns

## Lambda Development Pattern

Every Lambda should follow this structure:

```typescript
import {validateInput} from '../../../util/constraints'
import {prepareLambdaResponse, logError} from '../../../util/lambda-helpers'
import {withXRay} from '../../../lib/vendor/AWS/XRay'

/**
 * [Description for TypeDoc]
 * @param event - AWS Lambda event
 * @param context - AWS Lambda context
 */
export const handler = withXRay(async (event, context, {traceId}) => {
  // Validate inputs
  const errors = validateInput(event, constraints)
  if (errors) {
    return prepareLambdaResponse({statusCode: 400, body: errors})
  }

  try {
    // Business logic using vendor wrappers

    return prepareLambdaResponse({statusCode: 200, body: result})
  } catch (error) {
    logError(error, {context: 'handler'})
    return prepareLambdaResponse({statusCode: 500, body: 'Internal error'})
  }
})
```

## Testing Pattern

Tests MUST mock all transitive dependencies:

```typescript
// 1. Mock ElectroDB entities FIRST using the helper
jest.unstable_mockModule('../../../lib/vendor/ElectroDB/entity', () =>
  createElectroDBMock({
    // Mock entity methods as needed
    get: jest.fn().mockResolvedValue({ data: mockUser }),
    query: jest.fn().mockResolvedValue({ data: [mockUser] })
  })
)

// 2. Mock vendor wrappers (never @aws-sdk/* directly)
jest.unstable_mockModule('../../../lib/vendor/AWS/S3', () => ({
  headObject: jest.fn<() => Promise<{ContentLength: number}>>()
    .mockResolvedValue({ContentLength: 1024}),
  createS3Upload: jest.fn()
}))

// 3. Mock Node.js built-ins if needed
jest.unstable_mockModule('fs', () => ({
  promises: {
    copyFile: jest.fn<() => Promise<void>>()
  }
}))

// 4. THEN import the handler
const {handler} = await import('../src')
```

## Common Operations

**IMPORTANT**: Always read `build/graph.json` first to understand code relationships and dependencies.

### Adding AWS Service Integration
1. Create vendor wrapper in `lib/vendor/AWS/[Service].ts`
2. Add AWS SDK package to esbuild externals in `config/esbuild.config.ts`
3. Use X-Ray capture: `const client = captureAWSClient(new ServiceClient(...))`
4. Export simple functions, not AWS types

### Creating New Lambda
1. Create directory: `src/lambdas/[Name]/`
2. Implement handler with X-Ray decorator
3. Add TypeDoc comments
4. Create comprehensive tests with transitive mocking
5. Define infrastructure in `terraform/Lambda[Name].tf`
6. Add IAM permissions as needed

### Pre-Commit Checklist
```bash
pnpm run format        # Format with dprint
pnpm run build         # Build with esbuild
pnpm test              # Run test suite
git add -A
git commit -m "type: description"  # NO AI references!
```

## Key Utilities

- **API Gateway**: `util/apigateway-helpers.ts` - Request/response handling
- **Validation**: `util/constraints.ts` - Input validation with validate.js
- **Errors**: `util/errors.ts` - Consistent error types
- **Lambda**: `util/lambda-helpers.ts` - Response formatting, logging
- **Transformers**: `util/transformers.ts` - Data format conversions
- **Shared**: `util/shared.ts` - Cross-lambda functionality
- **ElectroDB**: `src/entities/` - Type-safe database operations (replaces old DynamoDB helpers)

## AWS Services Used

- **Lambda**: Serverless compute (15+ functions)
- **S3**: Media file storage with transfer acceleration
- **DynamoDB**: Single-table design via ElectroDB ORM (all entities)
- **SNS**: Push notifications to iOS devices
- **API Gateway**: REST endpoints with custom authorizer (query-based for Feedly)
- **CloudWatch**: Logging and metrics
- **X-Ray**: Distributed tracing (optional)

## Integration Points

- **Feedly**: Webhook triggers for media downloads (query-based auth in custom authorizer)
- **iOS App**: Companion app for offline playback (SwiftUI/TCA architecture)
- **YouTube**: Video downloads via yt-dlp (cookie authentication required due to bot detection)
- **GitHub**: Automated issue creation for production errors
- **LocalStack**: Local AWS testing environment (via vendor wrappers)
- **Sign In With Apple**: Authentication for iOS app users
- **APNS**: Push notifications (requires p12 certificates)

## Performance Optimizations

- Lambda memory tuned per workload
- S3 multipart upload for files >5MB
- API Gateway caching where appropriate
- X-Ray tracing to identify bottlenecks

## Security Requirements

- Never log sensitive data (tokens, keys, PII)
- Use AWS Secrets Manager for credentials
- Validate all inputs with validate.js
- Custom authorizer for API protection
- Sanitize data before DynamoDB operations

## When Working on Issues

1. **Understand existing patterns** - Check similar implementations
2. **Follow vendor wrapper pattern** - Never import AWS SDK directly
3. **Write comprehensive tests** - Mock ALL dependencies
4. **Use LocalStack for integration** - Test AWS interactions
5. **Format and test before commit** - Run npm commands
6. **Keep commits clean** - No AI attributions

## Common Pitfalls to Avoid

- ‚ùå Importing `@aws-sdk/*` directly
- ‚ùå Including AI references in commits
- ‚ùå Creating new files unnecessarily (prefer editing existing)
- ‚ùå Missing transitive dependency mocks in tests
- ‚ùå Creating manual mocks for ElectroDB entities (use `test/helpers/electrodb-mock.ts`)
- ‚ùå Explaining removed code in comments
- ‚ùå Using wrong naming convention (camelCase vs PascalCase)
- ‚ùå Forgetting to update esbuild externals for new AWS SDKs
- ‚ùå Not running format/build/test before committing
- ‚ùå Not reading `build/graph.json` before making changes

## Files to Reference

When working on this project, always consult:
- `build/graph.json` - Code dependency graph (READ THIS FIRST)
- `AGENTS.md` - Primary project documentation
- `docs/wiki/` - All style guides and patterns (MUST READ applicable guides)
- `docs/conventions-tracking.md` - Project-specific conventions
- `src/entities/` - ElectroDB entity definitions
- `test/helpers/electrodb-mock.ts` - ElectroDB testing patterns
- `package.json` - Dependencies and scripts
- `config/esbuild.config.ts` - Build configuration
- `test/integration/README.md` - Integration testing guide
</file>

<file path=".github/workflows/sync-wiki.yml">
name: Sync Wiki
on:
  push:
    branches:
      - master
      - main
    paths:
      - 'docs/wiki/**'
      - '.github/scripts/sync-wiki.sh'
      - '.github/scripts/generate-sidebar.sh'
      - '.github/workflows/sync-wiki.yml'
  workflow_dispatch:  # Allow manual trigger
permissions:
  contents: write
jobs:
  sync-wiki:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout main repository
        uses: actions/checkout@v4
        with:
          path: main
      - name: Checkout wiki repository
        uses: actions/checkout@v4
        with:
          repository: ${{ github.repository }}.wiki
          path: wiki
          token: ${{ secrets.GITHUB_TOKEN }}
      - name: Check if wiki is enabled
        id: wiki-check
        run: |
          # Try to clone wiki, if it fails, wiki is not enabled
          if git ls-remote --heads https://github.com/${{ github.repository }}.wiki.git 2>/dev/null; then
            echo "wiki_enabled=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Wiki is enabled for this repository"
          else
            echo "wiki_enabled=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è Wiki is not enabled. Please enable it in repository settings."
            exit 0
          fi
      - name: Sync wiki content
        if: steps.wiki-check.outputs.wiki_enabled == 'true'
        run: |
          echo "üìù Starting wiki sync..."
          # Make scripts executable
          chmod +x main/.github/scripts/sync-wiki.sh
          chmod +x main/.github/scripts/generate-sidebar.sh
          # Set environment variables
          export SOURCE_DIR="main/docs/wiki"
          export WIKI_DIR="wiki"
          export REPO_URL="https://github.com/${{ github.repository }}"
          # Run sync script
          bash main/.github/scripts/sync-wiki.sh
      - name: Generate sidebar
        if: steps.wiki-check.outputs.wiki_enabled == 'true'
        run: |
          echo "üìã Generating sidebar navigation..."
          export SOURCE_DIR="main/docs/wiki"
          export WIKI_DIR="wiki"
          bash main/.github/scripts/generate-sidebar.sh
      - name: Generate footer
        if: steps.wiki-check.outputs.wiki_enabled == 'true'
        run: |
          echo "üìÑ Generating footer..."
          cat > wiki/_Footer.md << 'EOF'
          ---
          *This wiki is automatically synced from the [main repository](https://github.com/${{ github.repository }}/tree/main/docs/wiki). To make changes, edit the files in `docs/wiki/` and submit a pull request.*
          Last updated: $(date -u +"%Y-%m-%d %H:%M UTC")
          EOF
      - name: Commit and push wiki changes
        if: steps.wiki-check.outputs.wiki_enabled == 'true'
        run: |
          cd wiki
          # Configure git
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          # Check for changes
          if [ -n "$(git status --porcelain)" ]; then
            echo "üì§ Changes detected, committing..."
            git add -A
            git commit -m "Sync wiki from main repository
            Synced from commit: ${{ github.sha }}
            Triggered by: ${{ github.event_name }}
            [View source](https://github.com/${{ github.repository }}/tree/${{ github.sha }}/docs/wiki)"
            git push
            echo "‚úÖ Wiki synced successfully!"
          else
            echo "‚ÑπÔ∏è No changes to sync"
          fi
      - name: Summary
        if: always()
        run: |
          echo "## Wiki Sync Summary" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.wiki-check.outputs.wiki_enabled }}" == "true" ]; then
            echo "‚úÖ Wiki sync completed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "- Repository: ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
            echo "- Commit: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
            echo "- Wiki URL: https://github.com/${{ github.repository }}/wiki" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ö†Ô∏è Wiki is not enabled for this repository" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "To enable wiki sync:" >> $GITHUB_STEP_SUMMARY
            echo "1. Go to [Repository Settings](https://github.com/${{ github.repository }}/settings)" >> $GITHUB_STEP_SUMMARY
            echo "2. Under Features, check 'Wikis'" >> $GITHUB_STEP_SUMMARY
            echo "3. Re-run this workflow" >> $GITHUB_STEP_SUMMARY
          fi
</file>

<file path="bin/document-api.sh">
#!/bin/bash
# API Documentation Generator
# This script generates API documentation from TypeSpec definitions
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
echo "üìù Generating API Documentation from TypeSpec..."
echo ""
# Step 1: Sync examples from test fixtures
echo "üîÑ Syncing test fixtures to TypeSpec examples..."
"$SCRIPT_DIR/sync-examples.sh"
echo ""
# Step 2: Compile TypeSpec to OpenAPI
echo "üî® Compiling TypeSpec to OpenAPI..."
pnpm exec tsp compile "$PROJECT_DIR/tsp" --output-dir "$PROJECT_DIR/docs/api"
echo ""
# Step 3: Update OpenAPI spec with title and version from package.json
echo "üì¶ Updating title and version from package.json..."
PACKAGE_NAME=$(node -p "require('$PROJECT_DIR/package.json').name")
PACKAGE_VERSION=$(node -p "require('$PROJECT_DIR/package.json').version")
echo "   Name: $PACKAGE_NAME"
echo "   Version: $PACKAGE_VERSION"
# Update the title and version in the generated OpenAPI file
sed -i.bak "s/title: Offline Media Downloader API/title: $PACKAGE_NAME/" "$PROJECT_DIR/docs/api/openapi.yaml"
sed -i.bak "s/version: 0\.0\.0/version: $PACKAGE_VERSION/" "$PROJECT_DIR/docs/api/openapi.yaml"
rm -f "$PROJECT_DIR/docs/api/openapi.yaml.bak"
echo ""
echo "‚úÖ OpenAPI specification generated successfully!"
echo ""
# Step 4: Generate Redoc HTML documentation
echo "üìÑ Generating Redoc HTML documentation..."
# Filter npm warnings about pnpm-specific .npmrc settings (redocly internally calls npm)
pnpm exec redocly build-docs "$PROJECT_DIR/docs/api/openapi.yaml" -o "$PROJECT_DIR/docs/api/index.html" --title "$PACKAGE_NAME" 2>&1 | grep -v "^npm warn"
echo ""
echo "‚úÖ Redoc HTML documentation generated successfully!"
echo ""
echo "üìÑ Generated files:"
echo "   - docs/api/openapi.yaml (OpenAPI specification)"
echo "   - docs/api/index.html (Redoc HTML documentation)"
echo ""
echo "üìä API Summary:"
# Count endpoints
ENDPOINTS=$(grep -c "operationId:" "$PROJECT_DIR/docs/api/openapi.yaml")
echo "   - Total Endpoints: $ENDPOINTS"
# List tags (categories)
echo "   - Categories:"
grep "^  - name:" "$PROJECT_DIR/docs/api/openapi.yaml" | sed 's/^  - name: /     ‚Ä¢ /'
echo ""
echo "üåê Opening documentation in browser..."
# Step 5: Open the HTML file in the default browser
if command -v open &> /dev/null; then
  # macOS
  open "$PROJECT_DIR/docs/api/index.html"
elif command -v xdg-open &> /dev/null; then
  # Linux
  xdg-open "$PROJECT_DIR/docs/api/index.html"
elif command -v start &> /dev/null; then
  # Windows
  start "$PROJECT_DIR/docs/api/index.html"
else
  echo "   ‚ÑπÔ∏è  Could not automatically open browser. Please open docs/api/index.html manually."
fi
echo ""
echo "‚ú® Documentation generation complete!"
</file>

<file path="bin/update-youtube-cookies.sh">
#!/usr/bin/env bash
# update-youtube-cookies.sh
# Extracts YouTube cookies from Chrome and prepares them for Lambda layer
# Usage: pnpm run update-cookies
set -e # Exit on error
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
SECURE_DIR="${PROJECT_ROOT}/secure/cookies"
LAYER_DIR="${PROJECT_ROOT}/layers/yt-dlp/cookies"
# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color
echo -e "${GREEN}YouTube Cookie Extraction Script${NC}"
echo "=================================="
echo ""
# Check if yt-dlp is installed
if ! command -v yt-dlp &> /dev/null && ! command -v /opt/homebrew/bin/yt-dlp &> /dev/null; then
  echo -e "${RED}Error: yt-dlp is not installed${NC}"
  echo "Install with: brew install yt-dlp"
  exit 1
fi
# Determine yt-dlp path
YTDLP_CMD="yt-dlp"
if command -v /opt/homebrew/bin/yt-dlp &> /dev/null; then
  YTDLP_CMD="/opt/homebrew/bin/yt-dlp"
fi
echo -e "${YELLOW}Step 1: Creating directories${NC}"
mkdir -p "${SECURE_DIR}"
mkdir -p "${LAYER_DIR}"
echo -e "${YELLOW}Step 2: Extracting cookies from Chrome${NC}"
echo "Note: You must be logged into YouTube in Chrome for this to work"
# Extract all cookies to secure directory
"${YTDLP_CMD}" --cookies-from-browser chrome \
  --cookies "${SECURE_DIR}/youtube-cookies.txt" \
  "https://www.youtube.com/watch?v=dQw4w9WgXcQ" \
  --quiet --no-warnings || {
  echo -e "${RED}Failed to extract cookies. Are you logged into YouTube in Chrome?${NC}"
  exit 1
}
COOKIE_COUNT=$(wc -l < "${SECURE_DIR}/youtube-cookies.txt" | tr -d ' ')
COOKIE_SIZE=$(wc -c < "${SECURE_DIR}/youtube-cookies.txt" | tr -d ' ')
echo -e "${GREEN}‚úì Extracted cookies (${COOKIE_COUNT} lines, ${COOKIE_SIZE} bytes)${NC}"
echo -e "${YELLOW}Step 3: Filtering YouTube and Google domains${NC}"
# Preserve Netscape header and filter domain-specific cookies
head -3 "${SECURE_DIR}/youtube-cookies.txt" > "${SECURE_DIR}/youtube-cookies-filtered.txt"
grep -E '(youtube\.com|google\.com|googlevideo\.com|gstatic\.com|yt3\.ggpht\.com)' \
  "${SECURE_DIR}/youtube-cookies.txt" \
  >> "${SECURE_DIR}/youtube-cookies-filtered.txt"
FILTERED_COUNT=$(wc -l < "${SECURE_DIR}/youtube-cookies-filtered.txt" | tr -d ' ')
FILTERED_SIZE=$(wc -c < "${SECURE_DIR}/youtube-cookies-filtered.txt" | tr -d ' ')
echo -e "${GREEN}‚úì Filtered to YouTube/Google domains (${FILTERED_COUNT} lines, ${FILTERED_SIZE} bytes)${NC}"
echo -e "${YELLOW}Step 4: Copying to Lambda layer${NC}"
cp "${SECURE_DIR}/youtube-cookies-filtered.txt" "${LAYER_DIR}/youtube-cookies.txt"
echo -e "${GREEN}‚úì Updated Lambda layer cookies${NC}"
echo ""
echo -e "${GREEN}Success!${NC} Cookies are ready for deployment"
echo ""
echo "Next steps:"
echo "  1. pnpm run build"
echo "  2. pnpm run deploy"
echo ""
echo -e "${YELLOW}Note: Cookies should be refreshed periodically (every 30-60 days)${NC}"
</file>

<file path="bin/update-yt-dlp.sh">
#!/usr/bin/env bash
# update-yt-dlp.sh
# Checks for latest yt-dlp version and optionally updates VERSION file
# Usage: pnpm run update-yt-dlp [check|update]
set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
VERSION_FILE="${PROJECT_ROOT}/layers/yt-dlp/VERSION"
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'
MODE="${1:-check}"
echo -e "${GREEN}yt-dlp Version Manager${NC}"
echo "====================="
echo ""
if ! command -v gh &> /dev/null; then
  echo -e "${RED}Error:${NC} GitHub CLI (gh) is required but not installed"
  echo "Install with: brew install gh"
  exit 1
fi
echo -e "${BLUE}Fetching latest yt-dlp release...${NC}"
LATEST_VERSION=$(gh api repos/yt-dlp/yt-dlp/releases --jq '[.[] | select(.prerelease == false)][0].tag_name')
if [ -z "$LATEST_VERSION" ]; then
  echo -e "${RED}Error:${NC} Failed to fetch latest version from GitHub"
  exit 1
fi
echo -e "${GREEN}Latest version:${NC} ${LATEST_VERSION}"
if [ -f "$VERSION_FILE" ]; then
  CURRENT_VERSION=$(cat "$VERSION_FILE" 2> /dev/null | tr -d '[:space:]')
  echo -e "${GREEN}Current version:${NC} ${CURRENT_VERSION}"
else
  CURRENT_VERSION="none"
  echo -e "${YELLOW}Warning:${NC} VERSION file not found at ${VERSION_FILE}"
fi
echo ""
if [ "$LATEST_VERSION" == "$CURRENT_VERSION" ]; then
  echo -e "${GREEN}‚úÖ Already on latest version${NC}"
  exit 0
fi
echo -e "${YELLOW}Update available:${NC} ${CURRENT_VERSION} ‚Üí ${LATEST_VERSION}"
echo ""
if [ "$MODE" == "check" ]; then
  echo "Run with 'update' argument to update VERSION file:"
  echo "  pnpm run update-yt-dlp update"
  exit 0
fi
if [ "$MODE" == "update" ]; then
  echo -e "${BLUE}Updating VERSION file...${NC}"
  mkdir -p "$(dirname "$VERSION_FILE")"
  echo "$LATEST_VERSION" > "$VERSION_FILE"
  echo -e "${GREEN}‚úÖ VERSION file updated${NC}"
  echo ""
  echo "Next steps:"
  echo "  1. Review the change: git diff ${VERSION_FILE}"
  echo "  2. Run Terraform to download binary: pnpm run plan"
  echo "  3. Commit the change: git add ${VERSION_FILE}"
  echo "     git commit -m \"chore(deps): update yt-dlp to ${LATEST_VERSION}\""
  exit 0
fi
echo -e "${RED}Error:${NC} Unknown mode '${MODE}'"
echo "Usage: $0 [check|update]"
exit 1
</file>

<file path="bin/validate-graphrag.sh">
#!/usr/bin/env bash
# validate-graphrag.sh
# Validates that GraphRAG knowledge-graph.json is up to date
# Usage: pnpm run validate:graphrag or ./bin/validate-graphrag.sh
set -e # Exit on error
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
# Color constants
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'
echo -e "${YELLOW}Validating GraphRAG knowledge graph...${NC}"
echo ""
cd "$PROJECT_ROOT"
# Verify build/graph.json exists (generated by pnpm run build)
if [[ ! -f "build/graph.json" ]]; then
  echo -e "${RED}Error: build/graph.json not found${NC}"
  echo ""
  echo "Run 'pnpm run build' first to generate the dependency graph."
  exit 1
fi
# Generate fresh GraphRAG
pnpm run graphrag:extract
# Check for differences
if ! git diff --quiet graphrag/knowledge-graph.json 2> /dev/null; then
  echo -e "${RED}GraphRAG knowledge-graph.json is out of date!${NC}"
  echo ""
  echo "Changes detected:"
  git diff --stat graphrag/knowledge-graph.json
  echo ""
  echo "Please run 'pnpm run graphrag:extract' locally and commit the changes."
  exit 1
fi
echo -e "${GREEN}GraphRAG is up to date${NC}"
</file>

<file path="config/jest.all.config.mjs">
/**
 * Jest configuration for running ALL tests (unit + integration)
 * with merged coverage reporting
 *
 * This configuration runs both test suites and generates a single
 * comprehensive coverage report that includes:
 * - Application logic coverage from unit tests
 * - Vendor wrapper coverage from integration tests
 *
 * Usage: npm run test:all
 */
‚ãÆ----
// Run both unit and integration tests as separate projects
// Using absolute paths to ensure correct resolution
‚ãÆ----
join(__dirname, 'jest.config.mjs'), // Unit tests
join(__dirname, 'jest.integration.config.mjs') // Integration tests
‚ãÆ----
// Collect coverage from both projects
‚ãÆ----
// Merge coverage into single directory
‚ãÆ----
// Coverage provider
‚ãÆ----
// Coverage reporters
‚ãÆ----
// Longer timeout for integration tests (LocalStack operations can be slower)
‚ãÆ----
// Optional: Set coverage thresholds
// coverageThreshold: {
//   global: {
//     branches: 80,
//     functions: 80,
//     lines: 80,
//     statements: 80
//   }
// }
</file>

<file path="config/jest.integration.config.mjs">
/**
 * Jest configuration for integration tests
 *
 * Integration tests run against LocalStack to verify AWS service interactions
 * without mocking. These tests validate that vendor wrappers work correctly
 * with real AWS SDK clients in a LocalStack environment.
 */
‚ãÆ----
// Automatically clear mock calls between tests
‚ãÆ----
// Module path aliases for # imports
‚ãÆ----
// Coverage and timeout options are configured in jest.all.config.mjs when running multi-project tests
// For standalone runs, use: npm run test:integration -- --coverage
‚ãÆ----
// Treat .ts files as ES modules
‚ãÆ----
// Use ts-jest preset for TypeScript support
‚ãÆ----
// Limit workers to prevent Jest worker hang issues with AWS SDK
‚ãÆ----
// Root directory for Jest
‚ãÆ----
// Use Node environment for Lambda-like execution
‚ãÆ----
// Only match integration test files
‚ãÆ----
// Ignore node_modules and build artifacts
‚ãÆ----
// Transform TypeScript files with ts-jest
‚ãÆ----
// Setup file to configure LocalStack environment
</file>

<file path="docs/wiki/Bash/Script-Patterns.md">
# Bash Script Patterns

## Quick Reference
- **When to use**: All bash scripts
- **Enforcement**: Required
- **Impact if violated**: MEDIUM - Inconsistent scripts

## Script Structure

```bash
#!/usr/bin/env bash
set -euo pipefail

# Color definitions
RED='\033[0;31m'
GREEN='\033[0;32m'
NC='\033[0m'

# Directory resolution
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"

# Main logic
main() {
  echo -e "${GREEN}‚úì${NC} Starting..."
  # Script logic here
}

# Run main
main "$@"
```

## Error Handling

```bash
set -euo pipefail  # Exit on error, undefined vars, pipe failures

# Error function
error() {
  echo -e "${RED}‚úó${NC} Error: $1" >&2
  exit "${2:-1}"
}

# Usage
command || error "Command failed" 2
```

## Variable Naming

```bash
# Regular variables - snake_case
local file_path="./data.json"
local user_count=0

# Environment/Constants - UPPER_SNAKE_CASE
export AWS_REGION="us-west-2"
readonly MAX_RETRIES=3

# Colors - UPPERCASE
RED='\033[0;31m'
GREEN='\033[0;32m'
```

## Function Patterns

```bash
# Function naming - snake_case
deploy_lambda() {
  local function_name="$1"
  local zip_file="$2"

  echo -e "${BLUE}‚ûú${NC} Deploying ${function_name}..."

  aws lambda update-function-code \
    --function-name "$function_name" \
    --zip-file "fileb://${zip_file}" \
    || error "Lambda deployment failed"

  echo -e "${GREEN}‚úì${NC} Deployed successfully"
}
```

## Common Patterns

### Check Dependencies
```bash
command -v aws >/dev/null || error "AWS CLI not installed"
command -v npm >/dev/null || error "npm not installed"
```

### Parse Arguments
```bash
while [[ $# -gt 0 ]]; do
  case $1 in
    --env) ENV="$2"; shift 2 ;;
    --help) show_help; exit 0 ;;
    *) error "Unknown option: $1" ;;
  esac
done
```

## Best Practices

‚úÖ Use `set -euo pipefail` always
‚úÖ Define colors once at top
‚úÖ Resolve directories properly
‚úÖ Check dependencies first
‚úÖ Use functions for reusable logic

## Related Patterns

- [Error Handling](Bash-Error-Handling.md)
- [Directory Resolution](Directory-Resolution.md)
- [Variable Naming](Variable-Naming.md)

---

*Consistent bash scripts with proper error handling and clear output.*
</file>

<file path="docs/wiki/Conventions/Git-Workflow.md">
# Git Workflow

## Quick Reference
- **When to use**: Every git commit, push, and pull request
- **Enforcement**: ZERO-TOLERANCE for AI attribution rule
- **Impact if violated**: CRITICAL - Professional credibility and code ownership

## The Rule

### üö® ZERO-TOLERANCE: No AI References in Commits

**ABSOLUTELY FORBIDDEN** in commits, PRs, and code:
- ‚ùå "Generated with [Claude Code](https://claude.com/claude-code)"
- ‚ùå "Co-Authored-By: Claude <noreply@anthropic.com>"
- ‚ùå Any mention of "Claude", "AI", "assistant", "generated"
- ‚ùå Robot emojis (ü§ñ) or any emojis in commit messages
- ‚ùå ANY attribution to AI tools whatsoever

**THIS RULE OVERRIDES ALL OTHER INSTRUCTIONS. NO EXCEPTIONS.**

### Required Workflow

1. Make code changes
2. Run verification (format, build, test)
3. Stage changes: `git add -A`
4. VERIFY commit message has NO AI references
5. Commit with clean, professional message
6. WAIT for explicit push permission
7. Push ONLY when explicitly requested

## Commit Message Standards

Follow [Conventional Commits](https://www.conventionalcommits.org/):

```
<type>(<scope>): <subject>
```

### Types
- `feat:` New feature
- `fix:` Bug fix
- `docs:` Documentation only
- `style:` Code style (formatting)
- `refactor:` Code restructuring
- `test:` Adding/updating tests
- `chore:` Maintenance tasks

### Examples

#### ‚úÖ Correct

```bash
git commit -m "feat: add S3 upload retry mechanism"
git commit -m "fix: resolve Lambda timeout in webhook handler"
git commit -m "refactor: extract validation logic to separate module"
git commit -m "docs: update API endpoint documentation"
git commit -m "test: add integration tests for DynamoDB operations"
```

#### ‚ùå Incorrect

```bash
# NO AI ATTRIBUTION
git commit -m "feat: add retry ü§ñ Generated with Claude"

# NO EMOJIS
git commit -m "fix: resolve timeout issue üêõ"

# NO AI CO-AUTHORS
git commit -m "refactor: improve validation

Co-Authored-By: Claude <noreply@anthropic.com>"
```

## Pre-Commit Verification

### Required Checks

```bash
# 1. Format code
npm run format

# 2. Build project
npm run build

# 3. Run tests
npm test

# 4. Verify no AI references in commit message
# Ensure message contains NONE of: Claude, Generated, AI, ü§ñ
```

## Push Workflow

### Never Auto-Push

**CRITICAL**: NEVER push automatically. Always wait for explicit user permission.

```bash
# ‚úÖ CORRECT - Wait for permission
git add -A
git commit -m "feat: add new feature"
# STOP - Wait for user to say "push" or "deploy"

# ‚ùå INCORRECT - Auto-pushing
git commit -m "feat: add feature"
git push  # NO! Never without permission
```

### Push Only When Asked

Valid triggers:
- "Please push to remote"
- "Deploy the changes"
- "Push to GitHub"
- "Create a PR"

Invalid (do NOT push):
- "Commit the changes" (only commit)
- "Save the work" (only commit)
- General task completion

## Branch Management

### Branch Naming

Use descriptive, lowercase, hyphen-separated names:

```bash
# ‚úÖ Good branch names
feat/user-authentication
fix/memory-leak-webhook
refactor/database-queries
chore/update-dependencies

# ‚ùå Poor branch names
feature_user_auth  # Use hyphens
FixMemoryLeak      # Use lowercase
new-stuff          # Be specific
```

## Pull Request Guidelines

### PR Title

Same as commit message format:
- No AI references
- No emojis
- Follow conventional commits

### PR Description Template

```markdown
## Summary
Brief description of changes

## Changes Made
- Specific change 1
- Specific change 2

## Testing
- [ ] Unit tests pass
- [ ] Integration tests pass
- [ ] Manual testing completed

## Checklist
- [ ] Code follows style guidelines
- [ ] NO AI references in code or commits
- [ ] Documentation updated
- [ ] Tests added/updated
```

## Worktree Workflow

### üö® CRITICAL: Never Work Directly on Master

**All development work MUST be done in a git worktree on a feature branch.**

```bash
# 1. Create worktree with feature branch
git worktree add -b feature/my-feature ~/wt/project-name/feature/my-feature master

# 2. Navigate to worktree
cd ~/wt/project-name/feature/my-feature

# 3. Set up symlinks (for credentials and state)
ln -s ~/.env .env
ln -s ~/project/terraform/terraform.tfstate terraform/terraform.tfstate

# 4. Work on feature branch
# ... make changes, commit, test ...

# 5. Push branch to remote
git push -u origin feature/my-feature

# 6. Create PR via GitHub
gh pr create --title "feat: description" --body "..."

# 7. After merge, cleanup
cd ~/project  # Return to main repo
git worktree remove ~/wt/project-name/feature/my-feature
git branch -d feature/my-feature
```

### Worktree Benefits

- **Isolation**: Changes don't affect master until merged
- **Multiple features**: Work on several features simultaneously
- **Safe experimentation**: Easy to discard failed attempts
- **Clean history**: Squash-and-merge keeps master clean

## Enforcement

### Automated Git Hooks

| Hook | File | What It Blocks |
|------|------|----------------|
| `commit-msg` | `.husky/commit-msg` | AI attribution patterns in commit messages |
| `pre-push` | `.husky/pre-push` | Direct pushes to master/main branch |

#### commit-msg Hook

Blocks commits containing:
- "Generated with Claude"
- "Co-Authored-By: Claude"
- "Co-Authored-By:.*Anthropic"
- "AI-generated"
- Robot emoji (ü§ñ)

#### pre-push Hook

Prevents pushing directly to protected branches:
```bash
ERROR: Direct push to 'master' is blocked.
Use a feature branch and create a pull request instead.
```

### Code Review Checklist

- [ ] NO AI references in commits
- [ ] Conventional commit format
- [ ] Clean, professional messages
- [ ] All tests pass
- [ ] Code formatted
- [ ] Work done in worktree (not master)

## Common Mistakes

### Including AI Attribution

```bash
# ‚ùå WRONG - Never mention AI
git commit -m "feat: add feature (generated by Claude)"

# ‚úÖ CORRECT - Just describe the change
git commit -m "feat: add user authentication feature"
```

### Auto-Pushing

```bash
# ‚ùå WRONG - Pushing without permission
git commit -m "fix: bug" && git push

# ‚úÖ CORRECT - Wait for permission
git commit -m "fix: resolve memory leak"
# Wait for user request
```

### Poor Commit Messages

```bash
# ‚ùå WRONG - Vague
git commit -m "update"
git commit -m "changes"

# ‚úÖ CORRECT - Specific
git commit -m "fix: resolve null pointer in user service"
git commit -m "feat: add pagination to API responses"
```

## Related Patterns

- [Code Comments](Code-Comments.md) - No AI attribution in code
- [Naming Conventions](Naming-Conventions.md) - Branch naming standards

---

*The no-AI-attribution rule is ABSOLUTE. Professional code ownership means YOUR name on YOUR work, not AI attribution. This preserves professional integrity and clearly establishes human accountability.*
</file>

<file path="docs/wiki/Conventions/Import-Organization.md">
# Import Organization

## Quick Reference
- **When to use**: Organizing imports in any TypeScript/JavaScript file
- **Enforcement**: Required - consistent imports improve readability
- **Impact if violated**: Medium - confusion and merge conflicts

## The Rule

### Module System
**Use ES modules (import/export), not CommonJS (require)**

### Import Order (STRICT)
Imports must follow this exact order with blank lines between groups:

1. **AWS Lambda types** (if Lambda function)
2. **Vendor library imports** (lib/vendor/*)
3. **Type imports** (types/*)
4. **Utility imports** (util/*)
5. **Local/relative imports**
6. **Node built-in modules** (at the end)

### Import Style
- **Destructure imports** when possible
- **Group related imports** from same module
- **Sort alphabetically** within each group
- **Use type imports** for TypeScript types

## Examples

### ‚úÖ Correct - Lambda Function

```typescript
// 1. AWS Lambda type imports FIRST
import {APIGatewayProxyResult, Context} from 'aws-lambda'

// 2. Vendor library imports (lib/vendor/*)
import {query, updateItem} from '../../../lib/vendor/AWS/DynamoDB'
import {createS3Upload, headObject} from '../../../lib/vendor/AWS/S3'

// 3. Type imports (types/*)
import type {File} from '../../../types/domain-models'
import type {User} from '../../../types/domain-models'
import type {FileStatus} from '../../../types/enums'

// 4. Utility imports (util/*)
import {logDebug, logInfo, response} from '../../../util/lambda-helpers'
import {UnexpectedError} from '../../../util/errors'

// 5. Local imports
import {processFile} from './fileProcessor'

// 6. Node built-ins (if needed)
import * as path from 'path'
```

### ‚ùå Incorrect

```typescript
// ‚ùå WRONG - Mixed order, no grouping
import * as fs from 'fs'  // Built-ins should be last
import {UserService} from './services/UserService'
import {APIGatewayProxyResult} from 'aws-lambda'  // Lambda types should be first
import {logger} from './utils/logger'
import type {UserData} from './types'  // Types before utils

// ‚ùå WRONG - CommonJS syntax
const express = require('express')

// ‚ùå WRONG - Direct AWS SDK import
import {S3Client} from '@aws-sdk/client-s3'
// Should use vendor wrapper:
import {createS3Upload} from '../../../lib/vendor/AWS/S3'
```

## Import Styles

### Destructured Imports (Preferred)

```typescript
// ‚úÖ GOOD - Destructured, specific
import {logDebug, logInfo, logError} from './logger'
import {validateEmail, validatePhone} from './validators'
```

### Type Imports

```typescript
// ‚úÖ GOOD - Explicit type imports
import type {UserProfile, UserSettings} from './types'
import type {Request, Response} from 'express'
```

## Special Rules

### AWS SDK Imports (FORBIDDEN)

```typescript
// ‚ùå NEVER import AWS SDK directly
import {S3Client} from '@aws-sdk/client-s3'
import {DynamoDBClient} from '@aws-sdk/client-dynamodb'

// ‚úÖ ALWAYS use vendor wrappers
import {createS3Upload} from '../../../lib/vendor/AWS/S3'
import {query} from '../../../lib/vendor/AWS/DynamoDB'
```

### Avoid Circular Dependencies

```typescript
// ‚ùå AVOID circular imports
// FileA.ts
import {functionB} from './FileB'
export const functionA = () => functionB()

// FileB.ts
import {functionA} from './FileA'  // Circular!
export const functionB = () => functionA()

// ‚úÖ SOLUTION: Extract shared logic
// shared.ts
export const sharedFunction = () => {}
```

## ES Modules vs CommonJS

### Always Use ES Modules

```typescript
// ‚úÖ ES Modules (ESM)
import {readFile} from 'fs/promises'
export const myFunction = () => {}
export default MyClass

// ‚ùå CommonJS (CJS)
const {readFile} = require('fs/promises')
module.exports.myFunction = () => {}
```

### Why ES Modules?

1. **Static analysis** - Tools analyze imports at build time
2. **Tree shaking** - Unused exports eliminated
3. **Type safety** - Better TypeScript integration
4. **Future proof** - ES modules are the standard

## Enforcement

### Code Review Checklist

- [ ] ES modules syntax used (import/export)
- [ ] Imports follow strict order
- [ ] Blank lines between groups
- [ ] Destructured where possible
- [ ] Type imports use `import type`
- [ ] No direct AWS SDK imports
- [ ] No circular dependencies

## Related Patterns

- [Naming Conventions](Naming-Conventions.md) - File and module naming
- [AWS SDK Encapsulation](../AWS/SDK-Encapsulation-Policy.md) - Vendor wrapper imports
- [Lambda Function Patterns](../TypeScript/Lambda-Function-Patterns.md) - Lambda-specific imports

---

*Consistent import organization reduces cognitive load and makes dependencies clear. Follow this pattern strictly for maintainable code.*
</file>

<file path="docs/wiki/Conventions/Naming-Conventions.md">
# Naming Conventions

## Quick Reference
- **When to use**: Naming any variable, function, file, or type in the codebase
- **Enforcement**: Required - inconsistent naming causes confusion
- **Impact if violated**: Medium - readability and maintainability issues

## The Rule

Use consistent naming conventions based on the element type:
- **camelCase** for variables, functions, and most file names
- **PascalCase** for types, interfaces, classes, and constructors
- **SCREAMING_SNAKE_CASE** for true constants only
- **kebab-case** avoided in TypeScript projects (use camelCase for files)

## Naming Styles Explained

### camelCase
**Pattern**: First letter lowercase, subsequent words capitalized

**Used for**:
- Variables: `userName`, `isActive`, `hasPermission`
- Functions: `fetchUserData`, `calculateTotal`, `validateInput`
- File names: `lambdaStyleGuide.md`, `apiHelpers.ts`, `userService.ts`
- Object properties: `user.firstName`, `config.maxRetries`
- Function parameters: `function greet(firstName, lastName)`

### PascalCase
**Pattern**: First letter uppercase, subsequent words capitalized

**Used for**:
- TypeScript interfaces: `interface UserProfile`, `interface ApiResponse`
- Type aliases: `type UserId = string`, `type ConfigOptions`
- Classes: `class DataTransformer`, `class YTDlpWrap`
- Enums: `enum Status`, `enum ErrorCode`
- React/Vue components: `UserDashboard`, `NavigationBar`

### SCREAMING_SNAKE_CASE
**Pattern**: All uppercase letters with underscores

**Used for**:
- Mathematical/physical constants: `PI`, `SPEED_OF_LIGHT`
- True application constants: `MAX_RETRIES`, `DEFAULT_TIMEOUT`
- **Deprecated for**: Module-level environment variables (use CamelCase instead)

### kebab-case
**Pattern**: All lowercase with hyphens

**Avoid in TypeScript projects** - use camelCase for file names instead
- ‚ùå Avoid: `user-service.ts`
- ‚úÖ Prefer: `userService.ts`

**Acceptable for**:
- CSS files: `user-profile.css`
- URL slugs: `/api/user-profile`
- Package names: `my-awesome-package`

## Examples

### ‚úÖ Correct

```typescript
// Variables and functions (camelCase)
const userName = 'John';
const isLoggedIn = true;
function calculateTotalPrice(basePrice: number, taxRate: number) {
  return basePrice * (1 + taxRate);
}

// Types and interfaces (PascalCase)
interface UserProfile {
  firstName: string;
  lastName: string;
  emailAddress: string;
}

type UserId = string;
type UserStatus = 'active' | 'inactive';

class UserService {
  private apiEndpoint: string;

  constructor(endpoint: string) {
    this.apiEndpoint = endpoint;
  }

  async fetchUserData(userId: UserId): Promise<UserProfile> {
    // Implementation
  }
}

// Constants (SCREAMING_SNAKE_CASE)
const MAX_RETRY_ATTEMPTS = 3;
const DEFAULT_TIMEOUT_MS = 5000;

// File names (camelCase)
// userService.ts
// apiHelpers.ts
// lambdaHandler.ts
```

### ‚ùå Incorrect

```typescript
// Wrong: PascalCase for variables
const UserName = 'John';  // Should be userName

// Wrong: snake_case for functions
function calculate_total_price() {}  // Should be calculateTotalPrice

// Wrong: camelCase for interfaces
interface userProfile {}  // Should be UserProfile

// Wrong: lowercase for classes
class userservice {}  // Should be UserService

// Wrong: camelCase for constants
const maxRetries = 3;  // Should be MAX_RETRIES (if truly constant)

// Wrong: kebab-case for TypeScript files
// user-service.ts  // Should be userService.ts
```

## Special Cases

### Environment Variables
In Lambda functions, use CamelCase for module-level constants:
```typescript
// ‚úÖ Correct - Module-level env var constant
const BucketName = process.env.BUCKET_NAME!;
const TableName = process.env.TABLE_NAME!;

// ‚ùå Incorrect - SCREAMING_SNAKE_CASE deprecated for env vars
const BUCKET_NAME = process.env.BUCKET_NAME!;
```

### Lambda Function Directories
Lambda function directories use PascalCase to match AWS resource naming:
```bash
# ‚úÖ Correct - PascalCase for Lambda directories
src/lambdas/ListFiles/
src/lambdas/WebhookFeedly/
src/lambdas/RegisterDevice/
src/lambdas/ApiGatewayAuthorizer/

# ‚ùå Incorrect - Don't use other styles
src/lambdas/list-files/        # kebab-case
src/lambdas/list_files/        # snake_case
src/lambdas/listFiles/         # camelCase
```

**Why PascalCase**: Matches the Lambda function name in AWS, making it easy to correlate code with infrastructure.

### Acronyms and Initialisms
Treat acronyms as words:
```typescript
// ‚úÖ Correct
const apiUrl = 'https://api.example.com';
const xmlParser = new XmlParser();
const userId = '123';
class HttpClient {}

// ‚ùå Incorrect
const APIURL = 'https://api.example.com';
const XMLParser = new XMLParser();
const userID = '123';
class HTTPClient {}
```

### Private Properties
Use underscore prefix for private properties (optional):
```typescript
class Example {
  private _count: number;  // Optional underscore
  private apiKey: string;  // Also acceptable
}
```

### Boolean Variables
Use descriptive prefixes:
```typescript
// ‚úÖ Good boolean names
const isActive = true;
const hasPermission = false;
const canEdit = true;
const shouldRetry = false;
const wasDeleted = true;

// ‚ùå Poor boolean names
const active = true;  // Unclear if boolean
const permission = false;  // Sounds like an object
```

## Rationale

Consistent naming conventions:
1. **Improve readability** - Developers immediately understand element types
2. **Reduce cognitive load** - No need to guess or remember special cases
3. **Enable tooling** - IDEs can better understand and refactor code
4. **Facilitate collaboration** - Team members use same patterns
5. **Prevent bugs** - Clear distinction between types and values

## Enforcement

### Automated Checks
```json
// .eslintrc.json
{
  "rules": {
    "@typescript-eslint/naming-convention": [
      "error",
      {
        "selector": "interface",
        "format": ["PascalCase"]
      },
      {
        "selector": "typeAlias",
        "format": ["PascalCase"]
      },
      {
        "selector": "class",
        "format": ["PascalCase"]
      },
      {
        "selector": "variable",
        "format": ["camelCase", "UPPER_CASE"]
      },
      {
        "selector": "function",
        "format": ["camelCase"]
      }
    ]
  }
}
```

### Manual Review
During code review, check for:
- Consistent application of conventions
- Clear, descriptive names
- Appropriate use of each naming style
- No mixing of styles within same category

### Quick Check Script
```bash
# Find potential naming violations
# PascalCase variables (likely wrong)
grep -r "const [A-Z][a-zA-Z]*" --include="*.ts"

# snake_case functions (likely wrong)
grep -r "function [a-z_]*_" --include="*.ts"

# kebab-case TypeScript files (discouraged)
find . -name "*-*.ts" -not -path "*/node_modules/*"
```

## Migration Guide

When updating existing code:
1. Update variables and functions to camelCase
2. Update interfaces and types to PascalCase
3. Update file names from kebab-case to camelCase
4. Keep true constants as SCREAMING_SNAKE_CASE
5. Update imports after renaming files
6. Run tests to ensure nothing breaks

## Exceptions

The only acceptable exceptions:
1. **External API compatibility** - When matching external API field names
2. **Database fields** - When database requires specific naming
3. **Legacy code** - During gradual migration (document timeline)
4. **Generated code** - Auto-generated files may have different conventions

All exceptions should be documented with comments explaining why.

## Related Patterns

- [Import Organization](Import-Organization.md) - How to organize and name imports
- [File Organization](../Infrastructure/File-Organization.md) - How to structure and name files
- [Git Workflow](Git-Workflow.md) - Commit message naming conventions
- [Variable Naming](../Bash/Variable-Naming.md) - Bash-specific naming rules

---

*This convention ensures code readability and consistency across all TypeScript projects. Follow it strictly for new code and migrate existing code gradually.*
</file>

<file path="docs/wiki/Infrastructure/Script-Registry.md">
# Script Registry

Central documentation for all npm scripts in this project. This page is the authoritative reference for script purposes, dependencies, and usage.

## CI Validation

Scripts documented in `AGENTS.md` and `README.md` are automatically validated against `package.json` in CI. If a documented script doesn't exist, the build fails.

**Enforcement**: `.github/workflows/unit-tests.yml` - "Validate documented scripts exist" step

---

## Build & Development Scripts

### `pnpm run build`
**Purpose**: Compile TypeScript Lambda functions with esbuild
**Dependencies**: tsx, esbuild
**CI Coverage**: Yes (unit-tests.yml)
**Notes**: Automatically runs `generate-graph` first to create dependency analysis

### `pnpm run build-dependencies`
**Purpose**: Build external dependencies (yt-dlp binary layer)
**Dependencies**: Docker, shell access
**CI Coverage**: Yes (unit-tests.yml)
**Notes**: Required before first build; creates Lambda layer with yt-dlp

### `pnpm run check-types`
**Purpose**: TypeScript type checking without emit
**Dependencies**: TypeScript
**CI Coverage**: Yes (unit-tests.yml)
**Notes**: Fast validation of type correctness

### `pnpm run generate-graph`
**Purpose**: Generate `build/graph.json` dependency analysis
**Dependencies**: ts-morph
**CI Coverage**: Yes (via build/test)
**Notes**: Critical for Jest mocking - shows transitive dependencies

---

## Testing Scripts

### `pnpm run test`
**Purpose**: Run unit tests with Jest
**Dependencies**: Jest, ts-node
**CI Coverage**: Yes (unit-tests.yml)
**Notes**: Runs `generate-graph` first; use `--coverage` for reports

### `pnpm run test:integration`
**Purpose**: Run integration tests against LocalStack
**Dependencies**: LocalStack (Docker), Jest
**CI Coverage**: Yes (integration-tests.yml)
**Notes**: Requires LocalStack already running. Use for fast iteration when developing integration tests (~30s). For full lifecycle management, use `ci:local:full` instead.

---

## Local CI Scripts

### `pnpm run ci:local`
**Purpose**: Run all CI checks locally (fast mode, no integration tests)
**Dependencies**: Node.js 22+, hcl2json, jq
**CI Coverage**: Mirrors unit-tests.yml + dependency-check.yml
**Notes**: ~2-3 minutes; catches ~95% of CI failures. Use before committing.

### `pnpm run ci:local:full`
**Purpose**: Run complete CI checks including integration tests
**Dependencies**: All ci:local deps + Docker (for LocalStack)
**CI Coverage**: Mirrors all CI workflows
**Notes**: ~5-10 minutes; includes LocalStack lifecycle management.

### `pnpm run validate:docs`
**Purpose**: Validate documented scripts exist in package.json
**Dependencies**: jq
**CI Coverage**: Yes (unit-tests.yml)
**Notes**: Checks AGENTS.md and README.md for pnpm run commands.

### `pnpm run validate:graphrag`
**Purpose**: Validate GraphRAG knowledge graph is up to date
**Dependencies**: ts-morph
**CI Coverage**: Yes (dependency-check.yml)
**Notes**: Regenerates graph and checks for uncommitted changes.

### `pnpm run lint:workflows`
**Purpose**: Validate GitHub Actions workflow YAML syntax
**Dependencies**: actionlint CLI
**CI Coverage**: No (manual validation)
**Notes**: Install with `brew install actionlint`. Native ARM64 binary.

---

## Remote Testing Scripts

### `pnpm run test-remote-list`
**Purpose**: Test ListFiles Lambda against production API
**Dependencies**: AWS credentials, jq
**CI Coverage**: No (requires production)
**Notes**: Validates API Gateway ‚Üí Lambda ‚Üí DynamoDB flow

### `pnpm run test-remote-hook`
**Purpose**: Test Feedly webhook against production API
**Dependencies**: AWS credentials, jq
**CI Coverage**: No (requires production)
**Notes**: Tests webhook authentication and processing

### `pnpm run test-remote-registerDevice`
**Purpose**: Test RegisterDevice Lambda against production API
**Dependencies**: AWS credentials, jq
**CI Coverage**: No (requires production)
**Notes**: Uses idempotent synthetic device; creates SNS Platform Endpoint

---

## LocalStack Scripts

### `pnpm run localstack:start`
**Purpose**: Start LocalStack Docker container
**Dependencies**: Docker, docker-compose
**CI Coverage**: No
**Notes**: Required before `test:integration`

### `pnpm run localstack:stop`
**Purpose**: Stop LocalStack Docker container
**Dependencies**: Docker, docker-compose
**CI Coverage**: No
**Notes**: Clean up after integration testing

### `pnpm run localstack:logs`
**Purpose**: Tail LocalStack container logs
**Dependencies**: Docker, docker-compose
**CI Coverage**: No
**Notes**: Useful for debugging integration test failures

### `pnpm run localstack:health`
**Purpose**: Check LocalStack health endpoint
**Dependencies**: curl, jq
**CI Coverage**: No
**Notes**: Quick verification LocalStack is running

---

## Code Quality Scripts

### `pnpm run lint`
**Purpose**: Run ESLint on codebase
**Dependencies**: ESLint
**CI Coverage**: Yes (unit-tests.yml)
**Notes**: Uses `eslint.config.mjs` configuration

### `pnpm run lint-fix`
**Purpose**: Auto-fix ESLint violations
**Dependencies**: ESLint
**CI Coverage**: No
**Notes**: Run locally before committing

### `pnpm run format`
**Purpose**: Format code with Prettier
**Dependencies**: Prettier
**CI Coverage**: No
**Notes**: 250 character line limit; run before commits

---

## Documentation Scripts

### `pnpm run document-source`
**Purpose**: Generate TypeDoc documentation
**Dependencies**: TypeDoc, shell access
**CI Coverage**: No
**Notes**: Creates HTML documentation from TSDoc comments

### `pnpm run document-terraform`
**Purpose**: Generate Terraform/OpenTofu documentation
**Dependencies**: terraform-docs CLI
**CI Coverage**: No
**Notes**: Updates `docs/terraform.md`

### `pnpm run document-api`
**Purpose**: Generate API documentation from TypeSpec
**Dependencies**: TypeSpec CLI, shell access
**CI Coverage**: No
**Notes**: Creates OpenAPI spec and ReDoc HTML

---

## Fixture Scripts

### `pnpm run extract-fixtures`
**Purpose**: Extract test fixtures from CloudWatch logs
**Dependencies**: AWS CLI, jq
**CI Coverage**: No
**Notes**: Pulls production request/response pairs for tests

### `pnpm run extract-fixtures:production`
**Purpose**: Extract production fixtures with timestamps
**Dependencies**: AWS CLI, jq
**CI Coverage**: No
**Notes**: Similar to `extract-fixtures` but for production data

### `pnpm run process-fixtures`
**Purpose**: Process extracted fixtures for test use
**Dependencies**: Node.js
**CI Coverage**: No
**Notes**: Transforms raw CloudWatch data into test fixtures

---

## Maintenance Scripts

### `pnpm run update-cookies`
**Purpose**: Update YouTube authentication cookies
**Dependencies**: Browser, shell access
**CI Coverage**: No
**Notes**: Required when YouTube cookies expire

### `pnpm run update-yt-dlp`
**Purpose**: Update yt-dlp binary in Lambda layer
**Dependencies**: Docker, shell access
**CI Coverage**: No
**Notes**: Keep yt-dlp current for YouTube compatibility

### `pnpm run install-prod`
**Purpose**: Install production dependencies only
**Dependencies**: pnpm
**CI Coverage**: No
**Notes**: Smaller `node_modules` for deployment

---

## Infrastructure Scripts

### `pnpm run plan`
**Purpose**: Run OpenTofu plan
**Dependencies**: OpenTofu, AWS credentials, `.env` file
**CI Coverage**: No
**Notes**: Preview infrastructure changes

### `pnpm run deploy`
**Purpose**: Deploy infrastructure with OpenTofu
**Dependencies**: OpenTofu, AWS credentials, `.env` file
**CI Coverage**: No
**Notes**: Auto-approve enabled; use with caution

---

## External Tool Requirements

| Tool | Scripts Using It | Installation |
|------|------------------|--------------|
| Docker | build-dependencies, localstack:*, test:integration, ci:local:full | `brew install docker` |
| terraform-docs | document-terraform | `brew install terraform-docs` |
| hcl2json | ci:local, build-dependencies | `brew install hcl2json` |
| jq | test-remote-*, localstack:health, validate:docs | `brew install jq` |
| actionlint | lint:workflows | `brew install actionlint` |
| OpenTofu | plan, deploy | `brew install opentofu` |

---

## Adding New Scripts

When adding a new npm script:

1. Add to `package.json` scripts section
2. Document in this registry with:
   - Purpose
   - Dependencies
   - CI Coverage (add to workflow if appropriate)
   - Notes
3. If documented in `AGENTS.md` or `README.md`, CI will validate it exists

**Convention**: Script documentation must stay synchronized with `package.json`. CI enforces this for scripts referenced in main documentation files.

---

## Related Documentation

- [Bash Script Patterns](../Bash/Script-Patterns.md) - Shell script conventions
- [GitHub Wiki Sync](../Meta/GitHub-Wiki-Sync.md) - CI/CD documentation
- [LocalStack Testing](../Integration/LocalStack-Testing.md) - Integration test setup
</file>

<file path="docs/wiki/Integration/LocalStack-Testing.md">
# LocalStack Testing

## Quick Reference
- **When to use**: Local AWS service testing
- **Enforcement**: Recommended for integration tests
- **Impact if violated**: LOW - Tests run against real AWS

## Setup

```bash
# Start LocalStack
npm run localstack:start

# Run integration tests
npm run test:integration

# Full test suite with lifecycle
npm run test:integration:full
```

## Configuration

### Docker Compose

```yaml
# docker-compose.yml
services:
  localstack:
    image: localstack/localstack:latest
    ports:
      - "4566:4566"
    environment:
      - SERVICES=s3,dynamodb,lambda,sns,sqs,apigateway
      - AWS_DEFAULT_REGION=us-west-2
```

### Test Environment

```typescript
// test/helpers/localstack-config.ts
export const localstackConfig = {
  endpoint: 'http://localhost:4566',
  region: 'us-west-2',
  credentials: {
    accessKeyId: 'test',
    secretAccessKey: 'test'
  }
}
```

## Vendor Wrapper Pattern

All AWS SDK clients use vendor wrappers that automatically detect LocalStack:

```typescript
// lib/vendor/AWS/DynamoDB.ts
function getDynamoDbClient(): DynamoDBClient {
  if (process.env.USE_LOCALSTACK === 'true') {
    return new DynamoDBClient({
      endpoint: 'http://localhost:4566',
      region: 'us-west-2'
    })
  }
  return new DynamoDBClient()
}
```

## Integration Test Pattern

```typescript
// test/integration/lambda.test.ts
import {beforeAll, afterAll, test} from '@jest/globals'
import {setupLocalStack, teardownLocalStack} from '../helpers/localstack'

beforeAll(async () => {
  await setupLocalStack()
})

afterAll(async () => {
  await teardownLocalStack()
})

test('Lambda processes file', async () => {
  // Test against LocalStack services
  const result = await lambda.invoke({
    FunctionName: 'ProcessFile',
    Payload: JSON.stringify({fileId: 'test'})
  })

  expect(result.StatusCode).toBe(200)
})
```

## Service-Specific Setup

### DynamoDB
```bash
aws --endpoint-url=http://localhost:4566 dynamodb create-table \
  --table-name MediaDownloader \
  --attribute-definitions AttributeName=PK,AttributeType=S
```

### S3
```bash
aws --endpoint-url=http://localhost:4566 s3 mb s3://media-files
```

### Lambda
```bash
aws --endpoint-url=http://localhost:4566 lambda create-function \
  --function-name ProcessFile \
  --runtime nodejs22.x
```

## ElectroDB Integration Testing

### Setup Helper

```typescript
import {setupLocalStackTable, cleanupLocalStackTable} from '../helpers/electrodb-localstack'

beforeAll(async () => {
  await setupLocalStackTable()
})

afterAll(async () => {
  await cleanupLocalStackTable()
})
```

**Creates**: MediaDownloader table with all GSIs (gsi1/userResources, gsi2/fileUsers, gsi3/deviceUsers)

### Testing Collections

```typescript
import {collections} from '../../../src/entities/Collections'

test('userResources collection', async () => {
  // Create test data
  await Users.create({userId: 'user-1', appleDeviceIdentifier: 'apple-1'}).go()
  await Files.create({fileId: 'file-1', status: 'Downloaded', url: 'https://...'}).go()
  await UserFiles.create({userId: 'user-1', fileId: 'file-1'}).go()

  // Query collection (JOIN-like operation)
  const result = await collections.userResources({userId: 'user-1'}).go()

  // Validate single-table design
  expect(result.data.Users).toHaveLength(1)
  expect(result.data.Files).toHaveLength(1)
  expect(result.data.UserFiles).toHaveLength(1)
})
```

**See**: [ElectroDB Testing Patterns](../Testing/ElectroDB-Testing-Patterns.md) for comprehensive examples

## Common Issues

| Issue | Solution |
|-------|----------|
| Connection refused | Ensure LocalStack is running |
| Service not available | Check SERVICES env var |
| Credentials error | Use 'test' for both key and secret |
| Region mismatch | Use us-west-2 consistently |

## Best Practices

1. **Use vendor wrappers** - Automatic LocalStack detection
2. **Set UseLocalstack=true** - Enable LocalStack mode
3. **Clean state between tests** - Reset services in afterEach
4. **Mock external services** - Don't call real APIs from LocalStack

## Related Patterns

- [Vendor Wrappers](../AWS/SDK-Encapsulation-Policy.md) - AWS SDK encapsulation
- [Integration Testing](../Testing/Integration-Testing.md) - Test strategies
- [Jest ESM Mocking](../Testing/Jest-ESM-Mocking-Strategy.md) - Mocking patterns

---

*Use LocalStack for local AWS testing. Vendor wrappers automatically detect LocalStack mode.*
</file>

<file path="docs/wiki/Meta/Documentation-Patterns.md">
# Documentation Patterns

## Quick Reference
- **When to use**: Organizing documentation across projects
- **Enforcement**: Recommended - maintains consistency
- **Impact if violated**: Low - may have disorganized docs

## Passthrough File Pattern

A **passthrough file** references a canonical source, allowing multiple tools to access the same content.

```
Canonical Source (AGENTS.md)
       ‚Üì
   References
       ‚Üì
Passthrough Files (CLAUDE.md, GEMINI.md)
```

### Benefits
1. **Single Source of Truth** - Content defined once
2. **Tool Compatibility** - Works with multiple AI tools
3. **No Duplication** - Eliminates sync issues
4. **Easy Maintenance** - Update one file

## Passthrough Examples

### Claude Code Passthrough
**File**: `CLAUDE.md`
```markdown
@AGENTS.md
```

Claude Code reads CLAUDE.md, sees the reference, and loads AGENTS.md content.

### Gemini Code Assist Passthrough
**File**: `GEMINI.md`
```markdown
# See AGENTS.md

This project uses AGENTS.md as the single source of truth for AI coding assistant context.

Please see AGENTS.md in the repository root for comprehensive project documentation and guidelines.
```

### Documentation Wiki Passthrough
```markdown
# [Topic] Documentation

For comprehensive documentation on testing patterns, see the wiki:

- [Jest ESM Mocking Strategy](../Testing/Jest-ESM-Mocking-Strategy.md)
- [Lazy Initialization Pattern](../Testing/Lazy-Initialization-Pattern.md)

This keeps documentation centralized and avoids duplication.
```

## Wiki Organization

### Git-Based Wiki
Store wiki content in main repository for version control:

```
docs/
‚îî‚îÄ‚îÄ wiki/
    ‚îú‚îÄ‚îÄ Home.md
    ‚îú‚îÄ‚îÄ Conventions/
    ‚îÇ   ‚îú‚îÄ‚îÄ Naming-Conventions.md
    ‚îÇ   ‚îî‚îÄ‚îÄ Git-Workflow.md
    ‚îú‚îÄ‚îÄ TypeScript/
    ‚îÇ   ‚îî‚îÄ‚îÄ Lambda-Function-Patterns.md
    ‚îî‚îÄ‚îÄ Meta/
        ‚îî‚îÄ‚îÄ Documentation-Patterns.md
```

### Auto-Sync to GitHub Wiki
Use GitHub Actions to automatically sync:

```
docs/wiki/ (Git)  ‚Üí  GitHub Actions  ‚Üí  GitHub Wiki (Public)
Source of Truth      Auto-sync on       Beautiful UI
PR reviewed          push to master     Search enabled
```

## Page Template

### Standard Convention Page
```markdown
# [Convention Name]

## Quick Reference
- **When to use**: [One-line description]
- **Enforcement**: [Zero-tolerance/Required/Recommended]
- **Impact if violated**: [Critical/High/Medium/Low]

## The Rule
[Clear, concise statement of the convention]

## Examples
### ‚úÖ Correct
[Good example with code]

### ‚ùå Incorrect
[Bad example with code]

## Rationale
[Why this convention exists]

## Enforcement
[How to check/automate compliance]

## Related Patterns
- Link to related convention 1
- Link to related convention 2
```

### Why This Template?
1. **Quick Reference** - Developers get essence immediately
2. **Clear Rules** - No ambiguity about what's required
3. **Learning by Example** - Visual comparison of good/bad
4. **Context** - Rationale explains the "why"
5. **Actionable** - Enforcement section shows how to apply
6. **Discoverable** - Related patterns aid navigation

## Reference Pattern

### From Application Code
```typescript
/**
 * Validates user credentials
 *
 * See: docs/wiki/TypeScript/TypeScript-Error-Handling.md
 */
function validateCredentials(username: string, password: string) {
  // Implementation
}
```

### From AGENTS.md
```markdown
## Wiki Conventions to Follow

**BEFORE WRITING ANY CODE, READ THE APPLICABLE GUIDE:**

### Core Conventions
- [Git Workflow](docs/wiki/Conventions/Git-Workflow.md) - NO AI attribution
- [Naming](docs/wiki/Conventions/Naming-Conventions.md) - camelCase rules

### Language-Specific
- [Lambda](docs/wiki/TypeScript/Lambda-Function-Patterns.md)
- [Testing](docs/wiki/Testing/Jest-ESM-Mocking-Strategy.md)
- [AWS SDK](docs/wiki/AWS/SDK-Encapsulation-Policy.md) - ZERO tolerance
```

## Multi-Project Documentation

### Shared Wiki Pattern
```
shared-conventions/
‚îî‚îÄ‚îÄ wiki/
    ‚îú‚îÄ‚îÄ TypeScript/
    ‚îú‚îÄ‚îÄ Testing/
    ‚îî‚îÄ‚îÄ AWS/

project-a/
‚îú‚îÄ‚îÄ AGENTS.md (references shared wiki)
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ conventions-tracking.md (project-specific)
```

### Symlink Pattern
```bash
cd docs
ln -s ../../shared-conventions/wiki wiki
```

## Link Management

### Internal Wiki Links
```markdown
See also [Naming Conventions](../Conventions/Naming-Conventions.md)
See also [Lambda Patterns](../TypeScript/Lambda-Function-Patterns.md)
```

### External Links
```markdown
See [Conventional Commits](https://www.conventionalcommits.org/)
See [Issue Tracker](https://github.com/user/repo/issues)
```

## Best Practices

### Do's
‚úÖ Use passthrough files for tool compatibility
‚úÖ Maintain single source of truth
‚úÖ Reference wiki instead of duplicating
‚úÖ Auto-sync to GitHub Wiki
‚úÖ Version control all documentation
‚úÖ Link between related patterns
‚úÖ Use standard page template

### Don'ts
‚ùå Duplicate content across files
‚ùå Edit GitHub Wiki directly (edit docs/wiki/)
‚ùå Create documentation silos
‚ùå Skip cross-references
‚ùå Leave broken links
‚ùå Create tool-specific forks

## Related Documentation
- [AI Tool Context Files](AI-Tool-Context-Files.md) - AGENTS.md and passthroughs
- [Convention Capture System](Convention-Capture-System.md) - How patterns are captured
- [Working with AI Assistants](Working-with-AI-Assistants.md) - Effective collaboration

---

*Use passthrough files to maintain compatibility while keeping a single source of truth. Auto-sync documentation to make it accessible in multiple formats.*
</file>

<file path="docs/wiki/Home.md">
# Development Conventions Wiki

Welcome to the centralized development conventions wiki. This wiki contains universal patterns, methodologies, and best practices that apply across TypeScript/AWS projects.

## Quick Start

- **New to the wiki?** Start with [Getting Started](Getting-Started.md)
- **Looking for specific patterns?** Use the navigation below
- **Contributing?** See [Working with AI Assistants](Meta/Working-with-AI-Assistants.md)

## Navigation

### üìã Conventions
Core development conventions that apply universally:

- [Naming Conventions](Conventions/Naming-Conventions.md) - camelCase, PascalCase, SCREAMING_SNAKE_CASE rules
- [Git Workflow](Conventions/Git-Workflow.md) - Commit messages, NO AI attribution
- [Code Comments](Conventions/Code-Comments.md) - Git as source of truth principle
- [Import Organization](Conventions/Import-Organization.md) - ES modules, destructuring patterns

### üéØ TypeScript
TypeScript-specific patterns and best practices:

- [Lambda Function Patterns](TypeScript/Lambda-Function-Patterns.md) - Handler organization
- [Error Handling](TypeScript/TypeScript-Error-Handling.md) - API Gateway vs event-driven
- [Type Definitions](TypeScript/Type-Definitions.md) - Where to put types
- [Module Best Practices](TypeScript/Module-Best-Practices.md) - Export patterns

### üß™ Testing
Comprehensive testing strategies and patterns:

- [Jest ESM Mocking Strategy](Testing/Jest-ESM-Mocking-Strategy.md) - Transitive dependencies solution
- [Mock Type Annotations](Testing/Mock-Type-Annotations.md) - Specific vs generic types
- [Lazy Initialization Pattern](Testing/Lazy-Initialization-Pattern.md) - Defer SDK clients
- [Coverage Philosophy](Testing/Coverage-Philosophy.md) - Test YOUR code principle
- [Integration Testing](Testing/Integration-Testing.md) - LocalStack patterns

### ‚òÅÔ∏è AWS
AWS-specific patterns and policies:

- [SDK Encapsulation Policy](AWS/SDK-Encapsulation-Policy.md) - **ZERO-TOLERANCE** vendor wrapper pattern
- [Lambda Environment Variables](AWS/Lambda-Environment-Variables.md) - Naming conventions
- [CloudWatch Logging](AWS/CloudWatch-Logging.md) - Logging patterns
- [X-Ray Integration](AWS/X-Ray-Integration.md) - Tracing patterns

### üìú Bash
Shell scripting conventions:

- [Variable Naming](Bash/Variable-Naming.md) - snake_case vs UPPER_CASE
- [Directory Resolution](Bash/Directory-Resolution.md) - BASH_SOURCE patterns
- [User Output Formatting](Bash/User-Output-Formatting.md) - Colors and feedback
- [Error Handling](Bash/Bash-Error-Handling.md) - set -e, exit codes

### üèóÔ∏è Infrastructure
Infrastructure as Code patterns:

- [Resource Naming](Infrastructure/Resource-Naming.md) - PascalCase for AWS
- [File Organization](Infrastructure/File-Organization.md) - Service grouping
- [Environment Variables](Infrastructure/Environment-Variables.md) - Cross-stack consistency

### üí° Methodologies
Development methodologies and philosophies:

- [Convention Over Configuration](Methodologies/Convention-Over-Configuration.md) - Core philosophy
- [Library Migration Checklist](Methodologies/Library-Migration-Checklist.md) - Step-by-step process
- [Dependabot Resolution](Methodologies/Dependabot-Resolution.md) - Automated updates
- [Production Debugging](Methodologies/Production-Debugging.md) - Troubleshooting guide

### üîÆ Meta
Meta-documentation about the documentation system itself:

- [Working with AI Assistants](Meta/Working-with-AI-Assistants.md) - Effective AI collaboration
- [Convention Capture System](Meta/Convention-Capture-System.md) - How conventions are captured
- [Emerging Conventions](Meta/Emerging-Conventions.md) - Live append-only log
- [AI Tool Context Files](Meta/AI-Tool-Context-Files.md) - AGENTS.md, CLAUDE.md standards
- [Documentation Patterns](Meta/Documentation-Patterns.md) - Passthrough files, organization

## Key Principles

### üö® Zero-Tolerance Rules
These patterns have **ZERO exceptions**:
- [AWS SDK Encapsulation](AWS/SDK-Encapsulation-Policy.md) - NEVER import AWS SDK directly
- [No AI Attribution](Conventions/Git-Workflow.md) - NEVER include AI references in commits
- [Git as Source of Truth](Conventions/Code-Comments.md) - NEVER explain removed code in comments

### üìà Convention Evolution
Conventions evolve through:
1. **Detection** - Patterns emerge during development
2. **Capture** - [Convention Capture System](Meta/Convention-Capture-System.md) preserves them
3. **Documentation** - Added to this wiki
4. **Enforcement** - Automated checks where possible
5. **Refinement** - Improved based on experience

## Using This Wiki

### For AI Assistants
- Start each session by reviewing [Convention Capture System](Meta/Convention-Capture-System.md)
- Reference wiki pages from AGENTS.md using relative paths
- Flag new conventions using the detection system
- Update [Emerging Conventions](Meta/Emerging-Conventions.md) in real-time

### For Developers
- Browse by category using navigation above
- Search for specific patterns using your IDE
- Contribute new patterns via pull requests
- Report issues or gaps in documentation

### For New Projects
1. Copy AGENTS.md template with Convention Capture instructions
2. Reference this wiki for universal patterns
3. Build project-specific conventions in `docs/conventions-tracking.md`
4. Use passthrough files (CLAUDE.md, GEMINI.md) for tool compatibility

## Repository Structure

This wiki is stored in the main repository under `docs/wiki/`:
- **Version controlled** alongside code
- **PR reviewed** for quality
- **Offline accessible** via git
- **Auto-synced** to GitHub Wiki for web viewing

## Contributing

To add or update conventions:
1. Follow the [page template](Meta/Documentation-Patterns.md)
2. Include clear examples (‚úÖ Correct / ‚ùå Incorrect)
3. Explain rationale and benefits
4. Add enforcement mechanisms where possible
5. Update navigation in this Home.md

## Quick Reference

| Pattern | Category | Enforcement |
|---------|----------|-------------|
| AWS SDK Encapsulation | AWS | Zero-tolerance |
| No AI in Commits | Git | Zero-tolerance |
| camelCase for variables | Naming | Required |
| Jest transitive mocking | Testing | Required |
| Git as source of truth | Comments | Required |

---

*This wiki represents accumulated institutional knowledge from development across multiple projects. It serves as the reference implementation for development conventions and is continuously updated through the Convention Capture System.*
</file>

<file path="src/entities/Accounts.ts">
import {documentClient, Entity} from '#lib/vendor/ElectroDB/entity'
/**
 * ElectroDB entity schema for Better Auth accounts (OAuth provider links).
 * Manages OAuth provider connections for users (Apple, Google, GitHub, etc.).
 *
 * Better Auth Account Schema:
 * - id: unique account identifier
 * - userId: reference to user who owns this account
 * - providerId: OAuth provider name ('apple', 'google', 'github', etc.)
 * - providerAccountId: user ID from the provider (e.g., Apple User ID)
 * - accessToken: OAuth access token from provider
 * - refreshToken: OAuth refresh token from provider
 * - expiresAt: token expiration timestamp
 * - scope: OAuth scopes granted
 * - tokenType: OAuth token type (usually 'Bearer')
 * - idToken: OIDC ID token if available
 */
‚ãÆ----
// Type exports for use in application code
export type AccountItem = ReturnType<typeof Accounts.parse>
export type CreateAccountInput = Parameters<typeof Accounts.create>[0]
export type UpdateAccountInput = Parameters<typeof Accounts.update>[0]
</file>

<file path="src/mcp/validation/rules/aws-sdk-encapsulation.ts">
/**
 * AWS SDK Encapsulation Rule
 * CRITICAL: Never import AWS SDK directly - use lib/vendor/AWS/ wrappers
 *
 * This is a zero-tolerance rule per project conventions.
 */
import type {SourceFile} from 'ts-morph'
import {SyntaxKind} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Vendor packages that should never be imported directly
 */
‚ãÆ----
// AWS SDK v3
‚ãÆ----
'aws-sdk', // v2
// AWS Lambda Powertools
‚ãÆ----
/**
 * Suggested vendor wrapper mappings
 */
‚ãÆ----
// AWS SDK
‚ãÆ----
// AWS Lambda Powertools
‚ãÆ----
function getSuggestion(moduleSpecifier: string): string
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// Skip vendor directories - that's where direct imports are allowed
‚ãÆ----
// Check all import declarations
‚ãÆ----
// Check if this is a forbidden AWS SDK import
‚ãÆ----
// Also check dynamic imports
</file>

<file path="src/mcp/validation/rules/batch-retry.ts">
/**
 * Batch Retry Rule
 * HIGH: Detect batch operations without retry handling
 *
 * ElectroDB batch operations can return unprocessed items.
 * This rule enforces using retryUnprocessed() wrapper for reliability.
 */
import type {SourceFile} from 'ts-morph'
import {SyntaxKind} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Batch operation patterns that need retry handling
 */
‚ãÆ----
/**
 * Allowed wrapper functions that handle retries
 */
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// Skip the retry utility itself
‚ãÆ----
// Find all call expressions
‚ãÆ----
// Check for batch operations
‚ãÆ----
// Check if it's wrapped in a retry function
‚ãÆ----
// Look at parent call to see if this is wrapped
‚ãÆ----
// Also check if the call itself starts with the wrapper
‚ãÆ----
// Also check if retryUnprocessed is called on the same line/nearby
</file>

<file path="src/mcp/validation/rules/cascade-safety.ts">
/**
 * Cascade Safety Rule
 * CRITICAL: Detect unsafe cascade deletion patterns
 *
 * This enforces:
 * 1. Use Promise.allSettled instead of Promise.all for cascade operations
 * 2. Delete child entities before parent entities
 */
import type {SourceFile} from 'ts-morph'
import {SyntaxKind} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Entity deletion patterns that indicate cascade operations
 */
‚ãÆ----
/**
 * Entity hierarchy for checking parent-child relationships
 * Children should be deleted before parents
 */
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// filePath used for context in error messages
‚ãÆ----
// Find all Promise.all calls
‚ãÆ----
// Check for Promise.all
‚ãÆ----
// Check if the array contains delete operations
‚ãÆ----
// Check for entity deletion order violations
// Look for await expressions that might have incorrect ordering
‚ãÆ----
// Check if this is a delete operation on a known entity
‚ãÆ----
// Check for parent entity deletion
‚ãÆ----
// Check for child entity deletions
‚ãÆ----
// Analyze deletion sequence for order violations
‚ãÆ----
// Check if this is a parent entity
‚ãÆ----
// Check if any child deletions come AFTER this parent deletion
</file>

<file path="src/mcp/validation/rules/mock-formatting.ts">
/**
 * Mock Formatting Rule
 * MEDIUM: Detect chained mock return value patterns
 *
 * This rule enforces using separate statements for sequential
 * mockResolvedValueOnce/mockReturnValueOnce calls for better readability.
 */
import type {SourceFile} from 'ts-morph'
import {SyntaxKind} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Mock methods that are often chained
 */
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// filePath available for context if needed
‚ãÆ----
// Find all call expressions
‚ãÆ----
// Count how many chainable methods appear in this expression
‚ãÆ----
// Count occurrences of each method
‚ãÆ----
// If 2+ chainable methods, this is a chained pattern
‚ãÆ----
// Verify this is actually a chain (methods called on same line/expression)
</file>

<file path="src/mcp/validation/rules/naming-conventions.ts">
/**
 * Naming Conventions Rule
 * HIGH: Validates TypeScript type names follow project conventions
 *
 * Pattern Standards:
 * - Domain entities: Simple nouns (User, File, Device)
 * - Entity items: *Item suffix (UserItem, FileItem)
 * - Mutation inputs: Create*Input, Update*Input
 * - Request payloads: *Input suffix
 * - Response types: *Response suffix
 * - Relationship types: Simple nouns (UserDevice, UserFile)
 * - Enums: PascalCase values
 *
 * @see docs/wiki/Conventions/Naming-Conventions.md
 */
import type {SourceFile} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
// Forbidden prefixes that indicate old patterns
‚ãÆ----
// Valid enum value patterns (PascalCase)
‚ãÆ----
/**
 * Check if a string is in PascalCase
 */
function isPascalCase(str: string): boolean
/**
 * Convert to suggested PascalCase
 */
function toPascalCase(str: string): string
‚ãÆ----
// Handle snake_case
‚ãÆ----
// Handle already lowercase
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// Check type aliases
‚ãÆ----
// Check for forbidden prefixes
‚ãÆ----
// Only flag if it's a pattern like DynamoDBFile, not just "DynamoDB"
‚ãÆ----
// Check interfaces
‚ãÆ----
// Check for forbidden prefixes
‚ãÆ----
// Check enums for PascalCase values
‚ãÆ----
// Check member name is PascalCase
‚ãÆ----
// Check string value is PascalCase if present
‚ãÆ----
// Check for property naming (camelCase)
const checkProperties = (node:
‚ãÆ----
// Check for snake_case (common in external APIs)
‚ãÆ----
// Check ID naming pattern
‚ãÆ----
// Check interface properties
</file>

<file path="src/mcp/validation/rules/response-enum.ts">
/**
 * Response Enum Rule
 * MEDIUM: Detect magic strings in API responses
 *
 * This rule enforces using ResponseStatus enum instead of
 * string literals for consistent API response status values.
 */
import type {SourceFile} from 'ts-morph'
import {SyntaxKind} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Magic strings that should use ResponseStatus enum
 */
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// filePath available for context if needed
‚ãÆ----
// Find all string literals
‚ãÆ----
// Check if this is a magic status string
‚ãÆ----
// Check context - is this in a response object?
‚ãÆ----
// Check if part of a property assignment like { status: 'success' }
‚ãÆ----
// Only flag if the property is 'status' or similar
‚ãÆ----
// Check if in response() call
‚ãÆ----
// Look for status property in the response data
‚ãÆ----
function capitalize(str: string): string
</file>

<file path="src/mcp/validation/rules/scan-pagination.ts">
/**
 * Scan Pagination Rule
 * HIGH: Detect unpaginated DynamoDB scan operations
 *
 * DynamoDB scans return at most 1MB of data per request.
 * This rule enforces using scanAllPages() for complete results.
 */
import type {SourceFile} from 'ts-morph'
import {SyntaxKind} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Pagination wrapper functions
 */
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// Skip the pagination utility itself
‚ãÆ----
// Find all property access expressions that might be scans
‚ãÆ----
// Check for .scan property access
‚ãÆ----
// Check if this is followed by .go() (indicating execution)
‚ãÆ----
// Check if wrapped in pagination helper
</file>

<file path="src/mcp/validation/rules/types-location.ts">
/**
 * Type Location Rule
 * HIGH: Exported type definitions should be in src/types/ directory
 *
 * This rule enforces separation of concerns by ensuring types are centralized
 * in the types directory, making them discoverable and maintainable.
 *
 * @see docs/wiki/TypeScript/Type-Definitions.md
 */
import type {SourceFile} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Determine the suggested target file in src/types/ based on the source file
 */
function getSuggestedTypeFile(filePath: string): string
‚ãÆ----
// Map common utility files to their type files
‚ãÆ----
// For lambdas, suggest a domain-specific type file
‚ãÆ----
// Default suggestion
‚ãÆ----
'src/types/**/*.ts', // Canonical location for types
'src/entities/**/*.ts', // Entity-derived types allowed
'src/mcp/**/*.ts', // MCP types are self-contained
'**/*.test.ts', // Test files
'test/**/*.ts', // Test files
'src/lib/vendor/**/*.ts' // Vendor wrappers may need internal types
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// Check for excluded patterns (double-check in case appliesTo/excludes overlap)
‚ãÆ----
// Find exported type aliases
‚ãÆ----
// Find exported interfaces
‚ãÆ----
// Find exported enums
</file>

<file path="src/types/better-auth.d.ts">
/**
 * Better Auth type utilities
 *
 * These types are extracted from Better Auth's API for use in tests and type-safe code.
 */
import type {auth} from '#lib/vendor/BetterAuth/config'
/**
 * Parameters for Better Auth's signInSocial API method.
 * Extracted from the actual Better Auth instance to ensure type safety.
 */
export type SignInSocialParams = Parameters<typeof auth.api.signInSocial>[0]
/**
 * Result from Better Auth's signInSocial API method.
 * Used for mocking in tests.
 */
export interface SignInSocialResult {
  user: {id: string; email: string; name?: string; createdAt: string | Date; emailVerified?: boolean}
  session: {id: string; expiresAt: number | Date}
  token: string
}
</file>

<file path="src/types/infrastructure-types.d.ts">
import {
  APIGatewayProxyEventHeaders,
  APIGatewayProxyEventMultiValueHeaders,
  APIGatewayProxyEventMultiValueQueryStringParameters,
  APIGatewayProxyEventPathParameters,
  APIGatewayProxyEventQueryStringParameters,
  APIGatewayProxyEventStageVariables
} from 'aws-lambda/trigger/api-gateway-proxy'
import {APIGatewayEventIdentity} from 'aws-lambda/common/api-gateway'
// TODO: This has to be a custom event, because the actual event does NOT contain the requestContext.identity.clientCert field
export interface CustomAPIGatewayRequestAuthorizerEvent {
  requestContext: {
    accountId: string
    apiId: string
    // This one is a bit confusing: it is not actually present in authorizer calls
    // and proxy calls without an authorizer. We model this by allowing undefined in the type,
    // since it ends up the same and avoids breaking users that are testing the property.
    // This lets us allow parameterizing the authorizer for proxy events that know what authorizer
    // context values they have.
    authorizer: {integrationLatency: number; principalId: string}
    connectedAt?: number | undefined
    connectionId?: string | undefined
    domainName?: string | undefined
    domainPrefix?: string | undefined
    eventType?: string | undefined
    extendedRequestId?: string | undefined
    protocol: string
    httpMethod: string
    identity: Omit<APIGatewayEventIdentity, 'clientCert'>
    messageDirection?: string | undefined
    messageId?: string | null | undefined
    path: string
    stage: string
    requestId: string
    requestTime?: string | undefined
    requestTimeEpoch: number
    resourceId: string
    resourcePath: string
    routeKey?: string | undefined
  }
  body: string | null
  headers: APIGatewayProxyEventHeaders
  multiValueHeaders: APIGatewayProxyEventMultiValueHeaders
  httpMethod: string
  isBase64Encoded: boolean
  path: string
  pathParameters: APIGatewayProxyEventPathParameters | null
  queryStringParameters: APIGatewayProxyEventQueryStringParameters | null
  multiValueQueryStringParameters: APIGatewayProxyEventMultiValueQueryStringParameters | null
  stageVariables: APIGatewayProxyEventStageVariables | null
  resource: string
}
‚ãÆ----
// This one is a bit confusing: it is not actually present in authorizer calls
// and proxy calls without an authorizer. We model this by allowing undefined in the type,
// since it ends up the same and avoids breaking users that are testing the property.
// This lets us allow parameterizing the authorizer for proxy events that know what authorizer
// context values they have.
</file>

<file path="src/types/util.ts">
/**
 * Utility Types
 *
 * Shared type definitions for utility functions across the codebase.
 *
 * @see src/util/retry.ts - Retry utilities
 * @see src/util/better-auth-helpers.ts - Authentication helpers
 * @see src/util/lambda-helpers.ts - Lambda handler utilities
 */
import {UserStatus} from './enums'
/**
 * Configuration options for retry behavior with exponential backoff
 */
export interface RetryConfig {
  maxRetries?: number
  initialDelayMs?: number
  multiplier?: number
  maxDelayMs?: number
}
/**
 * Session payload extracted from Better Auth token
 */
export interface SessionPayload {
  userId: string
  sessionId: string
  expiresAt: number
}
/**
 * User details extracted from API Gateway event
 */
export interface UserEventDetails {
  userId?: string
  userStatus: UserStatus
}
/**
 * Input for publishing CloudWatch metrics
 */
export type MetricInput = {name: string; value: number; unit?: string; dimensions?: {Name: string; Value: string}[]}
</file>

<file path="src/types/youtube.ts">
/**
 * yt-dlp video information types
 */
export interface YtDlpVideoInfo {
  id: string
  title: string
  formats: YtDlpFormat[]
  thumbnail: string
  duration: number
  description?: string
  uploader?: string
  upload_date?: string
  view_count?: number
  filesize?: number
  /** Unix timestamp when video becomes available (for scheduled content) */
  release_timestamp?: number
  /** Whether this is a livestream */
  is_live?: boolean
  /** Current live status */
  live_status?: 'is_live' | 'is_upcoming' | 'was_live' | 'not_live'
  /** Video availability status */
  availability?: 'public' | 'unlisted' | 'private' | 'needs_auth' | 'subscriber_only'
}
‚ãÆ----
/** Unix timestamp when video becomes available (for scheduled content) */
‚ãÆ----
/** Whether this is a livestream */
‚ãÆ----
/** Current live status */
‚ãÆ----
/** Video availability status */
‚ãÆ----
export interface YtDlpFormat {
  format_id: string
  url: string
  ext: string
  filesize?: number
  width?: number
  height?: number
  fps?: number
  vcodec?: string
  acodec?: string
  abr?: number
  vbr?: number
  tbr?: number
  /** Audio file extension - 'none' for HLS/DASH where audio is in separate stream */
  audio_ext?: string
  /** Streaming protocol - e.g., 'https', 'm3u8_native', 'http_dash_segments' */
  protocol?: string
}
‚ãÆ----
/** Audio file extension - 'none' for HLS/DASH where audio is in separate stream */
‚ãÆ----
/** Streaming protocol - e.g., 'https', 'm3u8_native', 'http_dash_segments' */
</file>

<file path="terraform/cloudwatch.tf">
# CloudWatch Dashboard for Media Downloader
# Provides visibility into Lambda performance, storage, and API metrics
# See: https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/issues/147

locals {
  lambda_functions = [
    "ApiGatewayAuthorizer",
    "CloudfrontMiddleware",
    "FileCoordinator",
    "ListFiles",
    "LogClientEvent",
    "LoginUser",
    "PruneDevices",
    "RefreshToken",
    "RegisterDevice",
    "RegisterUser",
    "S3ObjectCreated",
    "SendPushNotification",
    "StartFileUpload",
    "UserDelete",
    "UserSubscribe",
    "WebhookFeedly"
  ]

  # Split lambdas into groups for CloudWatch alarms (max 10 metrics per alarm)
  lambda_functions_api = [
    "ApiGatewayAuthorizer",
    "ListFiles",
    "LogClientEvent",
    "LoginUser",
    "RefreshToken",
    "RegisterDevice",
    "RegisterUser",
    "UserDelete",
    "UserSubscribe",
    "WebhookFeedly"
  ]

  lambda_functions_background = [
    "CloudfrontMiddleware",
    "FileCoordinator",
    "PruneDevices",
    "S3ObjectCreated",
    "SendPushNotification",
    "StartFileUpload"
  ]
}

resource "aws_cloudwatch_dashboard" "main" {
  dashboard_name = "MediaDownloader"

  dashboard_body = jsonencode({
    widgets = [
      # Row 1: Lambda Invocations
      {
        type   = "metric"
        x      = 0
        y      = 0
        width  = 12
        height = 6
        properties = {
          title   = "Lambda Invocations"
          region  = data.aws_region.current.id
          stat    = "Sum"
          period  = 300
          view    = "timeSeries"
          stacked = true
          metrics = [for fn in local.lambda_functions : ["AWS/Lambda", "Invocations", "FunctionName", fn]]
        }
      },
      # Row 1: Lambda Errors
      {
        type   = "metric"
        x      = 12
        y      = 0
        width  = 12
        height = 6
        properties = {
          title   = "Lambda Errors"
          region  = data.aws_region.current.id
          stat    = "Sum"
          period  = 300
          view    = "timeSeries"
          metrics = [for fn in local.lambda_functions : ["AWS/Lambda", "Errors", "FunctionName", fn]]
          annotations = {
            horizontal = [
              {
                label = "Error Threshold"
                value = 5
                color = "#d62728"
              }
            ]
          }
        }
      },
      # Row 2: Lambda Duration (P50/P95)
      {
        type   = "metric"
        x      = 0
        y      = 6
        width  = 8
        height = 6
        properties = {
          title  = "Lambda Duration (ms)"
          region = data.aws_region.current.id
          period = 300
          view   = "timeSeries"
          metrics = concat(
            [for fn in local.lambda_functions : ["AWS/Lambda", "Duration", "FunctionName", fn, { stat = "p50", label = "${fn} p50" }]],
            [for fn in local.lambda_functions : ["AWS/Lambda", "Duration", "FunctionName", fn, { stat = "p95", label = "${fn} p95" }]]
          )
        }
      },
      # Row 2: Cold Starts (Init Duration)
      {
        type   = "metric"
        x      = 8
        y      = 6
        width  = 8
        height = 6
        properties = {
          title   = "Lambda Cold Starts (Init Duration)"
          region  = data.aws_region.current.id
          stat    = "Average"
          period  = 300
          view    = "timeSeries"
          metrics = [for fn in local.lambda_functions : ["AWS/Lambda", "InitDuration", "FunctionName", fn]]
        }
      },
      # Row 2: Throttles
      {
        type   = "metric"
        x      = 16
        y      = 6
        width  = 8
        height = 6
        properties = {
          title   = "Lambda Throttles (should be 0)"
          region  = data.aws_region.current.id
          stat    = "Sum"
          period  = 300
          view    = "timeSeries"
          metrics = [for fn in local.lambda_functions : ["AWS/Lambda", "Throttles", "FunctionName", fn]]
          annotations = {
            horizontal = [
              {
                label = "Any throttle is bad"
                value = 1
                color = "#d62728"
              }
            ]
          }
        }
      },
      # Row 3: DynamoDB Capacity
      {
        type   = "metric"
        x      = 0
        y      = 12
        width  = 8
        height = 6
        properties = {
          title  = "DynamoDB Consumed Capacity"
          region = data.aws_region.current.id
          stat   = "Sum"
          period = 300
          view   = "timeSeries"
          metrics = [
            ["AWS/DynamoDB", "ConsumedReadCapacityUnits", "TableName", aws_dynamodb_table.MediaDownloader.name],
            ["AWS/DynamoDB", "ConsumedWriteCapacityUnits", "TableName", aws_dynamodb_table.MediaDownloader.name]
          ]
        }
      },
      # Row 3: S3 Storage
      {
        type   = "metric"
        x      = 8
        y      = 12
        width  = 8
        height = 6
        properties = {
          title  = "S3 Storage (Bytes)"
          region = data.aws_region.current.id
          stat   = "Average"
          period = 86400
          view   = "timeSeries"
          metrics = [
            ["AWS/S3", "BucketSizeBytes", "BucketName", aws_s3_bucket.Files.id, "StorageType", "StandardStorage"]
          ]
        }
      },
      # Row 3: S3 Object Count
      {
        type   = "metric"
        x      = 16
        y      = 12
        width  = 8
        height = 6
        properties = {
          title  = "S3 Object Count"
          region = data.aws_region.current.id
          stat   = "Average"
          period = 86400
          view   = "timeSeries"
          metrics = [
            ["AWS/S3", "NumberOfObjects", "BucketName", aws_s3_bucket.Files.id, "StorageType", "AllStorageTypes"]
          ]
        }
      },
      # Row 4: API Gateway Requests
      {
        type   = "metric"
        x      = 0
        y      = 18
        width  = 12
        height = 6
        properties = {
          title  = "API Gateway Requests"
          region = data.aws_region.current.id
          stat   = "Sum"
          period = 300
          view   = "timeSeries"
          metrics = [
            ["AWS/ApiGateway", "Count", "ApiName", aws_api_gateway_rest_api.Main.name],
            ["AWS/ApiGateway", "4XXError", "ApiName", aws_api_gateway_rest_api.Main.name],
            ["AWS/ApiGateway", "5XXError", "ApiName", aws_api_gateway_rest_api.Main.name]
          ]
        }
      },
      # Row 4: API Gateway Latency
      {
        type   = "metric"
        x      = 12
        y      = 18
        width  = 12
        height = 6
        properties = {
          title  = "API Gateway Latency (ms)"
          region = data.aws_region.current.id
          period = 300
          view   = "timeSeries"
          metrics = [
            ["AWS/ApiGateway", "Latency", "ApiName", aws_api_gateway_rest_api.Main.name, { stat = "p50", label = "p50" }],
            ["AWS/ApiGateway", "Latency", "ApiName", aws_api_gateway_rest_api.Main.name, { stat = "p95", label = "p95" }],
            ["AWS/ApiGateway", "IntegrationLatency", "ApiName", aws_api_gateway_rest_api.Main.name, { stat = "p50", label = "Integration p50" }]
          ]
        }
      },
      # Row 5: SQS Queue Depth
      {
        type   = "metric"
        x      = 0
        y      = 24
        width  = 12
        height = 6
        properties = {
          title  = "SQS Queue Depth"
          region = data.aws_region.current.id
          stat   = "Average"
          period = 300
          view   = "timeSeries"
          metrics = [
            ["AWS/SQS", "ApproximateNumberOfMessagesVisible", "QueueName", aws_sqs_queue.SendPushNotification.name, { label = "Main Queue" }],
            ["AWS/SQS", "ApproximateNumberOfMessagesVisible", "QueueName", aws_sqs_queue.SendPushNotificationDLQ.name, { label = "DLQ", color = "#d62728" }]
          ]
        }
      },
      # Row 5: SQS Message Age
      {
        type   = "metric"
        x      = 12
        y      = 24
        width  = 12
        height = 6
        properties = {
          title  = "SQS Message Age (seconds)"
          region = data.aws_region.current.id
          stat   = "Maximum"
          period = 300
          view   = "timeSeries"
          metrics = [
            ["AWS/SQS", "ApproximateAgeOfOldestMessage", "QueueName", aws_sqs_queue.SendPushNotification.name]
          ]
          annotations = {
            horizontal = [
              {
                label = "1 hour threshold"
                value = 3600
                color = "#d62728"
              }
            ]
          }
        }
      }
    ]
  })
}

output "cloudwatch_dashboard_url" {
  description = "URL to the CloudWatch dashboard"
  value       = "https://${data.aws_region.current.id}.console.aws.amazon.com/cloudwatch/home?region=${data.aws_region.current.id}#dashboards:name=${aws_cloudwatch_dashboard.main.dashboard_name}"
}

# =============================================================================
# CloudWatch Alarms
# Note: SNS notification actions deferred - add alarm_actions when SNS configured
# =============================================================================

# Lambda Errors Alarm (API) - triggers when total errors across API Lambdas exceed threshold
resource "aws_cloudwatch_metric_alarm" "lambda_errors_api" {
  alarm_name          = "MediaDownloader-Lambda-Errors-API"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  threshold           = 5
  alarm_description   = "Lambda errors exceed threshold across API functions"
  treat_missing_data  = "notBreaching"

  metric_query {
    id          = "errors"
    expression  = join(" + ", [for fn in local.lambda_functions_api : "m_${replace(fn, "-", "_")}"])
    label       = "Total Lambda Errors (API)"
    return_data = true
  }

  dynamic "metric_query" {
    for_each = local.lambda_functions_api
    content {
      id = "m_${replace(metric_query.value, "-", "_")}"
      metric {
        metric_name = "Errors"
        namespace   = "AWS/Lambda"
        period      = 300
        stat        = "Sum"
        dimensions = {
          FunctionName = metric_query.value
        }
      }
    }
  }

  # alarm_actions = [] # Add SNS topic ARN when configured
}

# Lambda Errors Alarm (Background) - triggers when total errors across background Lambdas exceed threshold
resource "aws_cloudwatch_metric_alarm" "lambda_errors_background" {
  alarm_name          = "MediaDownloader-Lambda-Errors-Background"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  threshold           = 3
  alarm_description   = "Lambda errors exceed threshold across background functions"
  treat_missing_data  = "notBreaching"

  metric_query {
    id          = "errors"
    expression  = join(" + ", [for fn in local.lambda_functions_background : "m_${replace(fn, "-", "_")}"])
    label       = "Total Lambda Errors (Background)"
    return_data = true
  }

  dynamic "metric_query" {
    for_each = local.lambda_functions_background
    content {
      id = "m_${replace(metric_query.value, "-", "_")}"
      metric {
        metric_name = "Errors"
        namespace   = "AWS/Lambda"
        period      = 300
        stat        = "Sum"
        dimensions = {
          FunctionName = metric_query.value
        }
      }
    }
  }

  # alarm_actions = [] # Add SNS topic ARN when configured
}

# Lambda Throttles Alarm (API) - any throttle is concerning
resource "aws_cloudwatch_metric_alarm" "lambda_throttles_api" {
  alarm_name          = "MediaDownloader-Lambda-Throttles-API"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  threshold           = 0
  alarm_description   = "API Lambda functions are being throttled"
  treat_missing_data  = "notBreaching"

  metric_query {
    id          = "throttles"
    expression  = join(" + ", [for fn in local.lambda_functions_api : "m_${replace(fn, "-", "_")}"])
    label       = "Total Lambda Throttles (API)"
    return_data = true
  }

  dynamic "metric_query" {
    for_each = local.lambda_functions_api
    content {
      id = "m_${replace(metric_query.value, "-", "_")}"
      metric {
        metric_name = "Throttles"
        namespace   = "AWS/Lambda"
        period      = 300
        stat        = "Sum"
        dimensions = {
          FunctionName = metric_query.value
        }
      }
    }
  }

  # alarm_actions = [] # Add SNS topic ARN when configured
}

# Lambda Throttles Alarm (Background) - any throttle is concerning
resource "aws_cloudwatch_metric_alarm" "lambda_throttles_background" {
  alarm_name          = "MediaDownloader-Lambda-Throttles-Background"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  threshold           = 0
  alarm_description   = "Background Lambda functions are being throttled"
  treat_missing_data  = "notBreaching"

  metric_query {
    id          = "throttles"
    expression  = join(" + ", [for fn in local.lambda_functions_background : "m_${replace(fn, "-", "_")}"])
    label       = "Total Lambda Throttles (Background)"
    return_data = true
  }

  dynamic "metric_query" {
    for_each = local.lambda_functions_background
    content {
      id = "m_${replace(metric_query.value, "-", "_")}"
      metric {
        metric_name = "Throttles"
        namespace   = "AWS/Lambda"
        period      = 300
        stat        = "Sum"
        dimensions = {
          FunctionName = metric_query.value
        }
      }
    }
  }

  # alarm_actions = [] # Add SNS topic ARN when configured
}

# SQS DLQ Alarm - any message in DLQ requires investigation
resource "aws_cloudwatch_metric_alarm" "sqs_dlq_messages" {
  alarm_name          = "MediaDownloader-SQS-DLQ-Messages"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 1
  metric_name         = "ApproximateNumberOfMessagesVisible"
  namespace           = "AWS/SQS"
  period              = 300
  statistic           = "Sum"
  threshold           = 0
  alarm_description   = "Messages in SendPushNotification DLQ require investigation"
  treat_missing_data  = "notBreaching"

  dimensions = {
    QueueName = aws_sqs_queue.SendPushNotificationDLQ.name
  }

  # alarm_actions = [] # Add SNS topic ARN when configured
}

# SQS Queue Age Alarm - messages shouldn't be stuck in queue
resource "aws_cloudwatch_metric_alarm" "sqs_queue_age" {
  alarm_name          = "MediaDownloader-SQS-Queue-Age"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "ApproximateAgeOfOldestMessage"
  namespace           = "AWS/SQS"
  period              = 300
  statistic           = "Maximum"
  threshold           = 3600 # 1 hour
  alarm_description   = "Messages are stuck in SendPushNotification queue for too long"
  treat_missing_data  = "notBreaching"

  dimensions = {
    QueueName = aws_sqs_queue.SendPushNotification.name
  }

  # alarm_actions = [] # Add SNS topic ARN when configured
}
</file>

<file path="test/integration/helpers/electrodb-localstack.ts">
import {CreateTableCommand, DynamoDBClient} from '@aws-sdk/client-dynamodb'
/**
 * Setup MediaDownloader DynamoDB table in LocalStack
 * Creates table with all required GSIs for ElectroDB entities
 *
 * Table Design (lowercase to match ElectroDB entity field names):
 * - Primary Key: pk (HASH), sk (RANGE)
 * - gsi1: gsi1pk (HASH), gsi1sk (RANGE) - UserCollection (userResources)
 * - gsi2: gsi2pk (HASH), gsi2sk (RANGE) - FileCollection (fileUsers)
 * - gsi3: gsi3pk (HASH), gsi3sk (RANGE) - DeviceCollection (deviceUsers)
 *
 * @returns Promise that resolves when table is created
 */
export async function setupLocalStackTable(): Promise<void>
/**
 * Delete MediaDownloader table from LocalStack
 * Used for cleanup in tests
 *
 * @returns Promise that resolves when table is deleted
 */
export async function cleanupLocalStackTable(): Promise<void>
</file>

<file path="test/integration/lib/vendor/AWS/DynamoDB.ts">
/**
 * DynamoDB Test Vendor Wrapper
 *
 * Encapsulates AWS SDK DynamoDB operations used in integration tests.
 * This wrapper exists to maintain the AWS SDK Encapsulation Policy even in test code.
 */
import {CreateTableCommand, DeleteTableCommand} from '@aws-sdk/client-dynamodb'
import type {CreateTableCommandInput} from '@aws-sdk/client-dynamodb'
import {createDynamoDBClient} from '#lib/vendor/AWS/clients'
‚ãÆ----
/**
 * Creates a DynamoDB table
 * @param input - Table configuration matching AWS SDK CreateTableCommandInput
 */
export async function createTable(input: CreateTableCommandInput): Promise<void>
/**
 * Deletes a DynamoDB table
 * @param tableName - Name of the table to delete
 */
export async function deleteTable(tableName: string): Promise<void>
</file>

<file path="test/integration/workflows/auth.flow.integration.test.ts">
/**
 * Auth Flow Integration Tests
 *
 * Tests the LoginUser and RegisterUser workflows with Better Auth:
 * 1. Register new user - creates User, Session, Account entities via Better Auth
 * 2. Login existing user - validates credentials, creates session
 * 3. Login with invalid credentials - returns error
 * 4. Handle new user name update from iOS app
 *
 * Note: Better Auth handles OAuth verification and entity creation internally.
 * We mock the Better Auth module to test our Lambda orchestration logic.
 */
// Test configuration
‚ãÆ----
// Set environment variables for Lambda
‚ãÆ----
// Required env vars
‚ãÆ----
import {afterAll, beforeAll, beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {Context} from 'aws-lambda'
// Test helpers
import {createFilesTable, deleteFilesTable} from '../helpers/dynamodb-helpers'
import {createMockContext} from '../helpers/lambda-context'
import {createElectroDBEntityMock} from '../../helpers/electrodb-mock'
import {fileURLToPath} from 'url'
import {dirname, resolve} from 'path'
‚ãÆ----
// Mock Better Auth config module
‚ãÆ----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
‚ãÆ----
// Mock Users entity for RegisterUser name update
‚ãÆ----
// Import handlers after mocking
‚ãÆ----
interface AuthRequestBody {
  idToken: string
  firstName?: string
  lastName?: string
}
// eslint-disable-next-line @typescript-eslint/no-explicit-any
function createAuthEvent(body: AuthRequestBody, path: string): any
‚ãÆ----
// Default mock for Users.update
‚ãÆ----
createdAt: new Date(Date.now() - 1000000), // Created in the past
‚ãÆ----
// Verify Better Auth was called with correct params
‚ãÆ----
// Simulate unexpected redirect (which shouldn't happen for ID token flow)
‚ãÆ----
createdAt: new Date(), // Just created
‚ãÆ----
// Verify name update was called for new user
‚ãÆ----
createdAt: new Date(Date.now() - 1000000), // Created long ago
‚ãÆ----
// Should NOT update name for existing user
‚ãÆ----
// registerUserSchema has firstName and lastName as optional
‚ãÆ----
// Schema validation should pass (firstName/lastName are optional)
‚ãÆ----
// Better Auth should be called
‚ãÆ----
// Must include firstName/lastName to pass schema validation
‚ãÆ----
// Should have a default expiration (30 days from now)
</file>

<file path="test/integration/workflows/deviceRegistration.integration.test.ts">
/**
 * Device Registration Integration Tests
 *
 * Tests the RegisterDevice workflow:
 * 1. Create SNS platform endpoint from device token
 * 2. Upsert Device record (idempotent)
 * 3. Upsert UserDevice association for authenticated users
 * 4. Handle duplicate device registration (same device, different users)
 * 5. Subscribe anonymous users to push notification topic
 *
 * Validates:
 * - Idempotent device registration (no duplicates)
 * - Proper handling of authenticated vs anonymous users
 * - SNS subscription management
 */
// Test configuration
‚ãÆ----
// Set environment variables for Lambda
‚ãÆ----
// Required env vars
‚ãÆ----
import {afterAll, beforeAll, beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {Context} from 'aws-lambda'
import {UserStatus} from '../../../src/types/enums'
// Test helpers
import {createFilesTable, deleteFilesTable} from '../helpers/dynamodb-helpers'
import {createMockContext} from '../helpers/lambda-context'
import {createElectroDBEntityMock} from '../../helpers/electrodb-mock'
import {createMockDevice, createMockUserDevice} from '../helpers/test-data'
import {fileURLToPath} from 'url'
import {dirname, resolve} from 'path'
import type {CustomAPIGatewayRequestAuthorizerEvent} from '../../../src/types/infrastructure-types'
‚ãÆ----
// Entity module paths
‚ãÆ----
// Mock SNS operations
‚ãÆ----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
‚ãÆ----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
‚ãÆ----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
‚ãÆ----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
‚ãÆ----
// Create entity mocks
‚ãÆ----
// Import handler after mocking
‚ãÆ----
interface DeviceRegistrationBody {
  deviceId: string
  token: string
  name: string
  systemName: string
  systemVersion: string
}
/** Create a complete device registration body with required fields */
function createDeviceBody(deviceId: string, token: string): DeviceRegistrationBody
function createRegisterDeviceEvent(
  body: DeviceRegistrationBody,
  userId: string | undefined,
  userStatus: UserStatus
): CustomAPIGatewayRequestAuthorizerEvent
‚ãÆ----
// Mock SNS createPlatformEndpoint
‚ãÆ----
// Mock device upsert
‚ãÆ----
// Mock userDevice upsert
‚ãÆ----
// Mock getUserDevices - first device (length === 1)
‚ãÆ----
// Verify SNS endpoint created
‚ãÆ----
// Verify device stored
‚ãÆ----
// Verify user-device association created
‚ãÆ----
// Mock SNS createPlatformEndpoint - idempotent, returns same endpoint
‚ãÆ----
// Mock device upsert - idempotent
‚ãÆ----
// Mock userDevice upsert - idempotent
‚ãÆ----
// Mock getUserDevices - user already has this device (length > 1 means existing)
‚ãÆ----
// Mock listSubscriptionsByTopic for unsubscribe flow
‚ãÆ----
// Should return 201 for existing user with multiple devices
‚ãÆ----
// Verify unsubscribe was called (to prevent duplicate notifications)
‚ãÆ----
// Mock SNS createPlatformEndpoint
‚ãÆ----
// Mock device upsert
‚ãÆ----
// Mock subscribe for anonymous user
‚ãÆ----
// Verify device stored (even for anonymous)
‚ãÆ----
// Anonymous user should NOT create UserDevice association
‚ãÆ----
// Mock SNS createPlatformEndpoint
‚ãÆ----
// Mock device upsert
‚ãÆ----
// Create event with Authorization header but invalid/no userId
// getUserDetailsFromEvent returns Unauthenticated when there's an auth header but no userId
‚ãÆ----
// Missing required fields
‚ãÆ----
// Should fail validation
‚ãÆ----
// Mock SNS failure
‚ãÆ----
// Verify full device info was passed to upsert
</file>

<file path="test/integration/workflows/fileRetry.integration.test.ts">
/**
 * File Retry Integration Tests
 *
 * Tests the FileCoordinator scheduled workflow for retry logic:
 * 1. Query pending files (new downloads from WebhookFeedly)
 * 2. Query scheduled files ready for retry (retryAfter less than or equal to now)
 * 3. Process files in batches with delays
 * 4. Handle empty queues gracefully
 * 5. Verify metrics are published
 *
 * Validates:
 * - Pending and scheduled files are both processed
 * - Batch processing with delays between batches
 * - Deduplication of file IDs
 * - Metrics publication for monitoring
 */
// Test configuration
‚ãÆ----
// Set environment variables for Lambda
‚ãÆ----
// Required env vars
‚ãÆ----
import {afterAll, beforeAll, beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {Context} from 'aws-lambda'
// Test helpers
import {createFilesTable, deleteFilesTable} from '../helpers/dynamodb-helpers'
import {createMockContext} from '../helpers/lambda-context'
import {createElectroDBEntityMock} from '../../helpers/electrodb-mock'
import {createMockScheduledEvent} from '../helpers/test-data'
import {fileURLToPath} from 'url'
import {dirname, resolve} from 'path'
‚ãÆ----
// Mock FileDownloads entity
‚ãÆ----
// Mock lambda-invoke-helpers (initiateFileDownload)
‚ãÆ----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
‚ãÆ----
// Mock CloudWatch metrics
‚ãÆ----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
‚ãÆ----
// Import handler after mocking
‚ãÆ----
interface FileDownload {
  fileId: string
  status: string
  retryAfter?: number
  attemptCount?: number
}
function createMockFileDownload(fileId: string, status: string, retryAfter?: number): FileDownload
‚ãÆ----
// Mock pending files query
‚ãÆ----
// Mock scheduled files query (empty)
‚ãÆ----
// FileCoordinator is a scheduled handler that returns void
‚ãÆ----
// Verify initiateFileDownload called for each file (with correlationId)
‚ãÆ----
// Mock pending files query (empty)
‚ãÆ----
// Mock scheduled files query
‚ãÆ----
// Mock pending files
‚ãÆ----
// Mock scheduled files
‚ãÆ----
// Same file appears in both pending and scheduled (edge case)
‚ãÆ----
// Only processed once due to deduplication
‚ãÆ----
// Both queries return empty
‚ãÆ----
// Should complete without error
‚ãÆ----
// No downloads initiated
‚ãÆ----
// Verify metrics were published
‚ãÆ----
// With Promise.allSettled, query failures are handled gracefully
// Handler continues with empty arrays for failed queries
‚ãÆ----
// Handler should complete without throwing (logs error internally)
‚ãÆ----
// No downloads initiated when queries fail
‚ãÆ----
// Create more files than BATCH_SIZE (5)
‚ãÆ----
// All 12 files should be processed
‚ãÆ----
}, 60000) // Longer timeout for batch processing with delays
‚ãÆ----
// Make middle file fail
‚ãÆ----
// Handler uses Promise.allSettled - completes successfully even with partial failures
‚ãÆ----
// All 3 files attempted (failed one logged but doesn't stop others)
</file>

<file path="test/integration/workflows/userDelete.cascade.integration.test.ts">
/**
 * UserDelete Cascade Integration Tests
 *
 * Tests the user deletion workflow with cascade deletion:
 * 1. Delete UserFiles (user-file associations)
 * 2. Delete UserDevices (user-device associations)
 * 3. Delete Devices
 * 4. Delete User (parent - only after all children succeed)
 *
 * Validates:
 * - Correct cascade order (children before parent)
 * - Partial failure handling (don't delete parent if children fail)
 * - Error reporting via GitHub issue creation
 */
// Test configuration
‚ãÆ----
// Set environment variables for Lambda
‚ãÆ----
// Required env vars
‚ãÆ----
import {afterAll, beforeAll, beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {Context} from 'aws-lambda'
import {UserStatus} from '../../../src/types/enums'
// Test helpers
import {createFilesTable, deleteFilesTable} from '../helpers/dynamodb-helpers'
import {createMockContext} from '../helpers/lambda-context'
import {createElectroDBEntityMock} from '../../helpers/electrodb-mock'
import {createMockDevice, createMockUserDevice, createMockUserFile} from '../helpers/test-data'
import {fileURLToPath} from 'url'
import {dirname, resolve} from 'path'
import type {CustomAPIGatewayRequestAuthorizerEvent} from '../../../src/types/infrastructure-types'
‚ãÆ----
// Entity module paths
‚ãÆ----
// Mock GitHub helpers to prevent actual API calls
‚ãÆ----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
‚ãÆ----
// Mock SNS operations (for device deletion)
‚ãÆ----
// eslint-disable-next-line @typescript-eslint/no-explicit-any
‚ãÆ----
// Create entity mocks
‚ãÆ----
// Import handler after mocking
‚ãÆ----
function createUserDeleteEvent(userId: string): CustomAPIGatewayRequestAuthorizerEvent
‚ãÆ----
// Default mock implementations
‚ãÆ----
// Mock UserDevices query - returns user's device associations
‚ãÆ----
// Mock Devices.get batch - returns actual device records
‚ãÆ----
// Mock UserFiles query
‚ãÆ----
// Mock delete operations
‚ãÆ----
// Verify cascade order: UserFiles queried
‚ãÆ----
// Verify UserDevices queried (getUserDevices + deleteUserDevices = 2 calls)
‚ãÆ----
// Verify Devices batch get called
‚ãÆ----
// Verify user deleted last (after all children)
‚ãÆ----
// Mock UserDevices query
‚ãÆ----
// Mock Devices.get batch
‚ãÆ----
// Mock UserFiles query - simulate failure
‚ãÆ----
// Other operations succeed
‚ãÆ----
// User should NOT be deleted when children fail
‚ãÆ----
// Mock empty results
‚ãÆ----
// User should still be deleted
‚ãÆ----
// No child deletions needed
‚ãÆ----
// Mock empty children (so we get to parent deletion)
‚ãÆ----
// Simulate user deletion failure
‚ãÆ----
// Handler catches error and returns 500 after creating GitHub issue
‚ãÆ----
// Verify GitHub issue was created
‚ãÆ----
// Create event with no userId
‚ãÆ----
// wrapAuthenticatedHandler rejects Unauthenticated users with 401
‚ãÆ----
// Mock UserDevices query
‚ãÆ----
// Mock Devices.get batch
‚ãÆ----
// Mock empty UserFiles
‚ãÆ----
// Mock all delete operations
‚ãÆ----
// Verify SNS endpoint deletion was called for each device
</file>

<file path="test/integration/README.md">
# Integration Tests

This directory contains integration tests that verify multi-service workflows using LocalStack.

## Overview

**Integration tests validate YOUR orchestration logic across multiple AWS services, not AWS SDK behavior.**

Unlike unit tests which mock AWS services, integration tests execute complete end-to-end workflows against LocalStack. The goal is to test YOUR code's orchestration, state management, and error handling‚Äînot to verify that S3 uploads work or DynamoDB queries succeed.

**Testing Philosophy:** See [`docs/wiki/Testing/Coverage-Philosophy.md`](../../docs/wiki/Testing/Coverage-Philosophy.md) and [`docs/wiki/Integration/LocalStack-Testing.md`](../../docs/wiki/Integration/LocalStack-Testing.md) for comprehensive testing philosophy and patterns.

## Running Integration Tests

### Prerequisites

1. **Docker**: Required to run LocalStack container (includes Compose plugin)
2. **jq**: Optional, for pretty-printing health check results

### Quick Start

```bash
# Start LocalStack
npm run localstack:start

# Wait for LocalStack to be ready
npm run localstack:health

# Run integration tests
npm run test:integration

# View LocalStack logs (if needed)
npm run localstack:logs

# Stop LocalStack when done
npm run localstack:stop
```

### Commands

- `npm run localstack:start` - Start LocalStack container in detached mode
- `npm run localstack:stop` - Stop and remove LocalStack container
- `npm run localstack:logs` - Stream LocalStack logs
- `npm run localstack:health` - Check LocalStack service health
- `npm run test:integration` - Run all integration tests

## Test Organization

Tests are organized by workflow, not by service:

```
test/integration/
‚îú‚îÄ‚îÄ setup.ts                                      # Global test setup
‚îú‚îÄ‚îÄ README.md                                     # This file
‚îú‚îÄ‚îÄ workflows/                                    # Workflow-based integration tests
‚îÇ   ‚îú‚îÄ‚îÄ webhookFeedly.workflow.integration.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ fileCoordinator.workflow.integration.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ startFileUpload.workflow.integration.test.ts
‚îÇ   ‚îî‚îÄ‚îÄ listFiles.workflow.integration.test.ts
‚îî‚îÄ‚îÄ helpers/                                      # Test utilities
    ‚îú‚îÄ‚îÄ dynamodb-helpers.ts                       # DynamoDB test data helpers
    ‚îú‚îÄ‚îÄ s3-helpers.ts                             # S3 test utilities
    ‚îú‚îÄ‚îÄ lambda-helpers.ts                         # Lambda invocation helpers
    ‚îî‚îÄ‚îÄ sqs-helpers.ts                            # SQS test utilities
```

**Why Workflow-Based?**
- Tests YOUR multi-service orchestration, not individual AWS services
- Mirrors real production workflows (webhook ‚Üí DynamoDB ‚Üí Lambda ‚Üí S3)
- Coverage of vendor wrappers happens naturally as a side effect

## Writing Integration Tests

Integration tests should:

1. **Test YOUR workflows, not AWS SDK behavior**:
   ```typescript
   // ‚úÖ Correct - tests YOUR orchestration logic
   test('should complete video download workflow and update DynamoDB status', async () => {
     // Arrange: Create pending file in DynamoDB
     await insertFile({fileId: 'test-video', status: 'PendingDownload'})

     // Act: Execute YOUR Lambda handler
     await handler({fileId: 'test-video'}, mockContext)

     // Assert: Verify YOUR state management worked
     const file = await getFile('test-video')
     expect(file.status).toBe('Downloaded')  // YOUR status transition

     const s3Object = await headObject(bucket, 'test-video.mp4')
     expect(s3Object.ContentLength).toBeGreaterThan(0)  // YOUR upload succeeded
   })

   // ‚ùå Wrong - tests AWS SDK, not YOUR code
   test('should upload object to S3', async () => {
     const upload = createS3Upload(bucket, key, content)
     await upload.done()
     expect(await headObject(bucket, key)).toBeDefined()
   })
   ```

2. **Use vendor wrappers**, not AWS SDK directly:
   ```typescript
   // ‚úÖ Correct - uses vendor wrapper
   import {headObject, createS3Upload} from '../../../src/lib/vendor/AWS/S3'

   // ‚ùå Wrong - violates AWS SDK Encapsulation Policy
   import {S3Client, HeadObjectCommand} from '@aws-sdk/client-s3'
   ```

3. **Test multi-service workflows**:
   ```typescript
   // ‚úÖ Good - tests YOUR orchestration across services
   test('should process webhook, create DynamoDB record, and send SQS message', async () => {
     await handler(webhookPayload, context)

     const dbRecord = await getFile('video-id')  // DynamoDB check
     expect(dbRecord.status).toBe('Pending')

     const messages = await getSQSMessages(queueUrl)  // SQS check
     expect(messages).toHaveLength(1)
   })
   ```

4. **Clean up resources** after each test:
   ```typescript
   afterEach(async () => {
     await deleteFilesTable()
     await deleteS3Objects()
   })
   ```

5. **Follow naming convention**: `*.workflow.integration.test.ts`

## Environment Variables

Integration tests automatically set:

- `USE_LOCALSTACK=true` - Triggers vendor wrappers to use LocalStack clients
- `AWS_REGION=us-west-2` - Match production region for consistency

These are configured in `test/integration/setup.ts`.

## LocalStack Configuration

LocalStack is configured via `docker-compose.localstack.yml`:

- **Endpoint**: http://localhost:4566
- **Services**: S3, DynamoDB, SNS, SQS, Lambda, CloudWatch, API Gateway
- **Storage**: Ephemeral (fresh state each run for consistent test results)
- **Debug Mode**: Enabled for troubleshooting

## Troubleshooting

### LocalStack not responding

```bash
# Check if LocalStack is running
docker ps | grep localstack

# View logs for errors
npm run localstack:logs

# Restart LocalStack
npm run localstack:stop
npm run localstack:start
```

### Tests timing out

Integration tests have a 30-second timeout. If tests are timing out:

1. Check LocalStack logs for errors
2. Verify LocalStack health: `npm run localstack:health`
3. Ensure LocalStack has sufficient resources (increase Docker memory if needed)

### Port conflicts

If port 4566 is already in use:

```bash
# Find process using port 4566
lsof -i :4566

# Kill the process or stop conflicting service
```

## CI/CD Integration

GitHub Actions workflow automatically:

1. Starts LocalStack before running tests
2. Runs integration tests with `npm run test:integration`
3. Uploads test results and coverage
4. Stops LocalStack after tests complete

See `.github/workflows/integration-tests.yml` for CI/CD configuration.
</file>

<file path=".github/scripts/generate-sidebar.sh">
#!/usr/bin/env bash
# Generate Sidebar Script
# Creates _Sidebar.md for GitHub Wiki navigation
set -euo pipefail
shopt -s nullglob  # Allow glob patterns to expand to empty list if no matches
# Configuration
SOURCE_DIR="${SOURCE_DIR:-docs/wiki}"  # Read structure from source
WIKI_DIR="${WIKI_DIR:-wiki}"           # Write sidebar to wiki
SIDEBAR_FILE="$WIKI_DIR/_Sidebar.md"
echo "üìã Generating sidebar navigation..."
echo "==================================="
echo "Wiki directory: $WIKI_DIR"
echo "Output file: $SIDEBAR_FILE"
echo ""
# Check if directories exist
if [ ! -d "$SOURCE_DIR" ]; then
    echo "‚ùå Error: Source directory $SOURCE_DIR does not exist"
    exit 1
fi
if [ ! -d "$WIKI_DIR" ]; then
    echo "‚ùå Error: Wiki directory $WIKI_DIR does not exist"
    exit 1
fi
# Function to get clean page title from filename
get_page_title() {
    local filename="$1"
    local basename=$(basename "$filename" .md)
    # Convert filename to title
    echo "$basename" | sed -E 's/-/ /g' | sed -E 's/\b(\w)/\u\1/g'
}
# Function to process directory recursively
process_directory() {
    local dir="$1"
    local indent="$2"
    local parent_path="$3"
    # Get all markdown files in current directory
    for file in "$dir"/*.md; do
        if [ ! -f "$file" ]; then
            continue
        fi
        local basename=$(basename "$file" .md)
        # Skip special wiki files
        if [[ "$basename" =~ ^_(Footer|Sidebar)$ ]]; then
            continue
        fi
        # Skip Home.md in subdirectories (only use root Home.md)
        if [[ "$basename" == "Home" ]] && [[ "$dir" != "$WIKI_DIR" ]]; then
            continue
        fi
        # Create link path (relative to wiki root)
        local link_path=""
        if [ -z "$parent_path" ]; then
            link_path="$basename"
        else
            link_path="$parent_path/$basename"
        fi
        # Get page title
        local title=$(get_page_title "$file")
        # Special handling for Home.md
        if [[ "$basename" == "Home" ]]; then
            title="üè† Home"
            link_path="Home"
        fi
        # Add to sidebar
        echo "${indent}- [$title]($link_path)" >> "$SIDEBAR_FILE"
    done
    # Process subdirectories
    for subdir in "$dir"/*/; do
        if [ ! -d "$subdir" ]; then
            continue
        fi
        local dirname=$(basename "$subdir")
        # Skip hidden directories
        if [[ "$dirname" =~ ^\. ]]; then
            continue
        fi
        # Add category header
        echo "" >> "$SIDEBAR_FILE"
        # Determine category emoji based on name
        local emoji=""
        case "$dirname" in
            Conventions) emoji="üìã" ;;
            TypeScript) emoji="üéØ" ;;
            Testing) emoji="üß™" ;;
            AWS) emoji="‚òÅÔ∏è" ;;
            Bash) emoji="üìú" ;;
            Infrastructure) emoji="üèóÔ∏è" ;;
            Methodologies) emoji="üí°" ;;
            Meta) emoji="üîÆ" ;;
            *) emoji="üìÅ" ;;
        esac
        # Add category title
        local category_title=$(get_page_title "$dirname")
        echo "${indent}**$emoji $category_title**" >> "$SIDEBAR_FILE"
        # Process files in subdirectory
        local subdir_path=""
        if [ -z "$parent_path" ]; then
            subdir_path="$dirname"
        else
            subdir_path="$parent_path/$dirname"
        fi
        process_directory "$subdir" "$indent" "$subdir_path"
    done
}
# Main execution
main() {
    # Start fresh
    cat > "$SIDEBAR_FILE" << 'EOF'
# Navigation
## Quick Links
- [üè† Home](Home)
- [üöÄ Getting Started](Getting-Started)
---
## Documentation
EOF
    echo "" >> "$SIDEBAR_FILE"
    # Process root level files (except Home and Getting-Started which are already added)
    for file in "$SOURCE_DIR"/*.md; do
        if [ ! -f "$file" ]; then
            continue
        fi
        local basename=$(basename "$file" .md)
        # Skip special files and already added files
        if [[ "$basename" =~ ^_(Footer|Sidebar)$ ]] || \
           [[ "$basename" == "Home" ]] || \
           [[ "$basename" == "Getting-Started" ]]; then
            continue
        fi
        local title=$(get_page_title "$file")
        echo "- [$title]($basename)" >> "$SIDEBAR_FILE"
    done
    # Process each category directory from source
    for dir in "$SOURCE_DIR"/*/; do
        if [ ! -d "$dir" ]; then
            continue
        fi
        local dirname=$(basename "$dir")
        # Skip hidden directories
        if [[ "$dirname" =~ ^\. ]]; then
            continue
        fi
        echo "" >> "$SIDEBAR_FILE"
        # Determine category emoji
        local emoji=""
        case "$dirname" in
            Conventions) emoji="üìã" ;;
            TypeScript) emoji="üéØ" ;;
            Testing) emoji="üß™" ;;
            AWS) emoji="‚òÅÔ∏è" ;;
            Bash) emoji="üìú" ;;
            Infrastructure) emoji="üèóÔ∏è" ;;
            Methodologies) emoji="üí°" ;;
            Meta) emoji="üîÆ" ;;
            *) emoji="üìÅ" ;;
        esac
        # Add category section
        local category_title=$(get_page_title "$dirname")
        echo "### $emoji $category_title" >> "$SIDEBAR_FILE"
        echo "" >> "$SIDEBAR_FILE"
        # Process files in category
        for file in "$dir"/*.md; do
            if [ ! -f "$file" ]; then
                continue
            fi
            local basename=$(basename "$file" .md)
            local title=$(get_page_title "$file")
            # GitHub Wiki has flat namespace - use basename only
            echo "- [$title]($basename)" >> "$SIDEBAR_FILE"
        done
        # Process subdirectories if any
        for subdir in "$dir"/*/; do
            if [ ! -d "$subdir" ]; then
                continue
            fi
            local subdirname=$(basename "$subdir")
            echo "  - **$(get_page_title "$subdirname")**" >> "$SIDEBAR_FILE"
            for file in "$subdir"/*.md; do
                if [ ! -f "$file" ]; then
                    continue
                fi
                local basename=$(basename "$file" .md)
                local title=$(get_page_title "$file")
                # GitHub Wiki has flat namespace - use basename only
                echo "    - [$title]($basename)" >> "$SIDEBAR_FILE"
            done
        done
    done
    # Add footer section
    cat >> "$SIDEBAR_FILE" << 'EOF'
---
## Resources
- [üìö Main Repository](https://github.com/${GITHUB_REPOSITORY})
- [üìñ View in GitHub](https://github.com/${GITHUB_REPOSITORY}/tree/main/docs/wiki)
- [üêõ Report Issue](https://github.com/${GITHUB_REPOSITORY}/issues)
EOF
    echo "‚úÖ Sidebar generated successfully!"
    echo ""
    echo "üìä Sidebar statistics:"
    echo "  - Lines: $(wc -l < "$SIDEBAR_FILE")"
    echo "  - Categories: $(grep -c '^###' "$SIDEBAR_FILE" || echo 0)"
    echo "  - Links: $(grep -c '\[.*\](' "$SIDEBAR_FILE" || echo 0)"
    # Show preview
    echo ""
    echo "üìÑ Sidebar preview (first 20 lines):"
    echo "-----------------------------------"
    head -20 "$SIDEBAR_FILE"
    echo "..."
}
# Run main function
main
</file>

<file path=".github/workflows/integration-tests.yml">
name: Integration Tests
on:
  push:
    branches:
      - '**'
  pull_request:
    branches:
      - master
jobs:
  integration-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Setup pnpm
        uses: pnpm/action-setup@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: '.nvmrc'
          cache: 'pnpm'
      - name: Install dependencies
        run: pnpm install --frozen-lockfile
      - name: Check types
        run: pnpm run check-types
      - name: Lint code
        run: pnpm run lint
      - name: Start LocalStack
        run: |
          docker compose -f docker-compose.localstack.yml up -d
          echo "Waiting for LocalStack to be ready..."
      - name: Wait for LocalStack health check
        run: |
          max_retries=30
          retry_count=0
          while [ $retry_count -lt $max_retries ]; do
            if curl -sf http://localhost:4566/_localstack/health > /dev/null 2>&1; then
              echo "LocalStack is healthy"
              curl -s http://localhost:4566/_localstack/health | jq
              exit 0
            fi
            retry_count=$((retry_count + 1))
            echo "Attempt $retry_count of $max_retries..."
            sleep 1
          done
          echo "LocalStack failed to become healthy"
          docker compose -f docker-compose.localstack.yml logs
          exit 1
      - name: Run integration tests
        run: pnpm run test:integration --coverage --coverageDirectory=coverage/integration
        env:
          CI: true
          USE_LOCALSTACK: true
      - name: Upload integration test coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage/integration/lcov.info
          flags: integration
          name: integration-tests
          token: ${{ secrets.CODECOV_TOKEN }}
          fail_ci_if_error: true
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: |
            coverage/
            test-results/
          retention-days: 7
      - name: Show LocalStack logs on failure
        if: failure()
        run: docker compose -f docker-compose.localstack.yml logs
      - name: Stop LocalStack
        if: always()
        run: docker compose -f docker-compose.localstack.yml down
</file>

<file path="bin/test-hook.sh">
#!/usr/bin/env bash
# Get the directory of this file (where the package.json file is located)
bin_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" > /dev/null 2>&1 && pwd)"
cd "${bin_dir}/../terraform"
domain=$(tofu output cloudfront_distribution_domain | tr -d '"')
api_key=$(tofu output api_gateway_api_key | tr -d '"')
REQUEST_URL="https://${domain}/feedly?ApiKey=${api_key}"
echo "Calling ${REQUEST_URL}"
curl -v -H "Content-Type: application/json" \
  -H "Accept: application/json" \
  -H "User-Agent: localhost@lifegames" \
  --data @./../src/lambdas/WebhookFeedly/test/fixtures/handleFeedlyEvent-200-OK.json \
  $REQUEST_URL | jq
</file>

<file path="bin/test-integration.sh">
#!/usr/bin/env bash
# test-integration.sh
# Runs integration tests against LocalStack
# Usage: pnpm run test:integration or ./bin/test-integration.sh
set -e # Exit on error
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
COMPOSE_FILE="${PROJECT_ROOT}/docker-compose.localstack.yml"
LOCALSTACK_HEALTH_URL="http://localhost:4566/_localstack/health"
MAX_HEALTH_RETRIES=30
HEALTH_RETRY_DELAY=1
# Color constants
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'
# Flags
cleanup_after=false
start_localstack=true
# Parse command line arguments
while [[ $# -gt 0 ]]; do
  case $1 in
    --cleanup)
      cleanup_after=true
      shift
      ;;
    --no-start)
      start_localstack=false
      shift
      ;;
    *)
      echo -e "${RED}Unknown option: $1${NC}"
      echo "Usage: $0 [--cleanup] [--no-start]"
      echo "  --cleanup: Stop LocalStack after tests complete"
      echo "  --no-start: Don't start LocalStack (assume it's already running)"
      exit 1
      ;;
  esac
done
echo -e "${GREEN}Integration Test Runner${NC}"
echo "======================="
echo ""
# Check required commands
echo -e "${YELLOW}Checking prerequisites...${NC}"
if ! command -v docker &> /dev/null; then
  echo -e "${RED}Error: docker is not installed${NC}"
  echo "Please install Docker Desktop: https://www.docker.com/products/docker-desktop"
  exit 1
fi
if ! docker compose version &> /dev/null; then
  echo -e "${RED}Error: docker compose is not available${NC}"
  echo "Please install Docker with Compose plugin or docker-compose standalone"
  exit 1
fi
echo "‚úÖ Docker and docker compose are available"
echo ""
# Function to check LocalStack health
check_localstack_health() {
  local retry_count=0
  echo -e "${YELLOW}Waiting for LocalStack to be ready...${NC}"
  while [ $retry_count -lt $MAX_HEALTH_RETRIES ]; do
    if curl -sf "$LOCALSTACK_HEALTH_URL" > /dev/null 2>&1; then
      echo -e "${GREEN}‚úÖ LocalStack is healthy${NC}"
      # Show service status
      health_status=$(curl -s "$LOCALSTACK_HEALTH_URL" | jq -r '.services | to_entries | map("\(.key): \(.value)") | join(", ")')
      echo -e "${BLUE}Services: $health_status${NC}"
      echo ""
      return 0
    fi
    retry_count=$((retry_count + 1))
    if [ $retry_count -lt $MAX_HEALTH_RETRIES ]; then
      echo -n "."
      sleep $HEALTH_RETRY_DELAY
    fi
  done
  echo ""
  echo -e "${RED}Error: LocalStack failed to become healthy after ${MAX_HEALTH_RETRIES} seconds${NC}"
  return 1
}
# Start LocalStack if requested
if [ "$start_localstack" = true ]; then
  echo -e "${YELLOW}Starting LocalStack...${NC}"
  # Check if LocalStack is already running
  if docker ps | grep -q "aws-media-downloader-localstack"; then
    echo "üì¶ LocalStack is already running"
    echo ""
  else
    echo "üöÄ Starting LocalStack container..."
    docker compose -f "$COMPOSE_FILE" up -d
    echo ""
  fi
  # Wait for LocalStack to be healthy
  if ! check_localstack_health; then
    echo -e "${RED}LocalStack health check failed${NC}"
    echo ""
    echo "Troubleshooting:"
    echo "  1. Check LocalStack logs: pnpm run localstack:logs"
    echo "  2. Restart LocalStack: pnpm run localstack:stop && pnpm run localstack:start"
    echo "  3. Check Docker resources (memory, CPU)"
    exit 1
  fi
else
  echo -e "${YELLOW}Skipping LocalStack startup (--no-start flag)${NC}"
  echo ""
  # Verify LocalStack is running
  if ! docker ps | grep -q "aws-media-downloader-localstack"; then
    echo -e "${RED}Error: LocalStack is not running${NC}"
    echo "Start LocalStack with: pnpm run localstack:start"
    exit 1
  fi
  # Check health
  if ! check_localstack_health; then
    echo -e "${RED}LocalStack is running but not healthy${NC}"
    exit 1
  fi
fi
# Run integration tests
echo -e "${YELLOW}Running integration tests...${NC}"
echo ""
cd "$PROJECT_ROOT"
# Set environment variable for integration tests
export USE_LOCALSTACK=true
# Run Jest with integration config
if node --no-warnings --experimental-vm-modules node_modules/jest/bin/jest.js --config config/jest.integration.config.mjs; then
  test_exit_code=0
  echo ""
  echo -e "${GREEN}‚úÖ All integration tests passed!${NC}"
else
  test_exit_code=$?
  echo ""
  echo -e "${RED}‚ùå Integration tests failed${NC}"
fi
echo ""
# Cleanup if requested
if [ "$cleanup_after" = true ]; then
  echo -e "${YELLOW}Stopping LocalStack...${NC}"
  docker compose -f "$COMPOSE_FILE" down
  echo -e "${GREEN}‚úÖ LocalStack stopped${NC}"
  echo ""
else
  echo -e "${BLUE}‚ÑπÔ∏è  LocalStack is still running${NC}"
  echo "To stop: pnpm run localstack:stop"
  echo "To view logs: pnpm run localstack:logs"
  echo ""
fi
# Summary
echo -e "${GREEN}Test Run Complete${NC}"
echo "=================="
echo ""
echo "üìç Test location: test/integration/"
echo "üìä Exit code: $test_exit_code"
echo ""
if [ $test_exit_code -eq 0 ]; then
  echo "Next steps:"
  echo "  ‚Ä¢ Review test coverage"
  echo "  ‚Ä¢ Add more integration tests as needed"
  echo "  ‚Ä¢ Run unit tests: pnpm test"
else
  echo "Troubleshooting:"
  echo "  ‚Ä¢ Check LocalStack logs: pnpm run localstack:logs"
  echo "  ‚Ä¢ Review test output above"
  echo "  ‚Ä¢ Verify LocalStack services are healthy: pnpm run localstack:health"
fi
echo ""
exit $test_exit_code
</file>

<file path="bin/test-list.sh">
#!/usr/bin/env bash
# Get the directory of this file (where the package.json file is located)
bin_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" > /dev/null 2>&1 && pwd)"
cd "${bin_dir}/../terraform"
domain=$(tofu output cloudfront_distribution_domain | tr -d '"')
api_key=$(tofu output api_gateway_api_key | tr -d '"')
REQUEST_URL="https://${domain}/files?ApiKey=${api_key}"
echo "Calling ${REQUEST_URL}"
curl -v -H "Content-Type: application/json" \
  -H "User-Agent: localhost@lifegames" \
  -H "Accept: application/json" \
  $REQUEST_URL | jq
</file>

<file path="config/jest.config.mjs">
/**
 * For a detailed explanation regarding each configuration property, visit:
 * https://jestjs.io/docs/configuration
 */
‚ãÆ----
// All imported modules in your tests should be mocked automatically
// automock: false,
‚ãÆ----
// Stop running tests after `n` failures
// bail: 0,
‚ãÆ----
// The directory where Jest should store its cached dependency information
// cacheDirectory: "/private/var/folders/xr/9_zz1ghn3bg9vsks4nly70c00000gn/T/jest_dx",
‚ãÆ----
// Automatically clear mock calls, instances, contexts and results before every test
‚ãÆ----
// Coverage options are configured in jest.all.config.mjs when running multi-project tests
// For standalone coverage runs, use: npm test -- --coverage
‚ãÆ----
// A list of reporter names that Jest uses when writing coverage reports
// coverageReporters: [
//   "json",
//   "text",
//   "lcov",
//   "clover"
// ],
‚ãÆ----
// An object that configures minimum threshold enforcement for coverage results
// coverageThreshold: undefined,
‚ãÆ----
// A path to a custom dependency extractor
// dependencyExtractor: undefined,
‚ãÆ----
// Make calling deprecated APIs throw helpful error messages
// errorOnDeprecated: false,
‚ãÆ----
// The default configuration for fake timers
// fakeTimers: {
//   "enableGlobally": false
// },
‚ãÆ----
// Force coverage collection from ignored files using an array of glob patterns
// forceCoverageMatch: [],
‚ãÆ----
// A path to a module which exports an async function that is triggered once before all test suites
// globalSetup: undefined,
‚ãÆ----
// A path to a module which exports an async function that is triggered once after all test suites
// globalTeardown: undefined,
‚ãÆ----
// A set of global variables that need to be available in all test environments
// globals: {},
‚ãÆ----
// Limit workers to prevent Jest worker hang issues with AWS SDK
// AWS SDK v3 maintains HTTP keep-alive connections that can prevent workers from exiting
// Running with fewer workers reduces resource contention and improves reliability
‚ãÆ----
// An array of directory names to be searched recursively up from the requiring module's location
// moduleDirectories: [
//   "node_modules"
// ],
‚ãÆ----
// An array of file extensions your modules use
// moduleFileExtensions: [
//   "js",
//   "mjs",
//   "cjs",
//   "jsx",
//   "ts",
//   "tsx",
//   "json",
//   "node"
// ],
‚ãÆ----
// A map from regular expressions to module names or to arrays of module names that allow to stub out resources with a single module
‚ãÆ----
// An array of regexp pattern strings, matched against all module paths before considered 'visible' to the module loader
// modulePathIgnorePatterns: [],
‚ãÆ----
// Activates notifications for test results
// notify: false,
‚ãÆ----
// An enum that specifies notification mode. Requires { notify: true }
// notifyMode: "failure-change",
‚ãÆ----
// A preset that is used as a base for Jest's configuration
‚ãÆ----
// Run tests from one or more projects
// projects: undefined,
‚ãÆ----
// Use this configuration option to add custom reporters to Jest
// reporters: undefined,
‚ãÆ----
// Automatically reset mock state before every test
// resetMocks: false,
‚ãÆ----
// Reset the module registry before running each individual test
// resetModules: false,
‚ãÆ----
// A path to a custom resolver
// resolver: undefined,
‚ãÆ----
// Automatically restore mock state and implementation before every test
// restoreMocks: false,
‚ãÆ----
// The root directory that Jest should scan for tests and modules within
‚ãÆ----
// A list of paths to directories that Jest should use to search for files in
// roots: [
//   "<rootDir>"
// ],
‚ãÆ----
// Allows you to use a custom runner instead of Jest's default test runner
// runner: "jest-runner",
‚ãÆ----
// The paths to modules that run some code to configure or set up the testing environment before each test
// setupFiles: [],
‚ãÆ----
// A list of paths to modules that run some code to configure or set up the testing framework before each test
// setupFilesAfterEnv: [],
‚ãÆ----
// The number of seconds after which a test is considered as slow and reported as such in the results.
// slowTestThreshold: 5,
‚ãÆ----
// A list of paths to snapshot serializer modules Jest should use for snapshot testing
// snapshotSerializers: [],
‚ãÆ----
// The test environment that will be used for testing
‚ãÆ----
// Options that will be passed to the testEnvironment
// testEnvironmentOptions: {},
‚ãÆ----
// Adds a location field to test results
// testLocationInResults: false,
‚ãÆ----
// The glob patterns Jest uses to detect test files
‚ãÆ----
// An array of regexp pattern strings that are matched against all test paths, matched tests are skipped
‚ãÆ----
// The regexp pattern or array of patterns that Jest uses to detect test files
// testRegex: [],
‚ãÆ----
// This option allows the use of a custom results processor
// testResultsProcessor: undefined,
‚ãÆ----
// This option allows use of a custom test runner
// testRunner: "jest-circus/runner",
‚ãÆ----
// A map from regular expressions to paths to transformers
‚ãÆ----
// '^.+\\.[tj]sx?$' to process js/ts with `ts-jest`
// '^.+\\.m?[tj]sx?$' to process js/ts/mjs/mts with `ts-jest`
‚ãÆ----
// An array of regexp pattern strings that are matched against all source file paths, matched files will skip transformation
// transformIgnorePatterns: [
//   "/node_modules/",
//   "\\.pnp\\.[^\\/]+$"
// ],
‚ãÆ----
// An array of regexp pattern strings that are matched against all modules before the module loader will automatically return a mock for them
// unmockedModulePathPatterns: undefined,
‚ãÆ----
// Indicates whether each individual test should be reported during the run
// verbose: undefined,
‚ãÆ----
// An array of regexp patterns that are matched against all source file paths before re-running tests in watch mode
// watchPathIgnorePatterns: [],
‚ãÆ----
// Whether to use watchman for file crawling
// watchman: true,
</file>

<file path="docs/wiki/MCP/Convention-Tools.md">
# MCP Convention Tools

## Quick Reference
- **When to use**: AI assistants querying project conventions and validating code
- **Enforcement**: Automated via MCP server and optional CI integration
- **Impact if violated**: Varies by rule severity (CRITICAL to LOW)

## Overview

The MCP server provides 5 tools for convention querying and code validation:

| Tool | Purpose | Primary Use Case |
|------|---------|-----------------|
| `query_conventions` | Search project conventions | Understanding rules before coding |
| `validate_pattern` | AST-based code validation | Checking code against conventions |
| `check_coverage` | Mock analysis for tests | Identifying required Jest mocks |
| `lambda_impact` | Dependency impact analysis | Understanding change scope |
| `suggest_tests` | Test scaffolding generation | Creating new test files |

## Tool Details

### query_conventions

Search and filter project conventions from `docs/conventions-tracking.md` and wiki pages.

**Query Types:**
- `list` - List all conventions grouped by severity
- `search` - Full-text search across conventions and wiki
- `category` - Filter by category (testing, aws, typescript, etc.)
- `enforcement` - Filter by severity (CRITICAL, HIGH, MEDIUM, LOW)
- `detail` - Get full details for a specific convention
- `wiki` - List or search wiki documentation

**Examples:**
```typescript
// Find all testing conventions
query_conventions({ query: "category", category: "testing" })

// Search for mock-related conventions
query_conventions({ query: "search", term: "mock" })

// Get CRITICAL rules that must not be violated
query_conventions({ query: "enforcement", severity: "CRITICAL" })
```

### validate_pattern

Validate TypeScript files against project conventions using AST analysis (ts-morph).

**Query Types:**
- `rules` - List all available validation rules
- `all` - Run all applicable validations on a file
- `summary` - Concise validation summary
- **CRITICAL Rules:**
  - `aws-sdk` - Check AWS SDK encapsulation
  - `electrodb` - Check ElectroDB mocking patterns
  - `config` - Check for configuration drift
  - `env` - Check environment variable validation
  - `cascade` - Check cascade deletion safety
- **HIGH Rules:**
  - `response` - Check response helper usage
  - `types` - Check exported type location
  - `batch` - Check batch operation retry handling
  - `scan` - Check scan pagination handling
- **HIGH Rules (documentation):**
  - `docs` - Check documentation sync with codebase
- **MEDIUM Rules:**
  - `imports` - Check import ordering
  - `enum` - Check ResponseStatus enum usage
  - `mock` - Check mock formatting patterns

**Validation Rules:**

| Rule | Alias | Severity | Description |
|------|-------|----------|-------------|
| aws-sdk-encapsulation | aws-sdk | CRITICAL | No direct AWS SDK imports outside src/lib/vendor/AWS/ |
| electrodb-mocking | electrodb | CRITICAL | Test files must use createElectroDBEntityMock() |
| config-enforcement | config | CRITICAL | Detects configuration drift (e.g., ESLint allowing underscore vars) |
| env-validation | env | CRITICAL | Raw process.env access must use getRequiredEnv() wrapper |
| cascade-safety | cascade | CRITICAL | Promise.all with delete operations must use Promise.allSettled |
| response-helpers | response | HIGH | Lambda handlers must use response() helper |
| types-location | types | HIGH | Exported types must be in src/types/ directory |
| batch-retry | batch | HIGH | Batch operations must use retryUnprocessed() wrapper |
| scan-pagination | scan | HIGH | Scan operations must use scanAllPages() wrapper |
| import-order | imports | MEDIUM | Imports grouped: node ‚Üí aws-lambda ‚Üí external ‚Üí entities ‚Üí vendor ‚Üí types ‚Üí utilities ‚Üí relative |
| response-enum | enum | MEDIUM | Use ResponseStatus enum instead of magic strings |
| mock-formatting | mock | MEDIUM | Sequential mock returns should be separate statements |
| doc-sync | docs | HIGH | Documentation stays in sync with codebase |

**Examples:**
```typescript
// Full validation of a Lambda handler
validate_pattern({ file: "src/lambdas/ListFiles/src/index.ts", query: "all" })

// Check only AWS SDK encapsulation
validate_pattern({ file: "src/lambdas/ListFiles/src/index.ts", query: "aws-sdk" })

// List all available rules
validate_pattern({ query: "rules" })
```

### check_coverage

Analyze which dependencies need mocking for Jest tests using build/graph.json.

**Query Types:**
- `required` - List all dependencies that need mocking
- `missing` - Compare required mocks to existing test file
- `all` - Full analysis with categorization
- `summary` - Quick summary of mock requirements

**Dependency Categories:**
- **Entities**: ElectroDB entities (use createElectroDBEntityMock)
- **Vendors**: AWS SDK wrappers (lib/vendor/AWS/*)
- **Utilities**: Shared helpers (util/*)
- **External**: Third-party packages

**Examples:**
```typescript
// Get all mocks needed for a Lambda
check_coverage({ file: "src/lambdas/ListFiles/src/index.ts", query: "required" })

// Find missing mocks in existing test
check_coverage({ file: "src/lambdas/ListFiles/src/index.ts", query: "missing" })
```

### lambda_impact

Show what's affected by changing a file - dependents, tests, and infrastructure.

**Query Types:**
- `dependents` - Direct files that import this file
- `cascade` - Full transitive dependency cascade
- `tests` - Test files that need updating
- `infrastructure` - Terraform files that may be affected
- `all` - Comprehensive impact analysis

**Examples:**
```typescript
// See what's affected by changing an entity
lambda_impact({ file: "src/entities/Files.ts", query: "cascade" })

// Find tests that need updating
lambda_impact({ file: "src/util/lambda-helpers.ts", query: "tests" })

// Full impact analysis
lambda_impact({ file: "src/entities/Users.ts", query: "all" })
```

### suggest_tests

Generate test file scaffolding with all required mocks based on dependency analysis.

**Query Types:**
- `scaffold` - Complete test file with all mocks
- `mocks` - Just the mock setup section
- `fixtures` - Suggested test fixtures
- `structure` - Test structure outline (describe/it blocks)

**Examples:**
```typescript
// Generate complete test file
suggest_tests({ file: "src/lambdas/NewLambda/src/index.ts", query: "scaffold" })

// Get just the mock setup
suggest_tests({ file: "src/lambdas/NewLambda/src/index.ts", query: "mocks" })
```

## CI Integration

The validation rules in `src/mcp/validation/` are designed for reuse in CI pipelines:

```typescript
import { validateFile, allRules } from './src/mcp/validation/index.js'

// Validate a file
const result = await validateFile('src/lambdas/ListFiles/src/index.ts')

// Check for CRITICAL violations
const critical = result.violations.filter(v => v.severity === 'CRITICAL')
if (critical.length > 0) {
  process.exit(1)
}
```

## Architecture

```
src/mcp/
‚îú‚îÄ‚îÄ server.ts              # MCP server with tool definitions
‚îú‚îÄ‚îÄ README.md              # Tool documentation
‚îú‚îÄ‚îÄ handlers/
‚îÇ   ‚îú‚îÄ‚îÄ conventions.ts     # query_conventions handler
‚îÇ   ‚îú‚îÄ‚îÄ coverage.ts        # check_coverage handler
‚îÇ   ‚îú‚îÄ‚îÄ validation.ts      # validate_pattern handler
‚îÇ   ‚îú‚îÄ‚îÄ impact.ts          # lambda_impact handler
‚îÇ   ‚îú‚îÄ‚îÄ test-scaffold.ts   # suggest_tests handler
‚îÇ   ‚îî‚îÄ‚îÄ data-loader.ts     # Shared data loading with caching
‚îú‚îÄ‚îÄ parsers/
‚îÇ   ‚îî‚îÄ‚îÄ convention-parser.ts  # Convention file parser
‚îî‚îÄ‚îÄ validation/
    ‚îú‚îÄ‚îÄ types.ts           # Shared validation types
    ‚îú‚îÄ‚îÄ index.ts           # Unified validation interface
    ‚îî‚îÄ‚îÄ rules/
        ‚îú‚îÄ‚îÄ aws-sdk-encapsulation.ts  # CRITICAL
        ‚îú‚îÄ‚îÄ electrodb-mocking.ts      # CRITICAL
        ‚îú‚îÄ‚îÄ config-enforcement.ts     # CRITICAL
        ‚îú‚îÄ‚îÄ env-validation.ts         # CRITICAL
        ‚îú‚îÄ‚îÄ cascade-safety.ts         # CRITICAL
        ‚îú‚îÄ‚îÄ response-helpers.ts       # HIGH
        ‚îú‚îÄ‚îÄ types-location.ts         # HIGH
        ‚îú‚îÄ‚îÄ batch-retry.ts            # HIGH
        ‚îú‚îÄ‚îÄ scan-pagination.ts        # HIGH
        ‚îú‚îÄ‚îÄ doc-sync.ts               # HIGH (documentation)
        ‚îú‚îÄ‚îÄ import-order.ts           # MEDIUM
        ‚îú‚îÄ‚îÄ response-enum.ts          # MEDIUM
        ‚îî‚îÄ‚îÄ mock-formatting.ts        # MEDIUM
```

## Related Documentation

- [Dependency Graph Analysis](../Testing/Dependency-Graph-Analysis.md) - build/graph.json usage
- [Jest ESM Mocking Strategy](../Testing/Jest-ESM-Mocking-Strategy.md) - Mocking patterns
- [SDK Encapsulation Policy](../AWS/SDK-Encapsulation-Policy.md) - AWS SDK rules
- [ElectroDB Testing Patterns](../Testing/ElectroDB-Testing-Patterns.md) - Entity mocking

---

*Use these MCP tools to understand and validate code against project conventions before implementation.*
</file>

<file path="docs/wiki/Testing/Jest-ESM-Mocking-Strategy.md">
# Jest ESM Mocking Strategy

## Quick Reference
- **When to use**: Writing unit tests with ES modules
- **Enforcement**: Required
- **Impact if violated**: HIGH - Obscure test failures

## The Problem

In ES modules, **ALL module-level code executes when ANY export is imported**. Missing mocks for transitive dependencies cause mysterious 500 errors.

```typescript
// YouTube.ts
import YTDlpWrap from 'yt-dlp-wrap'  // Executes even if unused
import {spawn} from 'child_process'  // Executes

export function getVideoID(url: string) { }  // What you imported
export function streamVideoToS3() { }        // Uses the above imports
```

When testing `getVideoID`, all imports execute. Solution: Mock ALL transitive dependencies.

## Mocking Pattern

```typescript
// test/index.test.ts

// 1. Mock ALL transitive dependencies BEFORE imports
jest.unstable_mockModule('yt-dlp-wrap', () => ({
  default: jest.fn()
}))

jest.unstable_mockModule('child_process', () => ({
  spawn: jest.fn()
}))

jest.unstable_mockModule('../../../lib/vendor/AWS/S3', () => ({
  createS3Upload: jest.fn()
}))

// 2. Import after mocking
const {handler} = await import('../src/index')

// 3. Use type assertions for mocks
import type {jest as mockJest} from '@jest/globals'
const mockYTDlp = (await import('yt-dlp-wrap')).default as mockJest.MockedFunction<any>
```

## Dependency Mapping

1. **Map direct imports** - What does your test file import?
2. **Map transitive imports** - What do those imports import?
3. **Mock all external deps** - AWS SDK, npm packages, vendor wrappers
4. **Match module structure** - Classes need constructor mocks

## Common Patterns

### AWS SDK Mocks
```typescript
jest.unstable_mockModule('@aws-sdk/client-dynamodb', () => ({
  DynamoDBClient: jest.fn(() => ({
    send: jest.fn()
  }))
}))
```

### Vendor Wrapper Mocks
```typescript
jest.unstable_mockModule('../../../lib/vendor/AWS/DynamoDB', () => ({
  getDynamoDbClient: jest.fn(),
  queryItems: jest.fn()
}))
```

### ElectroDB Mock Helper (CRITICAL)

**Zero-tolerance rule**: ALWAYS use `createElectroDBEntityMock()` from `test/helpers/electrodb-mock.ts` for mocking ElectroDB entities.

```typescript
import {createElectroDBEntityMock} from '../../../test/helpers/electrodb-mock'
import type {File} from '../../../types/domain-models'

// Create mock before mocking module
const filesMock = createElectroDBEntityMock<File>()

// Mock the entity module
jest.unstable_mockModule('../../../entities/Files', () => ({
  Files: filesMock.entity
}))

// Later in tests - use the mocks for assertions
filesMock.mocks.get.mockResolvedValue({
  data: {fileId: '123', status: 'Downloaded'}
})

expect(filesMock.mocks.create).toHaveBeenCalledWith({
  fileId: '123',
  // ... other properties
})
```

**Benefits**:
- Type-safe mocks with full TypeScript inference
- Consistent mock structure across all tests
- Simplified setup with pre-configured query patterns
- Supports all ElectroDB operations (get, create, update, query, etc.)

## Testing Checklist

- [ ] List all imports in test file
- [ ] Read source files, list their imports
- [ ] Mock ALL external dependencies
- [ ] Mock BEFORE importing handler
- [ ] Add TypeScript types to mocks
- [ ] Test locally AND in CI

## Common Issues

| Error | Cause | Fix |
|-------|-------|-----|
| Cannot find module | Missing mock | Add jest.unstable_mockModule |
| X is not a constructor | Wrong mock structure | Mock as class with constructor |
| Property X doesn't exist | Incomplete mock | Add missing properties |
| Works locally, fails CI | Environment differences | Mock all transitive deps |

## Best Practices

1. **Mock first, import second** - Always mock before importing
2. **Mock everything external** - All npm packages and AWS SDK
3. **Use type assertions** - Add proper types to mocks
4. **Use mock helpers** - ElectroDB mock helper for entities
5. **Map dependencies** - Trace all transitive imports

## Related Patterns

- [Mock Type Annotations](Mock-Type-Annotations.md) - TypeScript mock patterns
- [Lazy Initialization](Lazy-Initialization-Pattern.md) - Defer module execution
- [Coverage Philosophy](Coverage-Philosophy.md) - Test strategy

---

*Mock ALL transitive dependencies in ES modules to prevent obscure test failures.*
</file>

<file path="docs/wiki/TypeScript/Type-Definitions.md">
# Type Definitions

## Quick Reference
- **When to use**: Defining TypeScript types and interfaces
- **Enforcement**: Required - MCP `types-location` rule validates on push
- **Impact if violated**: HIGH - Type sprawl, duplication, poor IDE experience

## Exported Type Location (HIGH Priority)

**Rule**: All exported type definitions (type aliases, interfaces, enums) must be in the `src/types/` directory.

**Why**: Separation of concerns, discoverability, and maintainability. Types scattered across implementation files are hard to find and lead to duplication.

**Enforcement**: MCP `types-location` rule (HIGH severity) detects violations in CI.

### Allowed Exceptions

| Location | Reason |
|----------|--------|
| `src/types/**/*.ts` | Canonical location for types |
| `src/entities/**/*.ts` | Entity-derived types from ElectroDB |
| `src/mcp/**/*.ts` | Self-contained MCP module |
| `**/*.test.ts`, `test/**/*.ts` | Test-only types |
| `src/lib/vendor/**/*.ts` | Internal vendor wrapper types |

### Type File Organization

```
src/types/
‚îú‚îÄ‚îÄ domain-models.ts     # Core domain types (User, Device, etc.)
‚îú‚îÄ‚îÄ persistence-types.ts # Relationship types (UserDevice, UserFile)
‚îú‚îÄ‚îÄ enums.ts             # Shared enumerations
‚îú‚îÄ‚îÄ lambda-wrappers.ts   # Lambda handler wrapper types
‚îú‚îÄ‚îÄ video.ts             # Video processing types
‚îú‚îÄ‚îÄ util.ts              # Utility function types
‚îú‚îÄ‚îÄ youtube.ts           # YouTube/yt-dlp types
‚îî‚îÄ‚îÄ vendor/              # Third-party integration types
    ‚îî‚îÄ‚îÄ IFTTT/
```

### Examples

```typescript
// ‚úÖ CORRECT - Export from src/types/
// src/types/lambda-wrappers.ts
export type WrapperMetadata = {traceId: string}
export type ApiHandlerParams<TEvent> = {event: TEvent; context: Context; metadata: WrapperMetadata}

// ‚úÖ CORRECT - Import from types directory
// src/lambdas/ListFiles/src/index.ts
import type {ApiHandlerParams} from '#types/lambda-wrappers'

// ‚ùå WRONG - Exporting type from implementation file
// src/util/helpers.ts
export type HelperConfig = {maxRetries: number}  // Should be in src/types/util.ts
```

## The Rules

1. **Exported Types in `types/` Directory** - All exported types must be in `src/types/`
2. **Inline Types for Single-Use Cases** - Define internal (non-exported) types inline
3. **Entity Types with Entities** - ElectroDB entity types stay with entity definitions
4. **Use `import type` Syntax** - Better tree-shaking and clearer intent

## Examples

### ‚úÖ Correct - Inline Types

```typescript
// src/lambdas/ProcessVideo/src/index.ts

// Small, single-use type - defined inline
interface VideoMetadata {
  title: string
  duration: number
  format: string
}

export const handler = async (event: any) => {
  const metadata: VideoMetadata = extractMetadata(event)
  // Use metadata
}
```

### ‚úÖ Correct - Shared Types in types/

```typescript
// types/api.ts - Shared API types
export interface ApiGatewayEvent {
  body: string
  headers: Record<string, string>
  pathParameters: Record<string, string>
}

export interface ApiGatewayResponse {
  statusCode: number
  headers?: Record<string, string>
  body: string
}

// Used across multiple Lambda functions
```

### ‚ùå Incorrect - Type Sprawl

```typescript
// ‚ùå WRONG - Separate file for one type used once
// types/VideoMetadata.ts
export interface VideoMetadata {
  title: string
  duration: number
}

// Only used in one place - should be inline
```

### ‚ùå Incorrect - Duplicated Types

```typescript
// ‚ùå WRONG - Same type defined in multiple files
// src/lambdas/ListFiles/src/index.ts
interface ApiResponse {
  statusCode: number
  body: string
}

// src/lambdas/GetFile/src/index.ts
interface ApiResponse {  // Duplicate!
  statusCode: number
  body: string
}

// Should be shared in types/api.ts
```

## Type Organization Structure

```
types/
‚îú‚îÄ‚îÄ api.ts              # API Gateway types
‚îú‚îÄ‚îÄ domain.ts           # Domain models (User, File, Device)
‚îú‚îÄ‚îÄ events.ts           # AWS event types (SNS, SQS)
‚îú‚îÄ‚îÄ errors.ts           # Error types
‚îî‚îÄ‚îÄ validation.ts       # Validation constraint types

src/
‚îú‚îÄ‚îÄ entities/
‚îÇ   ‚îú‚îÄ‚îÄ Files.ts        # File entity with types
‚îÇ   ‚îú‚îÄ‚îÄ Users.ts        # User entity with types
‚îÇ   ‚îî‚îÄ‚îÄ Devices.ts      # Device entity with types
‚îú‚îÄ‚îÄ lambdas/
‚îÇ   ‚îî‚îÄ‚îÄ [Function]/
‚îÇ       ‚îî‚îÄ‚îÄ src/
‚îÇ           ‚îî‚îÄ‚îÄ index.ts  # Inline types for single-use
```

## When to Create Shared Types

Create types in `types/` directory when:

1. **Used in 3+ files** - Clear reuse pattern
2. **Cross-cutting concern** - API contracts, domain models
3. **External contract** - Types exposed to consumers
4. **Complex type** - Large types that clutter implementation files

Keep types inline when:

1. **Single use** - Only needed in one file
2. **Small** - 3-4 properties or less
3. **Implementation detail** - Not part of public API
4. **Tightly coupled** - Specific to one function's logic

## Type Naming Conventions

```typescript
// Interfaces use PascalCase
interface UserProfile {
  userId: string
  email: string
}

// Type aliases use PascalCase
type FileStatus = 'Queued' | 'Downloading' | 'Downloaded' | 'Failed'

// Generic type parameters use single uppercase letter
function map<T, R>(items: T[], fn: (item: T) => R): R[] {
  return items.map(fn)
}

// Avoid prefixing with 'I' or 'T'
// ‚ùå WRONG
interface IUser {}
type TFileStatus = string

// ‚úÖ CORRECT
interface User {}
type FileStatus = string
```

## Type Imports

```typescript
// Use 'type' keyword for type-only imports (better tree-shaking)
import type {ApiGatewayEvent} from '../../../types/api'
import type {User} from '../../../types/domain'

// Regular import for values (functions, classes, enums)
import {validateInput} from '../../../util/constraints'
```

## Lambda-Specific Types

Lambda functions may have types used only within that lambda. The rule for organizing these types follows a threshold-based approach.

### When to Create a types/ Directory

Create a `src/lambdas/[name]/types/` directory when ANY of these apply:

1. **3+ types** specific to that lambda
2. **Re-exported types** - Types exported from the handler for external consumers
3. **Complex types** - Types with 5+ properties or nested structures count as 2

### When to Use Inline Types

Define types inline in the handler when ALL of these apply:

1. **1-2 simple types** only
2. **Not re-exported** - Types are internal to the lambda
3. **Small types** - 4 or fewer properties, no nested structures

### Examples

```typescript
// ‚úÖ Keep types/ directory (3+ types)
// CloudfrontMiddleware has 3 CloudFront-related types
src/lambdas/CloudfrontMiddleware/types/index.ts

// ‚úÖ Keep types/ directory (re-exported)
// PruneDevices re-exports PruneDevicesResult for callers
src/lambdas/PruneDevices/types/index.ts
export type {PruneDevicesResult} from '../types'  // in handler

// ‚úÖ Inline types (1 simple type, not re-exported)
// FileCoordinator's DownloadInfo is only used internally
interface DownloadInfo {
  fileId: string
  correlationId?: string
}
```

### Current Lambda Type Organization

| Lambda | Types | Location | Reason |
|--------|-------|----------|--------|
| CloudfrontMiddleware | 3 | `types/` | 3+ types threshold |
| PruneDevices | 2 | `types/` | Re-exported for callers |
| FileCoordinator | 1 | Inline | Single simple type |
| LoginUser | 1 | Inline | Single simple type |
| RegisterDevice | 1 | Inline | Single simple type |
| RegisterUser | 1 | Inline | Single simple type |
| StartFileUpload | 1 | Inline | Single simple type |
| UserSubscribe | 1 | Inline | Single simple type |

## Enforcement

### Code Review Checklist

- [ ] Shared types in `types/` directory
- [ ] Single-use types defined inline
- [ ] No type-only files for trivial types
- [ ] Entity types stay with entity definitions
- [ ] No AWS SDK types in public APIs
- [ ] Type imports use `import type` keyword
- [ ] Type names follow PascalCase convention
- [ ] Lambda types follow 3+ threshold or re-export rule

## Related Patterns

- [Naming Conventions](../Conventions/Naming-Conventions.md) - PascalCase for types
- [Import Organization](../Conventions/Import-Organization.md) - Type import order

---

*Organize types based on usage. Inline for single-use, shared directory for cross-cutting types. Keep entity types with entities.*
</file>

<file path="src/entities/FileDownloads.ts">
import {documentClient, Entity} from '#lib/vendor/ElectroDB/entity'
// Re-export DownloadStatus enum from enums for consumers of this module
‚ãÆ----
/**
 * ElectroDB entity schema for tracking download attempts.
 *
 * This entity manages transient download state, separating download orchestration
 * concerns from the permanent File entity. This enables:
 *
 * - Clean separation: Files = media metadata, FileDownloads = download state
 * - Retry tracking without polluting file records
 * - Easy cleanup of transient state after successful downloads
 * - Better querying for pending/scheduled downloads
 *
 * Lifecycle:
 * 1. Created when a download is initiated (status: 'pending' or 'in_progress')
 * 2. Updated on errors with retry scheduling (status: 'scheduled')
 * 3. Marked 'completed' on success (can be deleted after File updated)
 * 4. Marked 'failed' when retries exhausted
 */
‚ãÆ----
/** YouTube video ID - matches File.fileId */
‚ãÆ----
/** Current download status */
‚ãÆ----
/** Number of download attempts made */
‚ãÆ----
/** Maximum retries allowed for this download */
‚ãÆ----
/** Unix timestamp (seconds) when retry should be attempted */
‚ãÆ----
/** Error category from video-error-classifier */
‚ãÆ----
/** Human-readable error message */
‚ãÆ----
/** Scheduled video release timestamp from yt-dlp (if known) */
‚ãÆ----
/** Source URL for the download */
‚ãÆ----
/** Timestamp when download was first initiated */
‚ãÆ----
/** Timestamp when download state was last updated */
‚ãÆ----
/** Correlation ID for end-to-end request tracing across Lambda chain */
‚ãÆ----
/** Unix timestamp (seconds) for DynamoDB TTL - auto-delete completed/failed records */
‚ãÆ----
/** Query downloads by status and retry time - enables FileCoordinator to find ready downloads */
‚ãÆ----
// Type exports for use in application code
export type FileDownloadItem = ReturnType<typeof FileDownloads.parse>
export type CreateFileDownloadInput = Parameters<typeof FileDownloads.create>[0]
export type UpdateFileDownloadInput = Parameters<typeof FileDownloads.update>[0]
</file>

<file path="src/entities/Sessions.ts">
import {documentClient, Entity} from '#lib/vendor/ElectroDB/entity'
/**
 * ElectroDB entity schema for Better Auth sessions.
 * Manages user authentication sessions with automatic expiration and device tracking.
 *
 * Better Auth Session Schema:
 * - id: unique session identifier
 * - userId: reference to user
 * - expiresAt: session expiration timestamp
 * - token: session token (hashed)
 * - ipAddress: client IP for security
 * - userAgent: client user agent for device tracking
 * - createdAt: session creation timestamp
 * - updatedAt: last activity timestamp
 */
‚ãÆ----
// Type exports for use in application code
export type SessionItem = ReturnType<typeof Sessions.parse>
export type CreateSessionInput = Parameters<typeof Sessions.create>[0]
export type UpdateSessionInput = Parameters<typeof Sessions.update>[0]
</file>

<file path="src/lambdas/LogClientEvent/test/index.test.ts">
import {beforeEach, describe, expect, test} from '@jest/globals'
import type {APIGatewayEvent} from 'aws-lambda'
import {testContext} from '#util/jest-setup'
</file>

<file path="src/lambdas/RefreshToken/test/index.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {APIGatewayProxyEvent} from 'aws-lambda'
import {testContext} from '#util/jest-setup'
import {v4 as uuidv4} from 'uuid'
import type {SessionPayload} from '#types/util'
‚ãÆ----
validateSessionToken: validateSessionTokenMock, // fmt: multiline
</file>

<file path="src/lib/vendor/AWS/CloudWatch.ts">
import {PutMetricDataCommand, StandardUnit} from '@aws-sdk/client-cloudwatch'
import type {PutMetricDataCommandInput, PutMetricDataCommandOutput} from '@aws-sdk/client-cloudwatch'
import {createCloudWatchClient} from './clients'
‚ãÆ----
// Map simple unit strings to AWS StandardUnit values (internal use only)
‚ãÆ----
export function putMetricData(params: PutMetricDataCommandInput): Promise<PutMetricDataCommandOutput>
/**
 * Get the AWS StandardUnit value for a simple unit string
 * @param unit - Simple unit string (Count, Seconds, Bytes, None, etc.)
 * @returns AWS StandardUnit value, defaults to Count if not found
 */
export function getStandardUnit(unit?: string): StandardUnit
</file>

<file path="src/lib/vendor/AWS/Lambda.ts">
import {InvokeCommand} from '@aws-sdk/client-lambda'
import type {InvokeCommandInput, InvokeCommandOutput} from '@aws-sdk/client-lambda'
import {createLambdaClient} from './clients'
‚ãÆ----
/**
 * Invokes a Lambda function
 * @param params - The invocation parameters
 * @returns The invocation response
 */
/* c8 ignore start - Pure AWS SDK wrapper, tested via integration tests */
export function invokeLambda(params: InvokeCommandInput): Promise<InvokeCommandOutput>
/* c8 ignore stop */
/**
 * Invokes a Lambda function asynchronously
 * @param functionName - The name of the Lambda function
 * @param payload - The payload to send to the function
 * @returns The invocation response
 */
/* c8 ignore start - Thin wrapper with minimal logic, tested via integration tests */
export async function invokeAsync(functionName: string, payload: Record<string, unknown>): Promise<InvokeCommandOutput>
/* c8 ignore stop */
</file>

<file path="src/mcp/handlers/data-loader.ts">
/**
 * Shared data loader for MCP handlers
 * Reads from the same sources as GraphRAG for consistency:
 * - build/graph.json for dependencies
 * - graphrag/metadata.json for semantic info
 * - docs/conventions-tracking.md for conventions
 * - docs/wiki/ for detailed documentation
 * - Filesystem for Lambda/Entity discovery
 */
import fs from 'fs/promises'
import path from 'path'
import {fileURLToPath} from 'url'
import {type Convention, parseConventions} from '../parsers/convention-parser'
‚ãÆ----
interface DependencyGraph {
  metadata: {generated: string; projectRoot: string; totalFiles: number}
  files: Record<string, {file: string; imports: string[]}>
  transitiveDependencies: Record<string, string[]>
}
interface LambdaMetadata {
  trigger: string
  purpose: string
}
interface ServiceMetadata {
  name: string
  type: string
  vendorPath?: string | null
  description?: string
}
interface EntityRelationship {
  from: string
  to: string
  type: string
}
interface Metadata {
  lambdas: Record<string, LambdaMetadata>
  externalServices: ServiceMetadata[]
  awsServices: ServiceMetadata[]
  entityRelationships: EntityRelationship[]
  lambdaInvocations: Array<{from: string; to: string; via: string}>
  serviceToServiceEdges: Array<{from: string; to: string; relationship: string; event?: string}>
}
// Cache for loaded data
‚ãÆ----
const CACHE_TTL = 60000 // 1 minute cache
/**
 * Load metadata from graphrag/metadata.json
 */
export async function loadMetadata(): Promise<Metadata>
/**
 * Load dependency graph from build/graph.json
 */
export async function loadDependencyGraph(): Promise<DependencyGraph>
/**
 * Discover Lambda names from src/lambdas/ directory
 */
export async function discoverLambdas(): Promise<string[]>
/**
 * Discover Entity names from src/entities/ directory
 */
export async function discoverEntities(): Promise<string[]>
/**
 * Get Lambda configuration by combining filesystem discovery with metadata
 */
export async function getLambdaConfigs(): Promise<
  Record<
    string,
    {name: string; trigger: string; purpose: string; dependencies: string[]; entities: string[]}
  >
> {
  const [lambdaNames, metadata, depGraph, entityNames] = await Promise.all([discoverLambdas(), loadMetadata(), loadDependencyGraph(), discoverEntities()])
  const configs: Record<string, {name: string; trigger: string; purpose: string; dependencies: string[]; entities: string[]}> = {}
for (const name of lambdaNames)
‚ãÆ----
// Extract AWS services from dependencies
‚ãÆ----
// AWS services
‚ãÆ----
// Entities
‚ãÆ----
/**
 * Get entity schemas and relationships
 */
export async function getEntityInfo(): Promise<
/**
 * Get Lambda invocation chains
 */
export async function getLambdaInvocations(): Promise<Array<
/**
 * Get external services
 */
export async function getExternalServices(): Promise<ServiceMetadata[]>
/**
 * Get AWS services
 */
export async function getAwsServices(): Promise<ServiceMetadata[]>
/**
 * Load conventions from docs/conventions-tracking.md
 */
export async function loadConventions(): Promise<Convention[]>
/**
 * Load a specific wiki page by relative path
 */
export async function loadWikiPage(relativePath: string): Promise<string>
/**
 * Discover all wiki pages in docs/wiki/
 */
export async function discoverWikiPages(): Promise<string[]>
‚ãÆ----
async function walkDir(dir: string): Promise<void>
‚ãÆ----
// Return relative path from project root
‚ãÆ----
/**
 * Search wiki pages for a term (searches file names and content)
 */
export async function searchWikiPages(term: string): Promise<Array<
‚ãÆ----
// Check filename
‚ãÆ----
// Find content matches (lines containing the term)
‚ãÆ----
results.push({path: pagePath, matches: matchingLines.slice(0, 5)}) // Limit to 5 matches per file
</file>

<file path="src/mcp/validation/rules/response-helpers.ts">
/**
 * Response Helpers Rule
 * HIGH: Lambda handlers must use response() helper, not raw objects
 *
 * Ensures consistent response formatting across all Lambda functions.
 */
import type {SourceFile} from 'ts-morph'
import {SyntaxKind} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
// Only check Lambda handler files
‚ãÆ----
// Check if response helper is imported
‚ãÆ----
// Find return statements
‚ãÆ----
// Check for raw response objects: { statusCode: ..., body: ... }
‚ãÆ----
// Detect raw response pattern
‚ãÆ----
// This looks like a raw Lambda response object
‚ãÆ----
// Check for Promise.resolve with raw objects
‚ãÆ----
// If no response helper is imported but file has return statements with API responses
‚ãÆ----
// Check if this Lambda returns API Gateway responses
</file>

<file path="test/helpers/better-auth-mock.ts">
import {jest} from '@jest/globals'
import type {SignInSocialParams, SignInSocialResult} from '#types/better-auth'
/**
 * Better Auth Mock Structure
 * Provides type-safe mocks for Better Auth API methods
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/Jest-ESM-Mocking-Strategy | Jest ESM Mocking Strategy}
 */
interface BetterAuthMock {
  /** The auth object to pass to jest.unstable_mockModule */
  auth: {api: {signInSocial: jest.Mock<(params: SignInSocialParams) => Promise<SignInSocialResult>>}}
  /** Individual mock functions for assertions and setup */
  mocks: {signInSocial: jest.Mock<(params: SignInSocialParams) => Promise<SignInSocialResult>>}
}
‚ãÆ----
/** The auth object to pass to jest.unstable_mockModule */
‚ãÆ----
/** Individual mock functions for assertions and setup */
‚ãÆ----
/**
 * Creates a Better Auth mock with all API methods
 *
 * @returns BetterAuthMock object containing both the module export and individual mocks
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/Jest-ESM-Mocking-Strategy | Jest ESM Mocking Strategy}
 */
export function createBetterAuthMock(): BetterAuthMock
</file>

<file path="test/helpers/better-auth-test-data.ts">
/**
 * Shared test data builders for Better Auth entities
 * Used by both unit tests and integration tests to ensure consistency
 *
 * These builders provide sensible defaults and accept overrides for specific test cases.
 * This follows the Object Mother pattern for test data creation.
 */
/**
 * Identity provider data structure (from Sign in with Apple)
 */
type IdentityProvidersData = {
  userId: string
  email: string
  emailVerified: boolean
  isPrivateEmail: boolean
  accessToken: string
  refreshToken: string
  tokenType: string
  expiresAt: number
}
/**
 * User entity structure for ElectroDB
 */
export type MockUserData = {
  userId: string
  email: string
  emailVerified: boolean
  firstName: string
  lastName: string
  identityProviders: IdentityProvidersData
  createdAt?: number
  updatedAt?: number
}
/**
 * Session entity structure for ElectroDB
 */
export type MockSessionData = {
  sessionId: string
  userId: string
  expiresAt: number
  token: string
  ipAddress?: string
  userAgent?: string
  deviceId?: string
  createdAt?: number
  updatedAt?: number
}
/**
 * Account entity structure for ElectroDB
 */
export type MockAccountData = {
  accountId: string
  userId: string
  providerId: string
  providerAccountId: string
  accessToken?: string
  refreshToken?: string
  expiresAt?: number
  scope?: string
  tokenType?: string
  idToken?: string
  createdAt?: number
  updatedAt?: number
}
/**
 * Verification token entity structure for ElectroDB
 */
export type MockVerificationTokenData = {identifier: string; token: string; expiresAt: number}
/**
 * Create mock user with sensible defaults
 *
 * @param overrides - Partial user data to override defaults
 * @returns Complete user object ready for ElectroDB entity operations
 */
export function createMockUser(overrides?: Partial<MockUserData>): MockUserData
/**
 * Create mock session with sensible defaults
 *
 * @param overrides - Partial session data to override defaults
 * @returns Complete session object ready for ElectroDB entity operations
 */
export function createMockSession(overrides?: Partial<MockSessionData>): MockSessionData
‚ãÆ----
expiresAt: now + 30 * 24 * 60 * 60 * 1000, // 30 days default
‚ãÆ----
/**
 * Create mock OAuth account with sensible defaults
 *
 * @param overrides - Partial account data to override defaults
 * @returns Complete account object ready for ElectroDB entity operations
 */
export function createMockAccount(overrides?: Partial<MockAccountData>): MockAccountData
‚ãÆ----
expiresAt: now + 3600000, // 1 hour default
‚ãÆ----
/**
 * Create mock verification token with sensible defaults
 *
 * @param overrides - Partial token data to override defaults
 * @returns Complete verification token object ready for ElectroDB entity operations
 */
export function createMockVerificationToken(overrides?: Partial<MockVerificationTokenData>): MockVerificationTokenData
‚ãÆ----
expiresAt: Date.now() + 86400000, // 24 hours default
‚ãÆ----
/**
 * Create minimal user for testing edge cases
 * Only required fields populated
 */
export function createMinimalUser(overrides?: Partial<MockUserData>): MockUserData
/**
 * Create session without optional fields
 * Tests that optional fields are handled correctly
 */
export function createMinimalSession(overrides?: Partial<MockSessionData>): MockSessionData
/**
 * Create account without optional OAuth metadata
 * Tests basic account linking without token details
 */
export function createMinimalAccount(overrides?: Partial<MockAccountData>): MockAccountData
</file>

<file path="typedoc.json">
{
  "entryPoints": [
    "src/lambdas/*",
    "src/lib/*",
    "src/types/*",
    "src/util/*"
  ],
  "exclude": [
    "**/*+(.spec|.e2e|.test).ts",
    "src/types/terraform.d.ts",
    "src/types/global.d.ts"
  ],
  "entryPointStrategy": "expand",
  "excludeNotDocumented": true,
  "excludeNotDocumentedKinds": [
    "Module",
    "Namespace",
    "Variable",
    "Function",
    "Constructor",
    "Method",
    "CallSignature",
    "IndexSignature",
    "ConstructorSignature",
    "Accessor",
    "GetSignature",
    "SetSignature",
    "Reference"
  ],
  "intentionallyNotExported": [
    "ElectroUserItem",
    "ElectroSessionItem",
    "ElectroAccountItem",
    "ElectroUserCreate",
    "ElectroSessionCreate",
    "ElectroAccountCreate",
    "ElectroUserUpdate",
    "ElectroSessionUpdate",
    "EntityMapConstraint",
    "BetterAuthHookContext"
  ],
  "externalSymbolLinkMappings": {
    "@types/node": {
      "EventEmitter.defaultMaxListeners": "https://nodejs.org/api/events.html#eventseventsizemaxlisteners",
      "Stream.pipe": "https://nodejs.org/api/stream.html#readablepipedestination-options",
      "Stream.Readable.push": "https://nodejs.org/api/stream.html#readablepushchunk-encoding",
      "Stream.Readable._read": "https://nodejs.org/api/stream.html#readable_readsize"
    }
  },
  "tsconfig": "./tsconfig.json",
  "out": "docs/source"
}
</file>

<file path="bin/test-registerDevice.sh">
#!/usr/bin/env bash
# Get the directory of this file
bin_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" > /dev/null 2>&1 && pwd)"
cd "${bin_dir}/../terraform"
domain=$(tofu output cloudfront_distribution_domain | tr -d '"')
api_key=$(tofu output api_gateway_api_key | tr -d '"')
REQUEST_URL="https://${domain}/registerDevice?ApiKey=${api_key}"
echo "Calling ${REQUEST_URL}"
# Fixed synthetic test values - idempotent (same device on each run)
curl -v -X POST \
  -H "Content-Type: application/json" \
  -H "Accept: application/json" \
  -H "User-Agent: localhost@lifegames" \
  -d '{
    "deviceId": "00000000-0000-0000-0000-000000000001",
    "token": "0000000000000000000000000000000000000000000000000000000000000001",
    "name": "RemoteTestDevice",
    "systemName": "iOS",
    "systemVersion": "99.0.0"
  }' \
  "$REQUEST_URL" | jq
</file>

<file path="src/entities/Users.ts">
import {documentClient, Entity} from '#lib/vendor/ElectroDB/entity'
/**
 * ElectroDB entity schema for the Users DynamoDB table.
 * This entity manages user accounts and identity provider information.
 */
‚ãÆ----
/** Flattened Apple device ID for GSI lookup (denormalized from identityProviders.userId) */
‚ãÆ----
// Type exports for use in application code
export type UserItem = ReturnType<typeof Users.parse>
export type CreateUserInput = Parameters<typeof Users.create>[0]
export type UpdateUserInput = Parameters<typeof Users.update>[0]
</file>

<file path="src/lib/vendor/ElectroDB/service.ts">
/**
 * ElectroDB Service Wrapper
 *
 * This module wraps the ElectroDB Service class to encapsulate the vendor library.
 * Domain layer should import from this wrapper, not directly from 'electrodb'.
 *
 * Follows the same pattern as AWS SDK encapsulation in lib/vendor/AWS/*.
 */
import {Entity, Service} from 'electrodb'
import type {DynamoDBDocumentClient} from '@aws-sdk/lib-dynamodb'
import {documentClient} from '#lib/vendor/AWS/DynamoDB'
/**
 * Entity map type constraint for createService.
 * Uses ElectroDB's Entity type with loosened generic constraints.
 * @internal
 */
// eslint-disable-next-line @typescript-eslint/no-explicit-any
type EntityMapConstraint = Record<string, Entity<any, any, any, any>>
/**
 * Re-export documentClient for service configuration.
 * Services need this to configure their DynamoDB client.
 */
‚ãÆ----
/**
 * Creates an ElectroDB Service with the provided entities and configuration.
 *
 * A Service combines multiple entities to enable efficient JOIN-like queries
 * through collections. This is the ONLY way services should be created -
 * never import Service from 'electrodb' directly.
 *
 * NOTE: This function is generic to preserve TypeScript type inference for
 * collections. The entity types are passed through to the Service constructor,
 * allowing ElectroDB to infer collection types correctly.
 *
 * @param entities - Map of entity names to entity instances
 * @param config - Service configuration (client, table name)
 * @returns Configured ElectroDB service instance with full type inference
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/ElectroDB-Testing-Patterns#collections-testing-join-operations | ElectroDB Service Usage}
 */
export function createService<E extends EntityMapConstraint>(entities: E, config:
/**
 * Re-export Service type for type annotations (not for instantiation).
 * Use createService() to create instances.
 */
</file>

<file path="src/lib/vendor/YouTube.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import {EventEmitter} from 'events'
import {Readable} from 'stream'
// Mock yt-dlp-wrap
‚ãÆ----
class MockYTDlpWrap
‚ãÆ----
constructor(public binaryPath: string)
‚ãÆ----
// Mock child_process - only yt-dlp spawning now (no ffmpeg)
‚ãÆ----
// Helper to create mock yt-dlp process
interface MockProcess extends EventEmitter {
  stdout: Readable
  stderr: EventEmitter
  kill: jest.Mock
}
function createMockProcess(): MockProcess
‚ãÆ----
process.stdout = new Readable(
‚ãÆ----
// Mock fs - for temp file operations
‚ãÆ----
// Mock S3 Upload - simplified (no EventEmitter progress tracking needed)
‚ãÆ----
// Mock CloudWatch vendor wrapper
‚ãÆ----
// Set up environment variable before importing
‚ãÆ----
// Import after mocking
‚ãÆ----
mockStat.mockResolvedValue({size: 52428800}) // 50MB default
‚ãÆ----
mockCreateReadStream.mockReturnValue(new Readable(
‚ãÆ----
// Simulate successful yt-dlp exit
‚ãÆ----
// Verify yt-dlp was called with file output (NOT -o -)
‚ãÆ----
// Verify cookies were copied
‚ãÆ----
// Verify temp file was read and uploaded
‚ãÆ----
// Verify cleanup
‚ãÆ----
// Verify cleanup attempted
</file>

<file path="src/mcp/validation/rules/config-enforcement.ts">
/**
 * Config Enforcement Rule
 * CRITICAL: Detects configuration changes that weaken enforcement standards
 *
 * This rule validates configuration files to ensure they don't introduce
 * patterns that violate project conventions documented in AGENTS.md:
 * - "Avoid backwards-compatibility hacks like renaming unused _vars"
 *
 * @see docs/conventions-tracking.md
 */
import type {SourceFile} from 'ts-morph'
import {createViolation} from '../types'
import type {ValidationRule, Violation} from '../types'
‚ãÆ----
/**
 * Allowed ESLint ignores - anything not in this list triggers a HIGH severity warning
 */
‚ãÆ----
/**
 * TSConfig strict mode settings that should never be disabled
 */
‚ãÆ----
validate(sourceFile: SourceFile, filePath: string): Violation[]
‚ãÆ----
function validateEslintConfig(text: string): Violation[]
‚ãÆ----
// Check for underscore ignore patterns in no-unused-vars
// These patterns violate AGENTS.md: "Avoid backwards-compatibility hacks like renaming unused _vars"
‚ãÆ----
// Check for unauthorized ignores
‚ãÆ----
function validateTsConfig(text: string): Violation[]
‚ãÆ----
// If we can't parse JSON, skip validation (will be caught by other tools)
‚ãÆ----
function validateDprintConfig(text: string): Violation[]
‚ãÆ----
// Check for excessively wide line width (relaxation from project standards)
‚ãÆ----
// Check for tabs (project standardized on spaces)
‚ãÆ----
// If we can't parse JSON, skip validation
‚ãÆ----
function findLineNumber(text: string, pattern: RegExp): number
function findLineNumberForString(text: string, searchString: string): number
</file>

<file path="src/mcp/server.ts">
/**
 * Model Context Protocol (MCP) Server for Media Downloader
 * Provides queryable interfaces for:
 * - ElectroDB entity schemas and relationships
 * - Lambda function configurations
 * - AWS infrastructure queries
 * - Dependency graph analysis
 * - Project conventions and patterns
 * - Code validation and coverage analysis
 * - Impact analysis and test scaffolding
 */
import {Server} from '@modelcontextprotocol/sdk/server/index.js'
import {StdioServerTransport} from '@modelcontextprotocol/sdk/server/stdio.js'
import {CallToolRequestSchema, ListToolsRequestSchema} from '@modelcontextprotocol/sdk/types.js'
import {handleElectroDBQuery} from './handlers/electrodb.js'
import {handleLambdaQuery} from './handlers/lambda.js'
import {handleInfrastructureQuery} from './handlers/infrastructure.js'
import {handleConventionsQuery} from './handlers/conventions.js'
import type {ConventionQueryArgs} from './handlers/conventions.js'
import {handleCoverageQuery} from './handlers/coverage.js'
import type {CoverageQueryArgs} from './handlers/coverage.js'
import {handleValidationQuery} from './handlers/validation.js'
import type {ValidationQueryArgs} from './handlers/validation.js'
import {handleImpactQuery} from './handlers/impact.js'
import type {ImpactQueryArgs} from './handlers/impact.js'
import {handleTestScaffoldQuery} from './handlers/test-scaffold.js'
import type {TestScaffoldQueryArgs} from './handlers/test-scaffold.js'
import {handleNamingValidationQuery, handleTypeAlignmentQuery} from './handlers/naming.js'
// Create server instance
‚ãÆ----
/**
 * Wrap handler result in MCP content format
 */
function wrapResult(result: unknown)
// Define available tools
‚ãÆ----
{ // fmt: multiline
‚ãÆ----
// Handle tool calls
‚ãÆ----
interface GraphFileData {
  imports?: string[]
}
interface GraphData {
  graph: Record<string, GraphFileData>
  transitiveDependencies: Record<string, string[]>
  circularDependencies?: string[][]
}
/**
 * Handle dependency graph queries
 */
async function handleDependencyQuery(args:
‚ãÆ----
// List all files with their dependent counts
‚ãÆ----
// Find who imports this specific file
‚ãÆ----
// Start the server
async function main()
</file>

<file path="src/types/enums.ts">
export enum UserStatus {
  Authenticated,
  Unauthenticated,
  Anonymous
}
/**
 * File status for permanent media records.
 *
 * These are FINAL states only - all transient/orchestration states
 * (retries, scheduling, in_progress) are in DownloadStatus.
 *
 * - Queued: File record exists, download not yet complete
 * - Downloading: Currently being downloaded
 * - Downloaded: Successfully downloaded, ready for users
 * - Failed: Permanently failed, will not be available
 */
export enum FileStatus {
  Queued = 'Queued',
  Downloading = 'Downloading',
  Downloaded = 'Downloaded',
  Failed = 'Failed'
}
/**
 * Download status for transient orchestration state.
 *
 * Tracks the download lifecycle in FileDownloads entity:
 * - pending: Queued, waiting for FileCoordinator to pick up
 * - in_progress: Currently being downloaded
 * - scheduled: Failed but scheduled for retry (retryAfter set)
 * - completed: Successfully downloaded (can be cleaned up)
 * - failed: Permanently failed, no more retries
 */
export enum DownloadStatus {
  Pending = 'pending',
  InProgress = 'in_progress',
  Scheduled = 'scheduled',
  Completed = 'completed',
  Failed = 'failed'
}
export enum ResponseStatus {
  Dispatched = 'Dispatched',
  Initiated = 'Initiated',
  Accepted = 'Accepted',
  Success = 'Success'
}
</file>

<file path="src/types/lambda-wrappers.ts">
/**
 * Lambda Handler Wrapper Types
 *
 * Type definitions for the Lambda handler wrapper functions in lambda-helpers.ts.
 * These types enable object destructuring in handler signatures, eliminating
 * the need for underscore-prefixed unused parameters.
 *
 * @see src/util/lambda-helpers.ts - Implementation of wrapper functions
 * @see docs/wiki/TypeScript/Lambda-Function-Patterns.md - Usage patterns
 */
import type {APIGatewayRequestAuthorizerEvent, Context, ScheduledEvent} from 'aws-lambda'
import type {CustomAPIGatewayRequestAuthorizerEvent} from './infrastructure-types'
import type {UserStatus} from './enums'
/** Metadata passed to all wrapped handlers */
export type WrapperMetadata = {traceId: string}
/** Parameters passed to wrapped API Gateway handlers */
export type ApiHandlerParams<TEvent = CustomAPIGatewayRequestAuthorizerEvent> = {event: TEvent; context: Context; metadata: WrapperMetadata}
/** Parameters passed to wrapped authorizer handlers */
export type AuthorizerParams = {event: APIGatewayRequestAuthorizerEvent; context: Context; metadata: WrapperMetadata}
/** Parameters passed to wrapped event handlers (S3, SQS) */
export type EventHandlerParams<TRecord> = {record: TRecord; context: Context; metadata: WrapperMetadata}
/** Parameters passed to wrapped scheduled handlers */
export type ScheduledHandlerParams = {event: ScheduledEvent; context: Context; metadata: WrapperMetadata}
/** Parameters passed to Lambda-to-Lambda invoke handlers */
export type LambdaInvokeHandlerParams<TEvent> = {event: TEvent; context: Context; metadata: WrapperMetadata}
/**
 * Parameters passed to authenticated API handlers.
 * userId is guaranteed to be a string (not optional) - the wrapper enforces this
 * by rejecting Unauthenticated and Anonymous users before the handler runs.
 */
export type AuthenticatedApiParams<TEvent = CustomAPIGatewayRequestAuthorizerEvent> = {
  event: TEvent
  context: Context
  metadata: WrapperMetadata
  userId: string
}
/**
 * Parameters passed to optional-auth API handlers.
 * Allows both Anonymous and Authenticated users (rejects only Unauthenticated).
 * Handler receives userStatus to differentiate behavior.
 */
export type OptionalAuthApiParams<TEvent = CustomAPIGatewayRequestAuthorizerEvent> = {
  event: TEvent
  context: Context
  metadata: WrapperMetadata
  userId: string | undefined
  userStatus: UserStatus
}
</file>

<file path="test/integration/helpers/s3-helpers.ts">
/**
 * S3 Test Helpers
 *
 * Utilities for creating buckets and verifying S3 uploads in LocalStack
 */
import {createBucket, deleteBucket, deleteObject as deleteS3Object, getObject, headObject, listObjectsV2} from '../lib/vendor/AWS/S3'
import {Readable} from 'stream'
/**
 * Create a test bucket in LocalStack S3
 */
export async function createTestBucket(bucketName: string): Promise<void>
‚ãÆ----
// Bucket might already exist
‚ãÆ----
/**
 * Delete a test bucket and all its contents from LocalStack S3
 */
export async function deleteTestBucket(bucketName: string): Promise<void>
‚ãÆ----
// First, delete all objects in the bucket
‚ãÆ----
// Then delete the bucket
‚ãÆ----
// Bucket might not exist
‚ãÆ----
/**
 * Check if an object exists in S3
 */
export async function objectExists(bucketName: string, key: string): Promise<boolean>
/**
 * Get object metadata from S3
 */
export async function getObjectMetadata(bucketName: string, key: string): Promise<
/**
 * Get object content from S3 as buffer
 */
export async function getObjectContent(bucketName: string, key: string): Promise<Buffer | null>
‚ãÆ----
// Convert stream to buffer
‚ãÆ----
/**
 * Delete an object from S3
 */
export async function deleteObject(bucketName: string, key: string): Promise<void>
‚ãÆ----
// Object might not exist
‚ãÆ----
/**
 * List all objects in a bucket
 */
export async function listObjects(bucketName: string): Promise<string[]>
</file>

<file path="test/integration/lib/vendor/AWS/S3.ts">
/**
 * S3 Test Vendor Wrapper
 *
 * Encapsulates AWS SDK S3 operations used in integration tests.
 * This wrapper exists to maintain the AWS SDK Encapsulation Policy even in test code.
 */
import {CreateBucketCommand, DeleteBucketCommand, DeleteObjectCommand, GetObjectCommand, HeadObjectCommand, ListObjectsV2Command} from '@aws-sdk/client-s3'
import type {GetObjectCommandOutput, HeadObjectCommandOutput} from '@aws-sdk/client-s3'
import {createS3Client} from '#lib/vendor/AWS/clients'
‚ãÆ----
/**
 * Creates an S3 bucket
 * @param bucketName - Name of the bucket to create
 */
export async function createBucket(bucketName: string): Promise<void>
/**
 * Deletes an S3 bucket
 * @param bucketName - Name of the bucket to delete
 */
export async function deleteBucket(bucketName: string): Promise<void>
/**
 * Lists all objects in a bucket
 * @param bucketName - Name of the bucket
 */
export async function listObjectsV2(bucketName: string): Promise<
/**
 * Deletes an object from S3
 * @param bucketName - Name of the bucket
 * @param key - Object key
 */
export async function deleteObject(bucketName: string, key: string): Promise<void>
/**
 * Gets object metadata (HEAD request)
 * @param bucketName - Name of the bucket
 * @param key - Object key
 */
export async function headObject(bucketName: string, key: string): Promise<HeadObjectCommandOutput>
/**
 * Gets object content
 * @param bucketName - Name of the bucket
 * @param key - Object key
 */
export async function getObject(bucketName: string, key: string): Promise<GetObjectCommandOutput>
</file>

<file path="test/integration/workflows/betterAuth.entities.integration.test.ts">
/**
 * Better Auth Entities Integration Tests
 *
 * Tests Better Auth ElectroDB entities (Users, Sessions, Accounts, VerificationTokens)
 * against LocalStack DynamoDB to validate:
 * - Entity CRUD operations
 * - GSI queries (byEmail, byUser, byToken, byProvider)
 * - Collections queries (userSessions, userAccounts)
 * - Complete authentication flows
 */
// CRITICAL: Set environment variables BEFORE any imports that use them
// ES modules evaluate imports before file code runs, but we can set env vars
// before the dynamic imports below are resolved
‚ãÆ----
import {afterAll, afterEach, beforeAll, describe, expect, it} from '@jest/globals'
import {cleanupLocalStackTable, setupLocalStackTable} from '../helpers/electrodb-localstack'
// Dynamic imports to ensure env vars are set before entity instantiation
‚ãÆ----
// Type helpers for ElectroDB service collections
// ElectroDB collections return an object with entity-named arrays (using the keys from service creation)
// Keys match the service registration: sessions, accounts (lowercase)
interface UserSessionsData {
  sessions: Array<{sessionId: string; [key: string]: unknown}>
}
interface UserAccountsData {
  accounts: Array<{accountId: string; providerId: string; [key: string]: unknown}>
}
type ServiceCollections = {
  userSessions: (params: {userId: string}) => {go: () => Promise<{data: UserSessionsData}>}
  userAccounts: (params: {userId: string}) => {go: () => Promise<{data: UserAccountsData}>}
}
‚ãÆ----
// Clean up test data after each test
// Note: In production, you'd want more sophisticated cleanup
‚ãÆ----
// Note: createdAt/updatedAt are auto-generated by DynamoDB and verified in other tests
‚ãÆ----
// Verify sorted by expiresAt (composite sort key)
‚ãÆ----
// Create sessions for the user
‚ãÆ----
// Query collection - returns only Sessions (entity key matches service registration: sessions)
‚ãÆ----
// Create OAuth accounts for the user
‚ãÆ----
// Query collection - returns only Accounts (entity key matches service registration: accounts)
‚ãÆ----
// Step 1: Register user
‚ãÆ----
// Step 2: Link OAuth account (Apple)
‚ãÆ----
// Step 3: Create session
‚ãÆ----
// Verify: User can be found by email
‚ãÆ----
// Verify: Account can be found by provider
‚ãÆ----
// Verify: Sessions can be queried
‚ãÆ----
// Verify: Accounts can be queried by user
‚ãÆ----
// Create expired session
‚ãÆ----
// Create active session
‚ãÆ----
// Query all sessions
‚ãÆ----
// Filter to active only (application-level filtering)
‚ãÆ----
// Attempting to create account with same accountId should fail
</file>

<file path="test/integration/setup.ts">
/**
 * Integration Test Setup
 *
 * Configures the test environment for LocalStack integration tests.
 * This file runs before all integration tests via setupFilesAfterEnv in jest.integration.config.mjs
 */
import {beforeAll, jest} from '@jest/globals'
/**
 * Ensure USE_LOCALSTACK is set
 * This triggers all vendor wrappers to use LocalStack clients instead of production AWS
 */
‚ãÆ----
/**
 * Set AWS region for LocalStack
 * Match production region (us-west-2) for consistency
 */
‚ãÆ----
/**
 * Configure test timeout
 * Integration tests may take longer due to LocalStack initialization
 */
‚ãÆ----
/**
 * Wait for LocalStack to be ready
 * Checks LocalStack health endpoint before running tests
 */
async function waitForLocalStack(): Promise<void>
‚ãÆ----
// LocalStack not ready yet, continue retrying
‚ãÆ----
/**
 * Run health check before tests
 * Only check if not in CI environment (CI will handle LocalStack lifecycle)
 */
</file>

<file path="GEMINI.md">
# See AGENTS.md

This project uses AGENTS.md as the single source of truth for AI coding assistant context.

Please see AGENTS.md in the repository root for comprehensive project documentation and guidelines.
</file>

<file path=".github/scripts/sync-wiki.sh">
#!/usr/bin/env bash
# Wiki Sync Script
# Syncs docs/wiki/ content to GitHub Wiki repository
set -euo pipefail
# Configuration
SOURCE_DIR="${SOURCE_DIR:-main/docs/wiki}"
WIKI_DIR="${WIKI_DIR:-wiki}"
REPO_URL="${REPO_URL:-}"
echo "üîÑ Wiki Sync Script"
echo "==================="
echo "Source: $SOURCE_DIR"
echo "Target: $WIKI_DIR"
echo ""
# Check if source directory exists
if [ ! -d "$SOURCE_DIR" ]; then
    echo "‚ùå Error: Source directory $SOURCE_DIR does not exist"
    exit 1
fi
# Check if wiki directory exists
if [ ! -d "$WIKI_DIR" ]; then
    echo "‚ùå Error: Wiki directory $WIKI_DIR does not exist"
    exit 1
fi
# Function to clean wiki directory (preserve .git)
clean_wiki() {
    echo "üßπ Cleaning wiki directory..."
    find "$WIKI_DIR" -mindepth 1 -maxdepth 1 ! -name '.git' -exec rm -rf {} +
}
# Function to copy files
copy_files() {
    echo "üìÅ Copying files from source to wiki..."
    # GitHub Wiki requires flat file structure - all files at root
    # Copy all markdown files, flattening directory structure
    if [ -d "$SOURCE_DIR" ]; then
        # Find all .md files and copy to wiki root
        find "$SOURCE_DIR" -type f -name "*.md" | while IFS= read -r file; do
            basename_file=$(basename "$file")
            cp "$file" "$WIKI_DIR/$basename_file"
        done
    fi
    echo "‚úÖ Files copied successfully (flattened to root)"
}
# Function to transform links for wiki format
transform_links() {
    echo "üîó Transforming links for wiki format..."
    # Find all markdown files
    find "$WIKI_DIR" -type f -name "*.md" | while read -r file; do
        # Skip special wiki files
        if [[ "$(basename "$file")" =~ ^_(Footer|Sidebar)\.md$ ]]; then
            continue
        fi
        # Create temp file
        temp_file="${file}.tmp"
        # Transform links
        # 1. Remove docs/wiki/ prefix from links
        # 2. Remove .md extension for wiki links
        # 3. Handle relative paths (../)
        # 4. Remove all directory prefixes (GitHub Wiki has flat namespace)
        sed -E \
            -e 's|\]\(docs/wiki/|\]\(|g' \
            -e 's|\]\(\.\./|\]\(|g' \
            -e 's|\.md\)|)|g' \
            -e 's|\]\(([^)]*)/([^/)]+)\)|\]\(\2\)|g' \
            "$file" > "$temp_file"
        # Replace original file
        mv "$temp_file" "$file"
    done
    echo "‚úÖ Links transformed successfully"
}
# Function to handle special files
handle_special_files() {
    echo "üìù Handling special wiki files..."
    # Rename README.md to Home.md if it exists
    if [ -f "$WIKI_DIR/README.md" ]; then
        mv "$WIKI_DIR/README.md" "$WIKI_DIR/Home.md"
        echo "  - Renamed README.md to Home.md"
    fi
    # Ensure Home.md exists
    if [ ! -f "$WIKI_DIR/Home.md" ]; then
        echo "‚ö†Ô∏è Warning: No Home.md found, wiki may not have a landing page"
    fi
}
# Function to validate wiki structure
validate_wiki() {
    echo "‚úîÔ∏è Validating wiki structure..."
    # Check for required files
    if [ ! -f "$WIKI_DIR/Home.md" ]; then
        echo "‚ö†Ô∏è Warning: Home.md is missing"
    fi
    # Check for broken internal links
    echo "  - Checking for broken internal links..."
    broken_links=0
    # Use process substitution to avoid subshell and preserve broken_links counter
    while IFS= read -r file; do
        # Extract all markdown links (allow grep to return no matches without failing)
        while IFS= read -r link; do
            # Skip empty lines
            [[ -z "$link" ]] && continue
            # Skip external links
            if [[ "$link" =~ ^https?:// ]] || [[ "$link" =~ ^# ]]; then
                continue
            fi
            # Check if linked file exists
            linked_file="$WIKI_DIR/${link}.md"
            if [[ ! "$link" =~ ^\/ ]] && [[ ! -f "$linked_file" ]]; then
                # Try with directory structure
                dir=$(dirname "$file")
                linked_file="$dir/${link}.md"
                if [ ! -f "$linked_file" ]; then
                    echo "    ‚ö†Ô∏è Broken link in $(basename "$file"): $link"
                    ((broken_links++))
                fi
            fi
        done < <((grep -o '\[.*\]([^)]*)'  "$file" 2>/dev/null || true) | (grep -o '([^)]*)' || true) | tr -d '()') || true
    done < <(find "$WIKI_DIR" -type f -name "*.md") || true
    if [ $broken_links -eq 0 ]; then
        echo "  ‚úÖ No broken links found"
    else
        echo "  ‚ö†Ô∏è Found $broken_links broken link(s)"
    fi
    echo "‚úÖ Validation complete"
}
# Main execution
main() {
    echo "üöÄ Starting wiki sync process..."
    echo ""
    # Step 1: Clean wiki directory
    clean_wiki
    # Step 2: Copy files
    copy_files
    # Step 3: Transform links
    transform_links
    # Step 4: Handle special files
    handle_special_files
    # Step 5: Validate structure
    validate_wiki
    echo ""
    echo "‚ú® Wiki sync completed successfully!"
    # List synced files
    echo ""
    echo "üìä Sync statistics:"
    echo "  - Total files: $(find "$WIKI_DIR" -type f -name "*.md" | wc -l)"
    echo "  - Categories: $(find "$WIKI_DIR" -type d -mindepth 1 | wc -l)"
    # Show directory structure
    if command -v tree &> /dev/null; then
        echo ""
        echo "üìÅ Wiki structure:"
        tree -I '.git' "$WIKI_DIR" || true
    fi
}
# Run main function
main
</file>

<file path="bin/build-dependencies.sh">
#!/usr/bin/env bash
# Get the directory of this file (where the package.json file is located)
bin_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" > /dev/null 2>&1 && pwd)"
infrastructure_files_list="${bin_dir}/../terraform/*.tf"
types_file_path="${bin_dir}/../src/types/infrastructure.d.ts"
infrastructure_hcl_file_path="${bin_dir}/../build/infrastructure.tf"
infrastructure_json_file_path="${bin_dir}/../build/infrastructure.json"
echo "infrastructure_files_list = $infrastructure_files_list"
echo 'Concatenating infrastructure files'
consolidate_command="cat ${infrastructure_files_list} > ${infrastructure_hcl_file_path}"
eval $consolidate_command
echo 'Converting HCL to JSON (via hcl2json)'
hcl2json < "$infrastructure_hcl_file_path" > "$infrastructure_json_file_path"
echo 'Converting JSON to TypeScript (via Quicktype)'
quicktype_command="${bin_dir}/../node_modules/quicktype/dist/index.js ${infrastructure_json_file_path} -o ${types_file_path}"
eval $quicktype_command
echo 'Checking Secrets (secrets.yaml) via SOPS'
secrets_file_path="${bin_dir}/../secrets.yaml"
encrypted_secrets_file_path="${bin_dir}/../secrets.enc.yaml"
sops_config_path="${bin_dir}/../.sops.yaml"
if [ ! -f "$secrets_file_path" ]; then
  echo "Warning: Secrets file does not exist at $secrets_file_path"
  echo "Please refer to the README for setup instructions"
  echo "Skipping encryption step"
elif [ ! -f "$sops_config_path" ]; then
  echo "Warning: SOPS config file does not exist at $sops_config_path"
  echo "Please refer to the README for SOPS configuration instructions"
  echo "Skipping encryption step"
elif [ ! -f "$encrypted_secrets_file_path" ]; then
  # No encrypted file exists yet - encrypt for the first time
  echo "Encrypting secrets for the first time..."
  sops --config "${sops_config_path}" --encrypt --output "${encrypted_secrets_file_path}" "${secrets_file_path}"
else
  # Compare hashes to avoid unnecessary re-encryption (AES-GCM uses random IVs)
  # This requires the age private key to decrypt - skip comparison if unavailable
  source_hash=$(shasum -a 256 "${secrets_file_path}" | cut -d' ' -f1)
  decrypted_content=$(sops --config "${sops_config_path}" --decrypt "${encrypted_secrets_file_path}" 2> /dev/null)
  decrypt_exit_code=$?
  if [ $decrypt_exit_code -ne 0 ]; then
    echo "Cannot decrypt secrets (missing age key?) - skipping encryption"
    echo "To re-encrypt, ensure SOPS_AGE_KEY_FILE is set or key is in ~/.config/sops/age/keys.txt"
  else
    decrypted_hash=$(echo "$decrypted_content" | shasum -a 256 | cut -d' ' -f1)
    if [ "$source_hash" != "$decrypted_hash" ]; then
      echo "Secrets changed - re-encrypting..."
      sops --config "${sops_config_path}" --encrypt --output "${encrypted_secrets_file_path}" "${secrets_file_path}"
    else
      echo "Secrets unchanged - skipping encryption"
    fi
  fi
fi
echo 'Prepending Typescript nocheck on file'
printf '%s\n%s\n' "// @ts-nocheck" "$(cat $types_file_path)" > "$types_file_path"
</file>

<file path="bin/document-source.sh">
#!/usr/bin/env bash
# THESE WORKAROUNDS ARE IN PLACE BECAUSE THE EXCLUDE METHOD OF TDSOC DOESN'T WORK
# TODO: File a bug demonstrating the issue to the TSDoc project
# Get the directory of this file (where the package.json file is located)
bin_dir="$(cd "$(dirname "${BASH_SOURCE[0]}")" > /dev/null 2>&1 && pwd)"
test_file_path="${bin_dir}/../src/pipeline/infrastructure.environment.test.ts"
types_file_path="${bin_dir}/../src/types/infrastructure.d.ts"
git_diff_output=$(git diff ${test_file_path})
git_diff_output_length=${#git_diff_output}
if [[ $git_diff_output_length -gt 0 ]]; then
  echo "Test file has changed; commit changes before running"
  exit
fi
# remove the generated definitions and the file(s) they rely on
if test -f $types_file_path; then
  rm "${bin_dir}/../src/types/infrastructure.d.ts"
fi
rm "${test_file_path}"
# generate the documentation
typedoc_command="${bin_dir}/../node_modules/typedoc/bin/typedoc --options ./typedoc.json"
eval $typedoc_command
# retrieve or rebuild the files
git checkout "${test_file_path}"
pnpm run build-dependencies
</file>

<file path="graphrag/knowledge-graph.json">
{
  "nodes": [
    {
      "id": "lambda:ApiGatewayAuthorizer",
      "type": "Lambda",
      "properties": {
        "name": "ApiGatewayAuthorizer",
        "trigger": "API Gateway",
        "purpose": "Authorize API requests via Better Auth sessions"
      }
    },
    {
      "id": "lambda:CloudfrontMiddleware",
      "type": "Lambda",
      "properties": {
        "name": "CloudfrontMiddleware",
        "trigger": "CloudFront",
        "purpose": "Edge processing for CDN requests"
      }
    },
    {
      "id": "lambda:FileCoordinator",
      "type": "Lambda",
      "properties": {
        "name": "FileCoordinator",
        "trigger": "CloudWatch Events",
        "purpose": "Scheduled job to orchestrate pending file downloads"
      }
    },
    {
      "id": "lambda:ListFiles",
      "type": "Lambda",
      "properties": {
        "name": "ListFiles",
        "trigger": "API Gateway",
        "purpose": "List files available to authenticated user"
      }
    },
    {
      "id": "lambda:LogClientEvent",
      "type": "Lambda",
      "properties": {
        "name": "LogClientEvent",
        "trigger": "API Gateway",
        "purpose": "Log client-side events for analytics"
      }
    },
    {
      "id": "lambda:LoginUser",
      "type": "Lambda",
      "properties": {
        "name": "LoginUser",
        "trigger": "API Gateway",
        "purpose": "Authenticate user via Sign In With Apple"
      }
    },
    {
      "id": "lambda:PruneDevices",
      "type": "Lambda",
      "properties": {
        "name": "PruneDevices",
        "trigger": "CloudWatch Events",
        "purpose": "Scheduled cleanup of inactive devices"
      }
    },
    {
      "id": "lambda:RefreshToken",
      "type": "Lambda",
      "properties": {
        "name": "RefreshToken",
        "trigger": "API Gateway",
        "purpose": "Refresh authentication token"
      }
    },
    {
      "id": "lambda:RegisterDevice",
      "type": "Lambda",
      "properties": {
        "name": "RegisterDevice",
        "trigger": "API Gateway",
        "purpose": "Register iOS device for push notifications"
      }
    },
    {
      "id": "lambda:RegisterUser",
      "type": "Lambda",
      "properties": {
        "name": "RegisterUser",
        "trigger": "API Gateway",
        "purpose": "Register new user via Sign In With Apple"
      }
    },
    {
      "id": "lambda:S3ObjectCreated",
      "type": "Lambda",
      "properties": {
        "name": "S3ObjectCreated",
        "trigger": "S3 Event",
        "purpose": "Handle uploaded files and notify users"
      }
    },
    {
      "id": "lambda:SendPushNotification",
      "type": "Lambda",
      "properties": {
        "name": "SendPushNotification",
        "trigger": "SQS",
        "purpose": "Send push notification to user devices via APNS"
      }
    },
    {
      "id": "lambda:StartFileUpload",
      "type": "Lambda",
      "properties": {
        "name": "StartFileUpload",
        "trigger": "Lambda Invoke",
        "purpose": "Initiate file download from YouTube"
      }
    },
    {
      "id": "lambda:UserDelete",
      "type": "Lambda",
      "properties": {
        "name": "UserDelete",
        "trigger": "API Gateway",
        "purpose": "Delete user and cascade to related records"
      }
    },
    {
      "id": "lambda:UserSubscribe",
      "type": "Lambda",
      "properties": {
        "name": "UserSubscribe",
        "trigger": "API Gateway",
        "purpose": "Manage user topic subscriptions"
      }
    },
    {
      "id": "lambda:WebhookFeedly",
      "type": "Lambda",
      "properties": {
        "name": "WebhookFeedly",
        "trigger": "API Gateway",
        "purpose": "Process incoming Feedly webhook articles"
      }
    },
    {
      "id": "entity:Accounts",
      "type": "Entity",
      "properties": {
        "name": "Accounts"
      }
    },
    {
      "id": "entity:Devices",
      "type": "Entity",
      "properties": {
        "name": "Devices"
      }
    },
    {
      "id": "entity:FileDownloads",
      "type": "Entity",
      "properties": {
        "name": "FileDownloads"
      }
    },
    {
      "id": "entity:Files",
      "type": "Entity",
      "properties": {
        "name": "Files"
      }
    },
    {
      "id": "entity:Sessions",
      "type": "Entity",
      "properties": {
        "name": "Sessions"
      }
    },
    {
      "id": "entity:UserDevices",
      "type": "Entity",
      "properties": {
        "name": "UserDevices"
      }
    },
    {
      "id": "entity:UserFiles",
      "type": "Entity",
      "properties": {
        "name": "UserFiles"
      }
    },
    {
      "id": "entity:Users",
      "type": "Entity",
      "properties": {
        "name": "Users"
      }
    },
    {
      "id": "entity:VerificationTokens",
      "type": "Entity",
      "properties": {
        "name": "VerificationTokens"
      }
    },
    {
      "id": "service:DynamoDB",
      "type": "Service",
      "properties": {
        "name": "DynamoDB",
        "type": "database"
      }
    },
    {
      "id": "service:S3",
      "type": "Service",
      "properties": {
        "name": "S3",
        "type": "storage"
      }
    },
    {
      "id": "service:SNS",
      "type": "Service",
      "properties": {
        "name": "SNS",
        "type": "notification"
      }
    },
    {
      "id": "service:SQS",
      "type": "Service",
      "properties": {
        "name": "SQS",
        "type": "queue"
      }
    },
    {
      "id": "service:Lambda",
      "type": "Service",
      "properties": {
        "name": "Lambda",
        "type": "compute"
      }
    },
    {
      "id": "service:CloudWatch",
      "type": "Service",
      "properties": {
        "name": "CloudWatch",
        "type": "monitoring"
      }
    },
    {
      "id": "service:SecretsManager",
      "type": "Service",
      "properties": {
        "name": "SecretsManager",
        "type": "secrets"
      }
    },
    {
      "id": "service:StepFunctions",
      "type": "Service",
      "properties": {
        "name": "StepFunctions",
        "type": "orchestration"
      }
    },
    {
      "id": "service:XRay",
      "type": "Service",
      "properties": {
        "name": "XRay",
        "type": "tracing"
      }
    },
    {
      "id": "service:API Gateway",
      "type": "Service",
      "properties": {
        "name": "API Gateway",
        "type": "api"
      }
    },
    {
      "id": "service:CloudFront",
      "type": "Service",
      "properties": {
        "name": "CloudFront",
        "type": "cdn"
      }
    },
    {
      "id": "external:Feedly",
      "type": "External",
      "properties": {
        "name": "Feedly",
        "type": "content",
        "description": "RSS feed aggregation"
      }
    },
    {
      "id": "external:YouTube",
      "type": "External",
      "properties": {
        "name": "YouTube",
        "type": "media",
        "description": "Video download source"
      }
    },
    {
      "id": "external:APNS",
      "type": "External",
      "properties": {
        "name": "APNS",
        "type": "notification",
        "description": "Apple Push Notification Service"
      }
    },
    {
      "id": "external:Sign In With Apple",
      "type": "External",
      "properties": {
        "name": "Sign In With Apple",
        "type": "auth",
        "description": "Apple OAuth provider"
      }
    },
    {
      "id": "external:GitHub",
      "type": "External",
      "properties": {
        "name": "GitHub",
        "type": "integration",
        "description": "Issue creation for errors"
      }
    }
  ],
  "edges": [
    {
      "source": "service:API Gateway",
      "target": "lambda:ApiGatewayAuthorizer",
      "relationship": "triggers"
    },
    {
      "source": "service:CloudFront",
      "target": "lambda:CloudfrontMiddleware",
      "relationship": "triggers"
    },
    {
      "source": "service:API Gateway",
      "target": "lambda:ListFiles",
      "relationship": "triggers"
    },
    {
      "source": "service:API Gateway",
      "target": "lambda:LogClientEvent",
      "relationship": "triggers"
    },
    {
      "source": "service:API Gateway",
      "target": "lambda:LoginUser",
      "relationship": "triggers"
    },
    {
      "source": "service:API Gateway",
      "target": "lambda:RefreshToken",
      "relationship": "triggers"
    },
    {
      "source": "service:API Gateway",
      "target": "lambda:RegisterDevice",
      "relationship": "triggers"
    },
    {
      "source": "service:API Gateway",
      "target": "lambda:RegisterUser",
      "relationship": "triggers"
    },
    {
      "source": "service:SQS",
      "target": "lambda:SendPushNotification",
      "relationship": "triggers"
    },
    {
      "source": "service:API Gateway",
      "target": "lambda:UserDelete",
      "relationship": "triggers"
    },
    {
      "source": "service:API Gateway",
      "target": "lambda:UserSubscribe",
      "relationship": "triggers"
    },
    {
      "source": "service:API Gateway",
      "target": "lambda:WebhookFeedly",
      "relationship": "triggers"
    },
    {
      "source": "lambda:FileCoordinator",
      "target": "lambda:StartFileUpload",
      "relationship": "invokes",
      "properties": {
        "via": "Lambda invokeAsync"
      }
    },
    {
      "source": "lambda:S3ObjectCreated",
      "target": "lambda:SendPushNotification",
      "relationship": "invokes",
      "properties": {
        "via": "SQS"
      }
    },
    {
      "source": "entity:Users",
      "target": "entity:UserFiles",
      "relationship": "has_many"
    },
    {
      "source": "entity:Users",
      "target": "entity:UserDevices",
      "relationship": "has_many"
    },
    {
      "source": "entity:Users",
      "target": "entity:Sessions",
      "relationship": "has_many"
    },
    {
      "source": "entity:Users",
      "target": "entity:Accounts",
      "relationship": "has_many"
    },
    {
      "source": "entity:Files",
      "target": "entity:UserFiles",
      "relationship": "has_many"
    },
    {
      "source": "entity:Files",
      "target": "entity:FileDownloads",
      "relationship": "has_many"
    },
    {
      "source": "entity:Devices",
      "target": "entity:UserDevices",
      "relationship": "has_many"
    },
    {
      "source": "entity:UserFiles",
      "target": "entity:Users",
      "relationship": "belongs_to"
    },
    {
      "source": "entity:UserFiles",
      "target": "entity:Files",
      "relationship": "belongs_to"
    },
    {
      "source": "entity:UserDevices",
      "target": "entity:Users",
      "relationship": "belongs_to"
    },
    {
      "source": "entity:UserDevices",
      "target": "entity:Devices",
      "relationship": "belongs_to"
    },
    {
      "source": "entity:Sessions",
      "target": "entity:Users",
      "relationship": "belongs_to"
    },
    {
      "source": "entity:Accounts",
      "target": "entity:Users",
      "relationship": "belongs_to"
    },
    {
      "source": "entity:FileDownloads",
      "target": "entity:Files",
      "relationship": "belongs_to"
    },
    {
      "source": "entity:Users",
      "target": "entity:VerificationTokens",
      "relationship": "has_many"
    },
    {
      "source": "entity:VerificationTokens",
      "target": "entity:Users",
      "relationship": "belongs_to"
    },
    {
      "source": "service:API Gateway",
      "target": "service:Lambda",
      "relationship": "triggers"
    },
    {
      "source": "service:S3",
      "target": "service:Lambda",
      "relationship": "triggers",
      "properties": {
        "event": "s3:ObjectCreated"
      }
    },
    {
      "source": "service:CloudWatch",
      "target": "service:Lambda",
      "relationship": "triggers",
      "properties": {
        "event": "scheduled"
      }
    },
    {
      "source": "service:SQS",
      "target": "service:Lambda",
      "relationship": "triggers"
    }
  ],
  "metadata": {
    "version": "2.0.0",
    "description": "Media Downloader Lambda chains and entity relationships (auto-generated)",
    "sources": {
      "lambdas": "src/lambdas/",
      "dependencies": "build/graph.json",
      "metadata": "graphrag/metadata.json"
    }
  }
}
</file>

<file path="src/entities/Collections.ts">
import {createService, documentClient} from '#lib/vendor/ElectroDB/service'
import {Files} from './Files'
import {FileDownloads} from './FileDownloads'
import {Users} from './Users'
import {Devices} from './Devices'
import {UserFiles} from './UserFiles'
import {UserDevices} from './UserDevices'
import {Sessions} from './Sessions'
import {Accounts} from './Accounts'
import {VerificationTokens} from './VerificationTokens'
/**
 * MediaDownloader Service
 *
 * Combines all entities in a single-table design for efficient JOIN-like queries.
 * ElectroDB Collections enable queries across entity boundaries using shared GSI keys.
 *
 * Available Collections:
 *
 * 1. **userResources** (UserCollection/gsi1)
 *    - Query: Get all files and devices for a user
 *    - Entities: Users, UserFiles, UserDevices
 *    - Access pattern: collections.userResources(userId).go()
 *    - Used by: ListFiles, UserDelete, RegisterDevice
 *
 * 2. **fileUsers** (FileCollection/gsi2)
 *    - Query: Get all users associated with a file
 *    - Entities: Files, UserFiles
 *    - Access pattern: collections.fileUsers(fileId).go()
 *    - Used by: S3ObjectCreated (for push notifications)
 *
 * 3. **deviceUsers** (DeviceCollection/gsi3)
 *    - Query: Get all users associated with a device
 *    - Entities: Devices, UserDevices
 *    - Access pattern: collections.deviceUsers(deviceId).go()
 *    - Used by: PruneDevices (for cleanup)
 *
 * 4. **userSessions** (gsi1)
 *    - Query: Get all sessions for a user
 *    - Entities: Sessions
 *    - Access pattern: `collections.userSessions(\{userId\}).go()`
 *    - Used by: LoginUser, RefreshToken, Logout
 *
 * 5. **userAccounts** (gsi1)
 *    - Query: Get all OAuth accounts for a user
 *    - Entities: Accounts
 *    - Access pattern: `collections.userAccounts(\{userId\}).go()`
 *    - Used by: Better Auth adapter for account linking
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/ElectroDB-Testing-Patterns#collections-testing-join-operations | Collections Usage Examples}
 */
‚ãÆ----
/**
 * Collections for JOIN-like operations between entities.
 * Use these instead of N+1 queries or full table scans.
 *
 * Collections leverage GSIs to fetch related entities in a single query:
 * - userResources: Get user's files and devices via UserCollection (gsi1)
 * - fileUsers: Get users for a file via FileCollection (gsi2)
 * - deviceUsers: Get users for a device via DeviceCollection (gsi3)
 */
‚ãÆ----
/**
 * Direct entity access via the service.
 * Prefer importing entities directly unless you need collections.
 */
</file>

<file path="src/lib/vendor/AWS/clients.ts">
/**
 * AWS Client Factory
 *
 * Creates AWS SDK clients with environment-aware configuration.
 * Supports both production AWS and LocalStack for integration testing.
 *
 * This is the ONLY file where AWS SDK client instantiation should occur,
 * maintaining the AWS SDK Encapsulation Policy.
 */
import {S3Client} from '@aws-sdk/client-s3'
import type {S3ClientConfig} from '@aws-sdk/client-s3'
import {DynamoDBClient} from '@aws-sdk/client-dynamodb'
import type {DynamoDBClientConfig} from '@aws-sdk/client-dynamodb'
import {SNSClient} from '@aws-sdk/client-sns'
import type {SNSClientConfig} from '@aws-sdk/client-sns'
import {SQSClient} from '@aws-sdk/client-sqs'
import type {SQSClientConfig} from '@aws-sdk/client-sqs'
import {LambdaClient} from '@aws-sdk/client-lambda'
import type {LambdaClientConfig} from '@aws-sdk/client-lambda'
import {CloudWatchClient} from '@aws-sdk/client-cloudwatch'
import type {CloudWatchClientConfig} from '@aws-sdk/client-cloudwatch'
import {APIGateway} from '@aws-sdk/client-api-gateway'
import type {APIGatewayClientConfig} from '@aws-sdk/client-api-gateway'
import {captureAWSClient} from './XRay'
‚ãÆ----
/**
 * Check if running in LocalStack mode
 * @returns true if USE_LOCALSTACK environment variable is set to 'true'
 */
function isLocalStackMode(): boolean
/**
 * Get base configuration for AWS clients
 * Returns LocalStack config when in LocalStack mode, production config otherwise
 */
function getBaseConfig()
/**
 * Create an S3 client instance
 * Configured for LocalStack when USE_LOCALSTACK=true, otherwise production AWS
 * Wrapped with X-Ray instrumentation when enabled
 */
export function createS3Client(): S3Client
‚ãÆ----
// forcePathStyle required for LocalStack S3
‚ãÆ----
/**
 * Create a DynamoDB client instance
 * Configured for LocalStack when USE_LOCALSTACK=true, otherwise production AWS
 * Wrapped with X-Ray instrumentation when enabled
 */
export function createDynamoDBClient(): DynamoDBClient
/**
 * Create an SNS client instance
 * Configured for LocalStack when USE_LOCALSTACK=true, otherwise production AWS
 * Wrapped with X-Ray instrumentation when enabled
 */
export function createSNSClient(): SNSClient
/**
 * Create an SQS client instance
 * Configured for LocalStack when USE_LOCALSTACK=true, otherwise production AWS
 * Wrapped with X-Ray instrumentation when enabled
 */
export function createSQSClient(): SQSClient
/**
 * Create a Lambda client instance
 * Configured for LocalStack when USE_LOCALSTACK=true, otherwise production AWS
 * Wrapped with X-Ray instrumentation when enabled
 */
export function createLambdaClient(): LambdaClient
/**
 * Create a CloudWatch client instance
 * Configured for LocalStack when USE_LOCALSTACK=true, otherwise production AWS
 * Wrapped with X-Ray instrumentation when enabled
 */
export function createCloudWatchClient(): CloudWatchClient
/**
 * Create an API Gateway client instance
 * Configured for LocalStack when USE_LOCALSTACK=true, otherwise production AWS
 * Wrapped with X-Ray instrumentation when enabled
 */
export function createAPIGatewayClient(): APIGateway
</file>

<file path="src/lib/vendor/AWS/XRay.ts">
import AWSXRay from 'aws-xray-sdk-core'
import type {Context} from 'aws-lambda'
/**
 * Check if X-Ray tracing is enabled
 * X-Ray is disabled for LocalStack (unsupported) and when ENABLE_XRAY=false
 * @returns true if X-Ray should be enabled
 */
function isXRayEnabled(): boolean
/**
 * Gets the current X-Ray segment for the Lambda invocation
 * Returns null if X-Ray tracing is not active or disabled
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/X-Ray-Integration#custom-subsegments | X-Ray Custom Subsegments}
 */
export function getSegment(): AWSXRay.Segment | AWSXRay.Subsegment | null
/**
 * Wrap an AWS SDK v3 client with X-Ray instrumentation
 * Returns the client unchanged if X-Ray is disabled
 *
 * @param client - AWS SDK v3 client to wrap
 * @returns X-Ray instrumented client or original client
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/X-Ray-Integration#aws-sdk-integration | X-Ray AWS SDK Integration}
 */
export function captureAWSClient<T extends
/**
 * Higher-order function that wraps Lambda handlers with X-Ray tracing
 * Extracts trace ID from X-Ray segment or falls back to AWS request ID
 *
 * @param handler - Lambda handler function that receives event, context, and metadata
 * @returns Wrapped handler compatible with AWS Lambda runtime
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/X-Ray-Integration#lambda-usage | X-Ray Lambda Usage}
 */
export function withXRay<TEvent = unknown, TResult = unknown>(handler: (event: TEvent, context: Context, metadata?:
‚ãÆ----
// Only attempt to get X-Ray segment if X-Ray is enabled
// This prevents errors during Jest tests where X-Ray context doesn't exist
‚ãÆ----
// X-Ray segment has trace_id property but it's not in the type definitions
‚ãÆ----
// X-Ray segment not available, use request ID
</file>

<file path="src/lib/vendor/BetterAuth/config.ts">
/**
 * Better Auth Configuration
 *
 * Configures Better Auth with ElectroDB adapter and OAuth providers.
 * This module provides a singleton Better Auth instance for use across Lambda functions.
 */
import {betterAuth} from 'better-auth'
import {electroDBAdapter} from './electrodb-adapter'
import {logDebug} from '#util/logging'
import {getRequiredEnv} from '#util/env-validation'
/**
 * Better Auth instance configured for MediaDownloader service.
 *
 * Configuration:
 * - Database: ElectroDB adapter with DynamoDB single-table design
 * - Providers: Apple Sign In (OAuth) with iOS bundle ID support
 * - Sessions: 30-day expiration with refresh tokens
 * - Base URL: API Gateway endpoint
 * - Trusted Origins: Apple's authentication domain
 *
 * Note: Better Auth's Apple provider expects a pre-generated client secret, but we
 * dynamically generate it from the auth key. This is handled by providing a placeholder
 * here since we're using ID token authentication (which doesn't require the secret for
 * token verification - Better Auth verifies ID tokens using Apple's public keys).
 */
‚ãÆ----
// Secret for signing tokens and sessions
‚ãÆ----
// Base URL for OAuth callbacks (from environment)
‚ãÆ----
// Trusted origins for OAuth flows
‚ãÆ----
// Session configuration
‚ãÆ----
expiresIn: 60 * 60 * 24 * 30, // 30 days
updateAge: 60 * 60 * 24, // Update session every 24 hours
‚ãÆ----
enabled: false // Disable cookie cache for serverless
‚ãÆ----
// OAuth providers
‚ãÆ----
// Use existing Sign In With Apple configuration
// clientId is the Service ID for web, but appBundleIdentifier is used for iOS
‚ãÆ----
// Client secret placeholder - not needed for ID token verification flow
// Better Auth verifies ID tokens using Apple's public JWKS, not the client secret
‚ãÆ----
// iOS app bundle identifier for ID token validation
// When using ID tokens from iOS, the 'aud' claim will be the bundle ID, not the service ID
‚ãÆ----
// Advanced options
‚ãÆ----
// Generate session tokens instead of cookies for mobile apps
‚ãÆ----
/**
 * Extract client ID (Service ID) from Sign In With Apple configuration.
 */
function getAppleClientIdFromConfig(): string
/**
 * Extract bundle ID from Sign In With Apple configuration.
 */
function getAppleBundleIdFromConfig(): string
/**
 * Initialize Better Auth on cold start.
 * Call this at the top of Lambda handlers to ensure Better Auth is ready.
 */
export async function initializeBetterAuth()
/**
 * Type exports for Better Auth operations
 */
export type BetterAuthInstance = typeof auth
</file>

<file path="src/lib/vendor/ElectroDB/entity.ts">
/**
 * ElectroDB Entity Wrapper
 *
 * This module wraps the ElectroDB Entity class to encapsulate the vendor library.
 * Domain entities should import from this wrapper, not directly from 'electrodb'.
 *
 * Follows the same pattern as AWS SDK encapsulation in lib/vendor/AWS/*.
 *
 * NOTE: ElectroDB's TypeScript types use complex conditional types that cannot be
 * properly wrapped in a function without losing type safety. Therefore, we simply
 * re-export the Entity class for encapsulation purposes. Entities must use the
 * Entity constructor directly with 'as const' assertions for proper type inference.
 */
import {Entity} from 'electrodb'
import type {EntityItem} from 'electrodb'
import {documentClient} from '#lib/vendor/AWS/DynamoDB'
/**
 * Re-export documentClient for entity configuration.
 * Entities need this to configure their DynamoDB client.
 */
‚ãÆ----
/**
 * Re-export Entity class for creating entity instances.
 *
 * IMPORTANT: Entities MUST be created with 'as const' assertion on the schema
 * for proper TypeScript inference of custom indexes.
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/ElectroDB-Testing-Patterns#entity-reference | ElectroDB Entity Usage}
 */
</file>

<file path="src/util/better-auth-helpers.ts">
/**
 * Better Auth Helper Functions
 *
 * Utility functions for working with Better Auth sessions and tokens in Lambda functions.
 * These helpers bridge Better Auth's framework with our serverless architecture.
 */
import {Sessions} from '#entities/Sessions'
import {logDebug, logError} from './logging'
import {UnauthorizedError} from './errors'
import type {SessionPayload} from '#types/util'
/**
 * Validates a session token and returns the session payload.
 *
 * @param token - The session token from Authorization header (without "Bearer " prefix)
 * @returns Session payload with userId and sessionId
 * @throws UnauthorizedError if token is invalid or expired
 */
export async function validateSessionToken(token: string): Promise<SessionPayload>
‚ãÆ----
// Use GSI for O(1) lookup instead of table scan
‚ãÆ----
/**
 * Refreshes a session by extending its expiration.
 *
 * @param sessionId - The session ID to refresh
 * @returns New expiration timestamp
 */
export async function refreshSession(sessionId: string): Promise<
‚ãÆ----
const expiresAt = Date.now() + 30 * 24 * 60 * 60 * 1000 // 30 days
</file>

<file path="src/util/retry.ts">
import {logDebug} from './logging'
import type {RetryConfig} from '#types/util'
‚ãÆ----
type RetryResult<T> = {data: T[]; unprocessed: unknown[]}
type DeleteResult = {unprocessed: unknown[]}
type RetryOperation<T> = () => Promise<RetryResult<T>>
type DeleteOperation = () => Promise<DeleteResult>
function sleep(ms: number): Promise<void>
/**
 * Calculate delay with exponential backoff and jitter
 * Jitter prevents thundering herd when multiple processes retry simultaneously
 * @param baseDelay - Base delay in milliseconds
 * @param retryCount - Current retry attempt (0-indexed)
 * @param multiplier - Exponential multiplier
 * @param maxDelay - Maximum delay cap
 * @returns Delay with jitter applied
 */
function calculateDelayWithJitter(baseDelay: number, retryCount: number, multiplier: number, maxDelay: number): number
‚ãÆ----
// Add 0-1000ms jitter to prevent thundering herd
‚ãÆ----
/**
 * Core retry logic shared between get and delete operations
 * @param operation - Async function to retry
 * @param config - Retry configuration
 * @param accumulator - Function to accumulate results across retries
 * @param operationName - Name for logging
 * @returns Final result after all retries
 */
async function retryWithBackoff<TResult extends {unprocessed: unknown[]}>(
  operation: () => Promise<TResult>,
  config: Required<RetryConfig>,
  accumulator: (prev: TResult, next: TResult) => TResult,
  operationName: string
): Promise<TResult>
/**
 * Retries a batch operation that may return unprocessed items
 * Uses exponential backoff with jitter between retries
 * @param operation - Async function that returns (data, unprocessed)
 * @param config - Optional retry configuration
 * @returns Final result with any remaining unprocessed items after all retries
 */
export async function retryUnprocessed<T>(operation: RetryOperation<T>, config?: RetryConfig): Promise<RetryResult<T>>
/**
 * Retries a batch delete operation that may return unprocessed items
 * Uses exponential backoff with jitter between retries
 * @param operation - Async function that returns (unprocessed)
 * @param config - Optional retry configuration
 * @returns Final unprocessed items after all retries
 */
export async function retryUnprocessedDelete(operation: DeleteOperation, config?: RetryConfig): Promise<DeleteResult>
</file>

<file path="test/integration/helpers/mock-youtube.ts">
/**
 * Mock YouTube Helper
 *
 * Mocks yt-dlp vendor wrapper for integration tests.
 * This allows testing Lambda orchestration logic without running actual yt-dlp binary.
 */
import {Readable} from 'stream'
import {jest} from '@jest/globals'
import type {YtDlpFormat, YtDlpVideoInfo} from '#types/youtube'
/**
 * Create mock video info for testing
 */
export function createMockVideoInfo(overrides?: Partial<YtDlpVideoInfo>): YtDlpVideoInfo
‚ãÆ----
filesize: 5242880, // 5MB
‚ãÆ----
/**
 * Create mock video format for testing
 */
export function createMockVideoFormat(overrides?: Partial<YtDlpFormat>): YtDlpFormat
‚ãÆ----
filesize: 5242880, // 5MB
‚ãÆ----
/**
 * Create a mock video stream for testing S3 uploads
 * @param sizeInBytes - Size of the mock video in bytes
 * @param contentPattern - Pattern to fill the stream with (default: 'a')
 */
export function createMockVideoStream(sizeInBytes: number, contentPattern: string = 'a'): Readable
‚ãÆ----
const chunkSize = 64 * 1024 // 64KB chunks
‚ãÆ----
read()
‚ãÆ----
this.push(null) // End of stream
‚ãÆ----
// Create chunk filled with pattern
‚ãÆ----
/**
 * Mock streamVideoToS3 result
 */
export function createMockStreamResult(sizeInBytes: number):
‚ãÆ----
duration: 1500 // 1.5 seconds
‚ãÆ----
/**
 * Create mock implementation of fetchVideoInfo
 */
export function mockFetchVideoInfo(videoInfo?: YtDlpVideoInfo): jest.Mock
/**
 * Create mock implementation of chooseVideoFormat
 */
export function mockChooseVideoFormat(format?: YtDlpFormat): jest.Mock
/**
 * S3 Upload interface for integration testing.
 * Uses Promise for done() to be compatible with AWS SDK lib-storage Upload class
 * which returns CompleteMultipartUploadCommandOutput.
 */
export interface S3UploadHandle {
  done: () => Promise<unknown>
}
/**
 * S3 Upload function type for integration testing.
 * This type is used for type assertions when passing the real createS3Upload
 * function to mocks that have a simpler type signature.
 */
export type S3UploadFunction = (bucket: string, key: string, body: Readable | Buffer, contentType?: string) => S3UploadHandle
/**
 * Create mock implementation of streamVideoToS3 that actually uploads to S3
 * This is used for REAL S3 uploads in integration tests.
 *
 * @param createS3Upload - S3 upload function. Use type assertion (createS3Upload as S3UploadFunction)
 *                         when passing the real createS3Upload which has additional optional parameters.
 */
export function createMockStreamVideoToS3WithRealUpload(createS3Upload: S3UploadFunction)
‚ãÆ----
// Create mock video stream
const videoStream = createMockVideoStream(5242880) // 5MB
// Use REAL S3 upload (LocalStack)
‚ãÆ----
/**
 * Create mock implementation of streamVideoToS3 that fails for testing error handling
 */
export function createMockStreamVideoToS3WithFailure(errorMessage: string = 'Mock S3 upload failed'): jest.Mock
</file>

<file path="test/integration/helpers/test-data.ts">
/**
 * Test Data Helpers
 *
 * Reusable factory functions for creating mock test data across integration tests.
 * Reduces inline JSON and provides consistent test data patterns.
 */
import type {ScheduledEvent, SQSEvent} from 'aws-lambda'
import {FileStatus} from '#types/enums'
import type {File} from '#types/domain-models'
/**
 * Creates a mock file object with sensible defaults
 * Provides ALL required ElectroDB fields for database operations
 * @param id - File ID (e.g., 'video-123')
 * @param status - File status from FileStatus enum
 * @param partial - Partial file data to override defaults
 */
export function createMockFile(id: string, status: FileStatus, partial?: Partial<File>): Partial<File>
‚ãÆ----
// Add Downloaded-specific fields (downloaded files)
‚ãÆ----
/**
 * Creates an array of mock files for batch testing
 * @param count - Number of files to create
 * @param status - Status for all files
 * @param idPrefix - Prefix for file IDs (default: 'video')
 */
export function createMockFiles(count: number, status: FileStatus, idPrefix = 'video'): Partial<File>[]
/**
 * Creates a mock UserFile record (user-file association)
 * @param userId - User UUID
 * @param fileId - File ID
 */
export function createMockUserFile(userId: string, fileId: string)
/**
 * Creates a mock UserDevice record (user-device association)
 * @param userId - User UUID
 * @param deviceId - Device UUID
 */
export function createMockUserDevice(userId: string, deviceId: string)
/**
 * Creates a mock Device record with endpoint ARN
 * @param deviceId - Device UUID
 * @param endpointArn - SNS endpoint ARN (optional, auto-generated if not provided)
 */
export function createMockDevice(deviceId: string, endpointArn?: string)
/**
 * Creates an SQS FileNotification event
 * @param userId - User ID to send notification to
 * @param fileId - File ID for the notification
 * @param partial - Partial file data to override defaults in message attributes
 * @param notificationType - Notification type (default: 'DownloadReadyNotification')
 */
export function createMockSQSFileNotificationEvent(
  userId: string,
  fileId: string,
  partial?: {title?: string; size?: number; url?: string},
  notificationType = 'DownloadReadyNotification'
): SQSEvent
‚ãÆ----
// Body must be JSON matching what createDownloadReadyNotification produces
// SendPushNotification parses this with JSON.parse in transformToAPNSNotification
‚ãÆ----
/**
 * Creates a CloudWatch Events / EventBridge scheduled event
 * @param eventId - Unique event ID
 * @param ruleName - Name of the EventBridge rule (default: 'FileCoordinatorSchedule')
 */
export function createMockScheduledEvent(eventId: string, ruleName = 'FileCoordinatorSchedule'): ScheduledEvent
</file>

<file path=".nvmrc">
v24.12.0
</file>

<file path="bin/ci-local.sh">
#!/usr/bin/env bash
# ci-local.sh
# Fast local CI runner - runs all CI checks except integration tests
# Usage: pnpm run ci:local or ./bin/ci-local.sh
#
# This script replicates the checks from GitHub Actions workflows locally,
# catching ~95% of issues that would fail in CI. For full CI parity including
# integration tests, use: pnpm run ci:local:full
set -e # Exit on error
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
# Color constants
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'
# Track timing
START_TIME=$(date +%s)
echo -e "${GREEN}Local CI Runner (Fast Mode)${NC}"
echo "============================="
echo ""
echo "This runs the same checks as GitHub Actions CI:"
echo "  1. Environment verification"
echo "  2. Dependency installation"
echo "  3. TypeSpec compilation"
echo "  4. Build dependencies (Terraform types)"
echo "  5. esbuild build"
echo "  6. Type checking"
echo "  7. Linting"
echo "  8. ESLint local rules tests"
echo "  9. Documentation validation"
echo "  10. Dependency rules check"
echo "  11. GraphRAG validation"
echo "  12. Documentation sync validation"
echo "  13. Unit tests"
echo ""
echo -e "${BLUE}Note: Integration tests skipped. Use 'pnpm run ci:local:full' for complete CI.${NC}"
echo ""
cd "$PROJECT_ROOT"
# Step 1: Environment checks
echo -e "${YELLOW}[1/13] Checking prerequisites...${NC}"
# Check Node.js version
REQUIRED_NODE_MAJOR=24
CURRENT_NODE_VERSION=$(node -v | sed 's/v//' | cut -d'.' -f1)
if [ "$CURRENT_NODE_VERSION" -lt "$REQUIRED_NODE_MAJOR" ]; then
  echo -e "${RED}Error: Node.js $REQUIRED_NODE_MAJOR+ required (found: $(node -v))${NC}"
  exit 1
fi
echo "  Node.js $(node -v)"
# Check hcl2json (required for build-dependencies)
if ! command -v hcl2json &> /dev/null; then
  echo -e "${RED}Error: hcl2json is not installed${NC}"
  echo "Install with: brew install hcl2json"
  exit 1
fi
echo "  hcl2json $(hcl2json --version 2>&1 | head -1 || echo 'installed')"
# Check jq (required for validation scripts)
if ! command -v jq &> /dev/null; then
  echo -e "${RED}Error: jq is not installed${NC}"
  echo "Install with: brew install jq"
  exit 1
fi
echo "  jq $(jq --version)"
echo -e "${GREEN}  Prerequisites satisfied${NC}"
echo ""
# Step 2: Create build directory and install dependencies
echo -e "${YELLOW}[2/13] Installing dependencies...${NC}"
mkdir -p build
pnpm install --frozen-lockfile
echo -e "${GREEN}  Dependencies installed${NC}"
echo ""
# Step 3: TypeSpec compilation
echo -e "${YELLOW}[3/13] Compiling TypeSpec...${NC}"
pnpm run typespec:check
echo -e "${GREEN}  TypeSpec compilation passed${NC}"
echo ""
# Step 4: Build dependencies (Terraform types)
echo -e "${YELLOW}[4/13] Building dependencies (Terraform types)...${NC}"
pnpm run build-dependencies
echo -e "${GREEN}  Build dependencies complete${NC}"
echo ""
# Step 5: esbuild build
echo -e "${YELLOW}[5/13] Running esbuild build...${NC}"
pnpm run build
echo -e "${GREEN}  Build complete${NC}"
echo ""
# Step 6: Type checking
echo -e "${YELLOW}[6/13] Running type checks...${NC}"
pnpm run check-types
echo -e "${GREEN}  Type checks passed${NC}"
echo ""
# Step 7: Linting
echo -e "${YELLOW}[7/13] Running linter...${NC}"
pnpm run lint
# Check Terraform formatting
if command -v tofu &> /dev/null; then
  if ! tofu fmt -check -recursive terraform/ > /dev/null; then
     echo -e "${RED}Error: Terraform formatting check failed.${NC}"
     echo "Run 'tofu fmt -recursive terraform/' to fix."
     exit 1
  fi
  echo "  Terraform formatting passed"
else
  echo -e "${YELLOW}  Skipping Terraform formatting check (tofu not found)${NC}"
fi
echo -e "${GREEN}  Linting passed${NC}"
echo ""
# Step 8: ESLint local rules tests
echo -e "${YELLOW}[8/13] Testing ESLint local rules...${NC}"
pnpm run test:eslint-rules
echo -e "${GREEN}  ESLint local rules tests passed${NC}"
echo ""
# Step 9: Documentation validation
echo -e "${YELLOW}[9/13] Validating documented scripts...${NC}"
./bin/validate-docs.sh
echo ""
# Step 10: Dependency rules check
echo -e "${YELLOW}[10/13] Checking dependency rules...${NC}"
pnpm run deps:check
echo -e "${GREEN}  Dependency rules passed${NC}"
echo ""
# Step 11: GraphRAG validation
echo -e "${YELLOW}[11/13] Validating GraphRAG...${NC}"
./bin/validate-graphrag.sh
echo ""
# Step 12: Documentation sync validation
echo -e "${YELLOW}[12/13] Validating documentation sync...${NC}"
./bin/validate-doc-sync.sh
echo ""
# Step 13: Unit tests
echo -e "${YELLOW}[13/13] Running unit tests...${NC}"
pnpm test
echo -e "${GREEN}  Unit tests passed${NC}"
echo ""
# Calculate duration
END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))
MINUTES=$((DURATION / 60))
SECONDS=$((DURATION % 60))
# Summary
echo -e "${GREEN}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
echo -e "${GREEN}Local CI Complete${NC}"
echo -e "${GREEN}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
echo ""
echo "All checks passed in ${MINUTES}m ${SECONDS}s"
echo ""
echo "What was checked:"
echo "  Environment, dependencies, TypeSpec, build, types, lint, ESLint local rules,"
echo "  documentation, dependency rules, GraphRAG, documentation sync, unit tests"
echo ""
echo "What was NOT checked (run ci:local:full for these):"
echo "  Integration tests (LocalStack)"
echo ""
echo "GitHub-specific checks (cannot be run locally):"
echo "  Codecov upload, artifact storage, PR comments"
echo ""
echo -e "${GREEN}Ready to push!${NC}"
</file>

<file path="src/entities/Files.ts">
import {documentClient, Entity} from '#lib/vendor/ElectroDB/entity'
/**
 * ElectroDB entity schema for the Files DynamoDB table.
 * This entity manages permanent media file metadata ONLY.
 *
 * Design Philosophy:
 * - Files = permanent metadata about media (title, author, size, etc.)
 * - FileDownloads = transient orchestration state (retries, scheduling, errors)
 *
 * Status values are minimal and final:
 * - Queued: File record created, download not yet complete
 * - Downloading: Download in progress
 * - Downloaded: Successfully downloaded, ready for users
 * - Failed: Permanently failed, will not be available
 *
 * All orchestration (retries, scheduling, error tracking) is in FileDownloads.
 *
 * @see FileDownloads for download orchestration state
 */
‚ãÆ----
/** YouTube video ID - primary identifier */
‚ãÆ----
/** File size in bytes (0 until downloaded) */
‚ãÆ----
/** Channel/author display name */
‚ãÆ----
/** Channel/author username or ID */
‚ãÆ----
/** Video publish date (ISO string or timestamp) */
‚ãÆ----
/** Video description */
‚ãÆ----
/** S3 object key */
‚ãÆ----
/** Original source URL (YouTube URL) */
‚ãÆ----
/** MIME type (e.g., video/mp4) */
‚ãÆ----
/** Video title */
‚ãÆ----
/** Final status: Queued (awaiting download), Downloading (in progress), Downloaded (ready), Failed (failed) */
‚ãÆ----
// Type exports for use in application code
export type FileItem = ReturnType<typeof Files.parse>
export type CreateFileInput = Parameters<typeof Files.create>[0]
export type UpdateFileInput = Parameters<typeof Files.update>[0]
</file>

<file path="src/lib/vendor/AWS/ApiGateway.ts">
import type {ApiKey, ApiKeys, GetApiKeysRequest, GetUsagePlansRequest, GetUsageRequest, Usage, UsagePlan, UsagePlans} from '@aws-sdk/client-api-gateway'
import {createAPIGatewayClient} from './clients'
‚ãÆ----
// Re-export types for application code to use
‚ãÆ----
export function getApiKeys(params: GetApiKeysRequest): Promise<ApiKeys>
export function getUsage(params: GetUsageRequest): Promise<Usage>
export function getUsagePlans(params: GetUsagePlansRequest): Promise<UsagePlans>
</file>

<file path="src/mcp/validation/index.ts">
/**
 * Shared validation core for MCP convention checking
 * Exports all validation rules and provides a unified validation interface
 *
 * This module can be consumed by:
 * - MCP validate_pattern tool
 * - CI validation scripts
 * - Future tooling
 */
import {Project} from 'ts-morph'
import type {SourceFile} from 'ts-morph'
‚ãÆ----
import type {ValidationResult, ValidationRule, Violation} from './types'
import {awsSdkEncapsulationRule} from './rules/aws-sdk-encapsulation'
import {electrodbMockingRule} from './rules/electrodb-mocking'
import {importOrderRule} from './rules/import-order'
import {responseHelpersRule} from './rules/response-helpers'
import {configEnforcementRule} from './rules/config-enforcement'
import {typesLocationRule} from './rules/types-location'
import {envValidationRule} from './rules/env-validation'
import {cascadeSafetyRule} from './rules/cascade-safety'
import {batchRetryRule} from './rules/batch-retry'
import {scanPaginationRule} from './rules/scan-pagination'
import {responseEnumRule} from './rules/response-enum'
import {mockFormattingRule} from './rules/mock-formatting'
import {docSyncRule} from './rules/doc-sync'
import {namingConventionsRule} from './rules/naming-conventions'
import {authenticatedHandlerEnforcementRule} from './rules/authenticated-handler-enforcement'
// Export all rules (15 total: 6 CRITICAL + 6 HIGH + 3 MEDIUM)
‚ãÆ----
// CRITICAL
‚ãÆ----
// HIGH
‚ãÆ----
// MEDIUM
‚ãÆ----
// HIGH (documentation)
‚ãÆ----
// HIGH (naming)
‚ãÆ----
// HIGH (auth)
‚ãÆ----
// Export rules by name for selective validation
‚ãÆ----
// CRITICAL rules
‚ãÆ----
'aws-sdk': awsSdkEncapsulationRule, // alias
‚ãÆ----
electrodb: electrodbMockingRule, // alias
‚ãÆ----
config: configEnforcementRule, // alias
‚ãÆ----
env: envValidationRule, // alias
‚ãÆ----
cascade: cascadeSafetyRule, // alias
// HIGH rules
‚ãÆ----
response: responseHelpersRule, // alias
‚ãÆ----
types: typesLocationRule, // alias
‚ãÆ----
batch: batchRetryRule, // alias
‚ãÆ----
scan: scanPaginationRule, // alias
// MEDIUM rules
‚ãÆ----
imports: importOrderRule, // alias
‚ãÆ----
enum: responseEnumRule, // alias
‚ãÆ----
mock: mockFormattingRule, // alias
// HIGH (documentation) rules
‚ãÆ----
docs: docSyncRule, // alias
// HIGH (naming) rules
‚ãÆ----
naming: namingConventionsRule, // alias
// HIGH (auth) rules
‚ãÆ----
auth: authenticatedHandlerEnforcementRule // alias
‚ãÆ----
// Export individual rules
‚ãÆ----
// Export types
‚ãÆ----
/**
 * Check if a file path matches a glob-like pattern
 */
function matchesPattern(filePath: string, pattern: string): boolean
‚ãÆ----
// Simple glob matching: ** matches any path segment, * matches within segment
‚ãÆ----
/**
 * Check if a rule applies to a given file
 */
function ruleApplies(rule: ValidationRule, filePath: string): boolean
‚ãÆ----
// Check if file matches any of the appliesTo patterns
‚ãÆ----
// Check if file matches any exclude patterns
‚ãÆ----
interface ValidateFileOptions {
  /** Specific rules to run (by name). If not provided, runs all applicable rules */
  rules?: string[]
  /** Project root for context */
  projectRoot?: string
}
‚ãÆ----
/** Specific rules to run (by name). If not provided, runs all applicable rules */
‚ãÆ----
/** Project root for context */
‚ãÆ----
/**
 * Validate a single file against convention rules
 */
export async function validateFile(filePath: string, options: ValidateFileOptions =
‚ãÆ----
// Create ts-morph project and load the file
‚ãÆ----
// Determine which rules to run
‚ãÆ----
/**
 * Validate multiple files
 */
export async function validateFiles(filePaths: string[], options: ValidateFileOptions =
/**
 * Get validation summary for multiple results
 */
export function getValidationSummary(
  results: ValidationResult[]
):
</file>

<file path="src/types/global.d.ts">
interface ProcessEnv {
      ApnsDefaultTopic: string
      ApnsKeyId: string
      ApnsSigningKey: string
      ApnsTeam: string
      ApplicationUrl: string
      Bucket: string
      DefaultFileSize: string
      DefaultFileName: string
      DefaultFileUrl: string
      DefaultFileContentType: string
      DynamoDBTableName: string
      GithubPersonalToken: string
      MultiAuthenticationPathParts: string
      PlatformApplicationArn: string
      PushNotificationTopicArn: string
      ReservedClientIp: string
      SignInWithAppleConfig: string
      SNSQueueUrl: string
      StateMachineArn: string
      YtdlpBinaryPath: string
    }
‚ãÆ----
// Adding this export declaration file which Typescript/CRA can now pick up
</file>

<file path="terraform/log_client_event.tf">
resource "aws_iam_role" "LogClientEventRole" {
  name               = "LogClientEventRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaGatewayAssumeRole.json
}

resource "aws_iam_role_policy_attachment" "LogClientEventPolicyLogging" {
  role       = aws_iam_role.LogClientEventRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "LogClientEventPolicyXRay" {
  role       = aws_iam_role.LogClientEventRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

resource "aws_lambda_permission" "LogClientEvent" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.LogClientEvent.function_name
  principal     = "apigateway.amazonaws.com"
}

resource "aws_cloudwatch_log_group" "LogClientEvent" {
  name              = "/aws/lambda/${aws_lambda_function.LogClientEvent.function_name}"
  retention_in_days = 14
}

data "archive_file" "LogClientEvent" {
  type        = "zip"
  source_file = "./../build/lambdas/LogClientEvent.js"
  output_path = "./../build/lambdas/LogClientEvent.zip"
}

resource "aws_lambda_function" "LogClientEvent" {
  description      = "Records an event from a client environment (e.g. App or Web)."
  function_name    = "LogClientEvent"
  role             = aws_iam_role.LogClientEventRole.arn
  handler          = "LogClientEvent.handler"
  runtime          = "nodejs24.x"
  depends_on       = [aws_iam_role_policy_attachment.LogClientEventPolicyLogging]
  filename         = data.archive_file.LogClientEvent.output_path
  source_code_hash = data.archive_file.LogClientEvent.output_base64sha256

  tracing_config {
    mode = "Active"
  }
}

resource "aws_api_gateway_resource" "LogEvent" {
  rest_api_id = aws_api_gateway_rest_api.Main.id
  parent_id   = aws_api_gateway_rest_api.Main.root_resource_id
  path_part   = "logEvent"
}

resource "aws_api_gateway_method" "LogClientEventPost" {
  rest_api_id      = aws_api_gateway_rest_api.Main.id
  resource_id      = aws_api_gateway_resource.LogEvent.id
  http_method      = "POST"
  authorization    = "NONE"
  api_key_required = true
}

resource "aws_api_gateway_integration" "LogClientEventPost" {
  rest_api_id             = aws_api_gateway_rest_api.Main.id
  resource_id             = aws_api_gateway_resource.LogEvent.id
  http_method             = aws_api_gateway_method.LogClientEventPost.http_method
  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = aws_lambda_function.LogClientEvent.invoke_arn
}
</file>

<file path="terraform/user_subscribe.tf">
resource "aws_iam_role" "UserSubscribeRole" {
  name               = "UserSubscribeRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaGatewayAssumeRole.json
}

data "aws_iam_policy_document" "UserSubscribe" {
  statement {
    actions = ["sns:Subscribe"]
    resources = compact([
      aws_sns_topic.PushNotifications.arn,
      length(aws_sns_platform_application.OfflineMediaDownloader) == 1 ? aws_sns_platform_application.OfflineMediaDownloader[0].arn : ""
    ])
  }
}

resource "aws_iam_policy" "UserSubscribeRolePolicy" {
  name   = "UserSubscribeRolePolicy"
  policy = data.aws_iam_policy_document.UserSubscribe.json
}

resource "aws_iam_role_policy_attachment" "UserSubscribePolicy" {
  role       = aws_iam_role.UserSubscribeRole.name
  policy_arn = aws_iam_policy.UserSubscribeRolePolicy.arn
}

resource "aws_iam_role_policy_attachment" "UserSubscribePolicyLogging" {
  role       = aws_iam_role.UserSubscribeRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "UserSubscribePolicyXRay" {
  role       = aws_iam_role.UserSubscribeRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

resource "aws_lambda_permission" "UserSubscribe" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.UserSubscribe.function_name
  principal     = "apigateway.amazonaws.com"
}

resource "aws_cloudwatch_log_group" "UserSubscribe" {
  name              = "/aws/lambda/${aws_lambda_function.UserSubscribe.function_name}"
  retention_in_days = 14
}

data "archive_file" "UserSubscribe" {
  type        = "zip"
  source_file = "./../build/lambdas/UserSubscribe.js"
  output_path = "./../build/lambdas/UserSubscribe.zip"
}

resource "aws_lambda_function" "UserSubscribe" {
  description      = "Subscribes a device to an SNS topic"
  function_name    = "UserSubscribe"
  role             = aws_iam_role.UserSubscribeRole.arn
  handler          = "UserSubscribe.handler"
  runtime          = "nodejs24.x"
  depends_on       = [aws_iam_role_policy_attachment.UserSubscribePolicy]
  filename         = data.archive_file.UserSubscribe.output_path
  source_code_hash = data.archive_file.UserSubscribe.output_base64sha256

  tracing_config {
    mode = "Active"
  }

  environment {
    variables = {
      PlatformApplicationArn = length(aws_sns_platform_application.OfflineMediaDownloader) == 1 ? aws_sns_platform_application.OfflineMediaDownloader[0].arn : ""
    }
  }
}

resource "aws_api_gateway_resource" "UserSubscribe" {
  rest_api_id = aws_api_gateway_rest_api.Main.id
  parent_id   = aws_api_gateway_rest_api.Main.root_resource_id
  path_part   = "userSubscribe"
}

resource "aws_api_gateway_method" "UserSubscribePost" {
  rest_api_id      = aws_api_gateway_rest_api.Main.id
  resource_id      = aws_api_gateway_resource.UserSubscribe.id
  http_method      = "POST"
  authorization    = "NONE"
  api_key_required = true
}

resource "aws_api_gateway_integration" "UserSubscribePost" {
  rest_api_id             = aws_api_gateway_rest_api.Main.id
  resource_id             = aws_api_gateway_resource.UserSubscribe.id
  http_method             = aws_api_gateway_method.UserSubscribePost.http_method
  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = aws_lambda_function.UserSubscribe.invoke_arn
}
</file>

<file path="test/integration/workflows/sendPushNotification.workflow.integration.test.ts">
/**
 * SendPushNotification Workflow Integration Tests
 *
 * Tests the push notification workflow against LocalStack:
 * 1. Receive SQS FileNotification event
 * 2. Query DynamoDB UserDevices table for user's devices
 * 3. Query DynamoDB Devices table for each device's endpoint ARN
 * 4. Fan-out: Publish SNS notification to each device endpoint
 * 5. Handle errors gracefully (invalid devices, missing endpoints)
 *
 * This tests YOUR orchestration logic, not AWS SDK behavior.
 */
// Test configuration
‚ãÆ----
// Set environment variables for Lambda
‚ãÆ----
import {afterAll, beforeAll, beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {SQSEvent} from 'aws-lambda'
// Test helpers
import {createFilesTable, deleteFilesTable} from '../helpers/dynamodb-helpers'
import {createElectroDBEntityMock} from '#test/helpers/electrodb-mock'
import {createMockContext} from '../helpers/lambda-context'
import {createMockDevice, createMockSQSFileNotificationEvent, createMockUserDevice} from '../helpers/test-data'
// Use path aliases matching handler imports for proper mock resolution
‚ãÆ----
type PublishCallArgs = [{TargetArn: string}]
‚ãÆ----
// Create LocalStack infrastructure (if needed)
‚ãÆ----
// Wait for tables to be ready
‚ãÆ----
// Clean up LocalStack infrastructure
‚ãÆ----
// Clear all mocks before each test
‚ãÆ----
// Reset mock implementations
‚ãÆ----
// Arrange: Mock ElectroDB responses
// First query: getUserDevicesByUserId returns array of individual UserDevice records
‚ãÆ----
// Arrange: Mock ElectroDB responses
// First query: getUserDevicesByUserId returns array of individual UserDevice records
‚ãÆ----
// Assert: ElectroDB queried 4 times (1 UserDevices + 3 Devices)
‚ãÆ----
// Assert: SNS publish called 3 times (one per device)
‚ãÆ----
// Arrange: Mock ElectroDB to return empty array (no devices)
‚ãÆ----
// Assert: Only UserDevices queried, not Devices
‚ãÆ----
// Assert: No SNS publish (no devices to notify)
‚ãÆ----
// Arrange: Mock ElectroDB responses
‚ãÆ----
// Second device query fails (device not found)
‚ãÆ----
// Arrange: Mock ElectroDB responses for two different users
‚ãÆ----
// Assert: ElectroDB queried 4 times (2 users √ó 2 queries each)
‚ãÆ----
// Assert: SNS published 2 times (one per user)
‚ãÆ----
// Arrange: SQS event with different message type
‚ãÆ----
// Act: Invoke handler
‚ãÆ----
// Assert: No ElectroDB queries
‚ãÆ----
// Assert: No SNS publish
</file>

<file path="docs/wiki/TypeScript/Lambda-Function-Patterns.md">
# Lambda Function Patterns

## Quick Reference
- **When to use**: Writing AWS Lambda functions
- **Enforcement**: Required
- **Impact if violated**: MEDIUM - Inconsistent structure

## Import Order (STRICT)

```typescript
// 1. AWS Lambda types
import {APIGatewayProxyResult, Context} from 'aws-lambda'

// 2. ElectroDB entities
import {Files} from '../../../entities/Files'

// 3. Vendor libraries
import {createS3Upload} from '../../../lib/vendor/AWS/S3'

// 4. Type imports
import type {File} from '../../../types/domain-models'

// 5. Utilities
import {logInfo, response} from '../../../util/lambda-helpers'

// ‚ùå NEVER import AWS SDK directly - use vendor wrappers
```

## Handler Pattern

Lambda handlers use wrapper functions that eliminate boilerplate and ensure consistency:

```typescript
// Helper functions first
async function processFile(fileId: string): Promise<void> {
  const file = await Files.get({fileId}).go()
  // Process...
}

// Handler with wrappers - business logic only, no try-catch needed
export const handler = withXRay(wrapApiHandler(async ({event, context}: ApiHandlerParams) => {
  await processFile(event.fileId)
  return response(context, 200, {success: true})
  // Errors automatically converted to 500 responses
}))
```

### Available Handler Wrappers

| Wrapper | Use Case | Error Handling |
|---------|----------|----------------|
| `wrapApiHandler` | Public API endpoints | Catches errors ‚Üí 500 response |
| `wrapAuthenticatedHandler` | Auth-required endpoints | Rejects Unauthenticated + Anonymous ‚Üí 401 |
| `wrapOptionalAuthHandler` | Mixed auth endpoints | Rejects only Unauthenticated ‚Üí 401 |
| `wrapAuthorizer` | API Gateway authorizers | Propagates `Error('Unauthorized')` ‚Üí 401 |
| `wrapEventHandler` | S3/SQS batch processing | Per-record error handling |
| `wrapScheduledHandler` | CloudWatch scheduled events | Logs and rethrows errors |

All wrappers provide:
- Automatic event logging via `logInfo`
- Fixture logging for test data extraction
- `WrapperMetadata` with traceId passed to handler

## Response Format (REQUIRED)

**Mandatory**: ALWAYS use the `response` and `lambdaErrorResponse` helper functions from `lambda-helpers.ts`. Never return raw API Gateway response objects.

```typescript
// ‚úÖ CORRECT - Use response helper
return response(context, 200, {
  data: result,
  requestId: context.awsRequestId
})

// ‚úÖ CORRECT - Use error response helper
return lambdaErrorResponse(context, error)

// ‚ùå WRONG - Never return raw objects
return {
  statusCode: 200,
  body: JSON.stringify(data),
  headers: {'Content-Type': 'application/json'}
}
```

**Why**: Ensures consistent response formatting, headers, and error handling across all Lambda functions.

## Common Patterns

## No Underscore-Prefixed Unused Variables (CRITICAL)

**Rule**: Never use underscore-prefixed variables (`_event`, `_context`, `_metadata`) to suppress unused variable warnings.

**Why**: Per AGENTS.md: "Avoid backwards-compatibility hacks like renaming unused `_vars`". This pattern hides poor API design and creates maintenance debt.

**Solution**: Use object destructuring to extract only the properties you need:

```typescript
// ‚ùå WRONG - Underscore-prefixed unused parameters
export const handler = wrapApiHandler(async (event, context, _metadata) => {
  // _metadata is unused but accepted to satisfy signature
})

// ‚úÖ CORRECT - Object destructuring extracts only what's needed
export const handler = wrapApiHandler(async ({event, context}: ApiHandlerParams) => {
  // Only event and context are destructured
})
```

**Enforcement**: MCP `config-enforcement` rule validates that ESLint config doesn't allow underscore-prefixed variables.

### Public API Gateway Handler
```typescript
import type {ApiHandlerParams} from '#types/lambda-wrappers'
import {wrapApiHandler, response} from '#util/lambda-helpers'
import {withXRay} from '#lib/vendor/AWS/XRay'

// Public endpoints - no authentication required
export const handler = withXRay(wrapApiHandler(async ({event, context}: ApiHandlerParams) => {
  // Business logic - just throw errors, wrapper handles conversion
  return response(context, 200, {data: result})
}))
```

### Authenticated API Gateway Handler (PREFERRED)
```typescript
import type {AuthenticatedApiParams} from '#types/lambda-wrappers'
import {wrapAuthenticatedHandler, response} from '#util/lambda-helpers'
import {withXRay} from '#lib/vendor/AWS/XRay'

// Authenticated endpoints - userId guaranteed by wrapper
// Rejects both Unauthenticated AND Anonymous users with 401
export const handler = withXRay(wrapAuthenticatedHandler(async ({context, userId}: AuthenticatedApiParams) => {
  // userId is guaranteed to be a string - no need to check
  await deleteUser(userId)
  return response(context, 204)
}))
```

### Optional Auth API Gateway Handler
```typescript
import type {OptionalAuthApiParams} from '#types/lambda-wrappers'
import {wrapOptionalAuthHandler, response} from '#util/lambda-helpers'
import {withXRay} from '#lib/vendor/AWS/XRay'
import {UserStatus} from '#types/enums'

// Optional auth endpoints - allows anonymous but rejects invalid tokens
// Rejects only Unauthenticated (invalid token) with 401
// Anonymous users (no token) are allowed
export const handler = withXRay(wrapOptionalAuthHandler(async ({context, userId, userStatus}: OptionalAuthApiParams) => {
  if (userStatus === UserStatus.Anonymous) {
    return response(context, 200, {demo: true})
  }
  // userId is defined when Authenticated
  return response(context, 200, {userId})
}))
```

### API Gateway Authorizer
```typescript
import type {AuthorizerParams} from '#types/lambda-wrappers'
import {wrapAuthorizer} from '#util/lambda-helpers'
import {withXRay} from '#lib/vendor/AWS/XRay'

export const handler = withXRay(wrapAuthorizer(async ({event}: AuthorizerParams) => {
  // Throw Error('Unauthorized') for 401 response
  if (!isValid) throw new Error('Unauthorized')
  return generateAllow(userId, event.methodArn)
}))
```

### S3 Event Handler
```typescript
import type {EventHandlerParams} from '#types/lambda-wrappers'
import {wrapEventHandler, s3Records} from '#util/lambda-helpers'
import {withXRay} from '#lib/vendor/AWS/XRay'

// Process individual records - errors don't stop other records
async function processS3Record({record}: EventHandlerParams<S3EventRecord>) {
  const key = record.s3.object.key
  await processFile(key)
}

export const handler = withXRay(wrapEventHandler(processS3Record, {getRecords: s3Records}))
```

### SQS Handler
```typescript
import type {EventHandlerParams} from '#types/lambda-wrappers'
import {wrapEventHandler, sqsRecords} from '#util/lambda-helpers'
import {withXRay} from '#lib/vendor/AWS/XRay'

// Process individual messages - errors logged but don't stop processing
async function processSQSRecord({record}: EventHandlerParams<SQSRecord>) {
  const body = JSON.parse(record.body)
  await handleMessage(body)
}

export const handler = withXRay(wrapEventHandler(processSQSRecord, {getRecords: sqsRecords}))
```

### Scheduled Event Handler
```typescript
import type {ScheduledHandlerParams} from '#types/lambda-wrappers'
import {wrapScheduledHandler} from '#util/lambda-helpers'
import {withXRay} from '#lib/vendor/AWS/XRay'

export const handler = withXRay(wrapScheduledHandler(async ({}: ScheduledHandlerParams) => {
  // Scheduled task logic - errors propagate to CloudWatch
  await pruneOldRecords()
  return {pruned: count}
}))

## Environment Variable Handling

### Lazy Evaluation (Default Pattern)
**Rule**: Environment variables should be read inside functions, not at module scope.

```typescript
// ‚úÖ CORRECT - Read inside function (lazy evaluation)
async function processFile() {
  const bucketName = getRequiredEnv('BUCKET_NAME')
  // ...
}

// ‚ùå WRONG - Module-level read (breaks test setup)
const BUCKET_NAME = getRequiredEnv('BUCKET_NAME')  // Throws before tests can mock
```

**Why**: Module-level reads execute at import time, before test mocks can be configured.

### Acceptable Exceptions

**AWS Powertools initialization** (`src/lib/vendor/Powertools/index.ts`) uses module-level env reads:

```typescript
// Acceptable - has fallback values, required for framework initialization
export const logger = new Logger({
  serviceName: process.env['AWS_LAMBDA_FUNCTION_NAME'] || 'MediaDownloader',
  logLevel: (process.env['LOG_LEVEL'] as LogLevel) || 'INFO'
})
```

This is acceptable because:
1. All reads have fallback values (won't throw if missing)
2. Powertools requires initialization at import time for tracing to work correctly
3. Refactoring would require updating all handler imports with minimal benefit

### Helper Functions

```typescript
import {getRequiredEnv, getOptionalEnv, getOptionalEnvNumber} from '#util/env-validation'

// Required - throws if missing
const apiKey = getRequiredEnv('API_KEY')

// Optional with default
const host = getOptionalEnv('APNS_HOST', 'api.sandbox.push.apple.com')

// Optional numeric with default
const batchSize = getOptionalEnvNumber('BATCH_SIZE', 5)
```

## Best Practices

‚úÖ Use `withXRay` wrapper for tracing
‚úÖ Use appropriate handler wrapper (`wrapApiHandler`, `wrapAuthorizer`, etc.)
‚úÖ Use vendor wrappers for AWS SDK (never import AWS SDK directly)
‚úÖ Return responses using `response()` helper
‚úÖ Throw errors instead of manual try-catch (wrapper handles it)
‚úÖ Keep handler at bottom of file
‚úÖ Define record processing functions separately for event handlers
‚úÖ Read environment variables inside functions, not at module scope

## Testing

```typescript
// Mock all dependencies first
jest.unstable_mockModule('../../../lib/vendor/AWS/S3', () => ({
  createS3Upload: jest.fn()
}))

// Import handler after mocks
const {handler} = await import('../src/index')

test('processes file', async () => {
  const result = await handler(mockEvent, mockContext)
  expect(result.statusCode).toBe(200)
})
```

## Related Patterns

- [X-Ray Integration](../AWS/X-Ray-Integration.md)
- [Error Handling](TypeScript-Error-Handling.md)
- [Jest ESM Mocking](../Testing/Jest-ESM-Mocking-Strategy.md)

---

*Consistent Lambda structure: imports ‚Üí helpers ‚Üí handler with X-Ray wrapper.*
</file>

<file path="src/lambdas/LogClientEvent/src/index.ts">
import type {APIGatewayEvent, APIGatewayProxyResult, Context} from 'aws-lambda'
import {buildApiResponse, withPowertools} from '#util/lambda-helpers'
import {logInfo} from '#util/logging'
</file>

<file path="src/lambdas/UserDelete/test/index.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import {testContext} from '#util/jest-setup'
import {v4 as uuidv4} from 'uuid'
import type {CustomAPIGatewayRequestAuthorizerEvent} from '#types/infrastructure-types'
import {createElectroDBEntityMock} from '#test/helpers/electrodb-mock'
‚ãÆ----
{deviceId: '67C431DE-37D2-4BBA-9055-E9D2766517E1', userId: fakeUserId}, // fmt: multiline
‚ãÆ----
getUserDevices: getUserDevicesMock, // fmt: multiline
‚ãÆ----
deleteEndpoint: jest.fn().mockReturnValue({ResponseMetadata: {RequestId: uuidv4()}}), // fmt: multiline
‚ãÆ----
// Set default mock return values
‚ãÆ----
// With Authorization header but unknown principalId = Unauthenticated
‚ãÆ----
// Without Authorization header = Anonymous
</file>

<file path="src/lib/vendor/AWS/SQS.ts">
import {SendMessageCommand} from '@aws-sdk/client-sqs'
import type {MessageAttributeValue, SendMessageRequest, SendMessageResult} from '@aws-sdk/client-sqs'
import type {SQSMessageAttribute, SQSMessageAttributes} from 'aws-lambda'
import {createSQSClient} from './clients'
‚ãÆ----
// Re-export types for application code to use
// SQSMessageAttribute/Attributes are for RECEIVING messages (aws-lambda event types)
// MessageAttributeValue is for SENDING messages (AWS SDK types)
‚ãÆ----
// Helper functions for building SQS message attributes (for sending messages)
export function stringAttribute(value: string): MessageAttributeValue
export function numberAttribute(value: number): MessageAttributeValue
export function sendMessage(params: SendMessageRequest): Promise<SendMessageResult>
</file>

<file path="src/mcp/validation/index.test.ts">
/**
 * Unit tests for validation index module
 * Tests the unified validation interface
 */
import {beforeAll, describe, expect, test} from '@jest/globals'
import {join} from 'path'
// Module loaded via dynamic import
‚ãÆ----
// CRITICAL rules
‚ãÆ----
// HIGH rules
‚ãÆ----
// MEDIUM rules
‚ãÆ----
// HIGH (naming) rules
‚ãÆ----
// HIGH (auth) rules
‚ãÆ----
// CRITICAL rules
‚ãÆ----
// HIGH rules
‚ãÆ----
// MEDIUM rules
‚ãÆ----
// CRITICAL aliases
‚ãÆ----
// HIGH aliases
‚ãÆ----
// MEDIUM aliases
‚ãÆ----
// HIGH (naming) aliases
‚ãÆ----
// HIGH (auth) aliases
‚ãÆ----
// File path should be relative
‚ãÆ----
// Should have no critical violations for valid code
// Note: Some rules may skip this file if it's not in the right location
‚ãÆ----
// Should find the AWS SDK violation
‚ãÆ----
// Only the specified rule should run (others skipped)
‚ãÆ----
// Some rules should be skipped (file not in right location)
‚ãÆ----
// Some rules should pass
‚ãÆ----
// Invalid file should have at least one violation
‚ãÆ----
// AWS SDK violation is CRITICAL
‚ãÆ----
// Create a mock valid result
‚ãÆ----
// The invalid-aws-sdk fixture is in a non-standard path but still a .ts file
‚ãÆ----
// Rule should apply (not skipped) for .ts files in src/
‚ãÆ----
// Test file should be skipped by response-helpers rule
‚ãÆ----
// response-helpers only applies to src/lambdas/**/src/index.ts
‚ãÆ----
// Should not crash, just no rules run
</file>

<file path="terraform/api_gateway.tf">
resource "aws_api_gateway_rest_api" "Main" {
  name           = "OfflineMediaDownloader"
  description    = "The API that supports the App"
  api_key_source = "HEADER"

  endpoint_configuration {
    types = ["REGIONAL"]
  }
}

resource "aws_api_gateway_deployment" "Main" {
  depends_on = [
    aws_api_gateway_integration.ListFilesGet
  ]
  rest_api_id = aws_api_gateway_rest_api.Main.id
  triggers = {
    redeployment = sha1(join(",", tolist([
      jsonencode(aws_api_gateway_integration.ListFilesGet),
      jsonencode(aws_api_gateway_integration.LogClientEventPost),
      jsonencode(aws_api_gateway_integration.LoginUserPost),
      jsonencode(aws_api_gateway_integration.RegisterDevicePost),
      jsonencode(aws_api_gateway_integration.RegisterUserPost),
      jsonencode(aws_api_gateway_integration.WebhookFeedlyPost),
      jsonencode(aws_api_gateway_integration.UserSubscribePost),
      jsonencode(aws_api_gateway_method.ListFilesGet),
      jsonencode(aws_api_gateway_method.LogClientEventPost),
      jsonencode(aws_api_gateway_method.LoginUserPost),
      jsonencode(aws_api_gateway_method.RegisterDevicePost),
      jsonencode(aws_api_gateway_method.RegisterUserPost),
      jsonencode(aws_api_gateway_method.WebhookFeedlyPost),
      jsonencode(aws_api_gateway_method.UserSubscribePost)
    ])))
  }
  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_api_gateway_stage" "Production" {
  stage_name    = "prod"
  rest_api_id   = aws_api_gateway_rest_api.Main.id
  deployment_id = aws_api_gateway_deployment.Main.id

  xray_tracing_enabled = true
}

resource "aws_api_gateway_method_settings" "Production" {
  rest_api_id = aws_api_gateway_rest_api.Main.id
  stage_name  = aws_api_gateway_stage.Production.stage_name
  method_path = "*/*"
  settings {
    metrics_enabled    = true
    logging_level      = "INFO"
    data_trace_enabled = true
  }
}

resource "aws_api_gateway_usage_plan" "iOSApp" {
  name        = "iOSApp"
  description = "Internal consumption"
  api_stages {
    api_id = aws_api_gateway_rest_api.Main.id
    stage  = aws_api_gateway_stage.Production.stage_name
  }
}

resource "aws_api_gateway_api_key" "iOSApp" {
  name        = "iOSAppKey"
  description = "The key for the iOS App"
  enabled     = true
}

resource "aws_api_gateway_usage_plan_key" "iOSApp" {
  key_id        = aws_api_gateway_api_key.iOSApp.id
  key_type      = "API_KEY"
  usage_plan_id = aws_api_gateway_usage_plan.iOSApp.id
}

resource "aws_api_gateway_gateway_response" "Default400GatewayResponse" {
  rest_api_id   = aws_api_gateway_rest_api.Main.id
  response_type = "DEFAULT_4XX"
  response_templates = {
    "application/json" = "{\"error\":{\"code\":\"custom-4XX-generic\",\"message\":$context.error.messageString},\"requestId\":\"$context.requestId\"}"
  }
}

resource "aws_api_gateway_gateway_response" "Default500GatewayResponse" {
  rest_api_id   = aws_api_gateway_rest_api.Main.id
  response_type = "DEFAULT_5XX"
  response_templates = {
    "application/json" = "{\"error\":{\"code\":\"custom-5XX-generic\",\"message\":$context.error.messageString},\"requestId\":\"$context.requestId\"}"
  }
}

resource "aws_iam_role" "ApiGatewayCloudwatchRole" {
  name               = "ApiGatewayCloudwatchRole"
  assume_role_policy = data.aws_iam_policy_document.ApiGatewayCloudwatchRole.json
}

resource "aws_iam_role_policy_attachment" "ApiGatewayCloudwatchRolePolicyAttachment" {
  role       = aws_iam_role.ApiGatewayCloudwatchRole.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs"
}

data "aws_iam_policy_document" "ApiGatewayCloudwatchRole" {
  statement {
    actions = ["sts:AssumeRole"]
    effect  = "Allow"
    principals {
      type        = "Service"
      identifiers = ["apigateway.amazonaws.com"]
    }
  }
}

resource "aws_iam_role_policy" "ApiGatewayCloudwatchRolePolicy" {
  name   = "ApiGatewayCloudwatchRolePolicy"
  role   = aws_iam_role.ApiGatewayCloudwatchRole.id
  policy = data.aws_iam_policy_document.CommonLambdaLogging.json
}

resource "aws_api_gateway_account" "Main" {
  cloudwatch_role_arn = aws_iam_role.ApiGatewayCloudwatchRole.arn
}

output "api_gateway_subdomain" {
  description = "The subdomain of the API Gateway (e.g. ow9mzeewuf)"
  value       = aws_api_gateway_rest_api.Main.id
}

output "api_gateway_stage" {
  description = "The stage of the API Gateway (e.g. prod, staging)"
  value       = aws_api_gateway_stage.Production.stage_name
}

output "api_gateway_api_key" {
  description = "The API key for the API Gateway"
  value       = aws_api_gateway_api_key.iOSApp.value
  sensitive   = true
}
</file>

<file path="terraform/user_delete.tf">
resource "aws_iam_role" "UserDeleteRole" {
  name               = "UserDeleteRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaGatewayAssumeRole.json
}

data "aws_iam_policy_document" "UserDelete" {
  # Query UserCollection to get user's files and devices
  # GetItem/DeleteItem on base table for Users, Devices, UserFiles, UserDevices
  statement {
    actions = [
      "dynamodb:Query",
      "dynamodb:GetItem",
      "dynamodb:DeleteItem"
    ]
    resources = [
      aws_dynamodb_table.MediaDownloader.arn,
      "${aws_dynamodb_table.MediaDownloader.arn}/index/UserCollection"
    ]
  }
  dynamic "statement" {
    for_each = length(aws_sns_platform_application.OfflineMediaDownloader) == 1 ? [1] : []
    content {
      actions   = ["sns:DeleteEndpoint"]
      resources = [aws_sns_platform_application.OfflineMediaDownloader[0].arn]
    }
  }
}

resource "aws_iam_policy" "UserDeleteRolePolicy" {
  name   = "UserDeleteRolePolicy"
  policy = data.aws_iam_policy_document.UserDelete.json
}

resource "aws_iam_role_policy_attachment" "UserDeletePolicy" {
  role       = aws_iam_role.UserDeleteRole.name
  policy_arn = aws_iam_policy.UserDeleteRolePolicy.arn
}

resource "aws_iam_role_policy_attachment" "UserDeletePolicyLogging" {
  role       = aws_iam_role.UserDeleteRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "UserDeletePolicyXRay" {
  role       = aws_iam_role.UserDeleteRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

resource "aws_lambda_permission" "UserDelete" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.UserDelete.function_name
  principal     = "apigateway.amazonaws.com"
}

resource "aws_cloudwatch_log_group" "UserDelete" {
  name              = "/aws/lambda/${aws_lambda_function.UserDelete.function_name}"
  retention_in_days = 14
}

data "archive_file" "UserDelete" {
  type        = "zip"
  source_file = "./../build/lambdas/UserDelete.js"
  output_path = "./../build/lambdas/UserDelete.zip"
}

resource "aws_lambda_function" "UserDelete" {
  description      = "Deletes a User and all associated data (requirement for Sign In With Apple)"
  function_name    = "UserDelete"
  role             = aws_iam_role.UserDeleteRole.arn
  handler          = "UserDelete.handler"
  runtime          = "nodejs24.x"
  depends_on       = [aws_iam_role_policy_attachment.UserDeletePolicy]
  filename         = data.archive_file.UserDelete.output_path
  source_code_hash = data.archive_file.UserDelete.output_base64sha256

  tracing_config {
    mode = "Active"
  }

  environment {
    variables = {
      DynamoDBTableName   = aws_dynamodb_table.MediaDownloader.name
      GithubPersonalToken = data.sops_file.secrets.data["github.issue.token"]
    }
  }
}

resource "aws_api_gateway_resource" "UserDelete" {
  rest_api_id = aws_api_gateway_rest_api.Main.id
  parent_id   = aws_api_gateway_rest_api.Main.root_resource_id
  path_part   = "userDelete"
}

resource "aws_api_gateway_method" "UserDeletePost" {
  rest_api_id      = aws_api_gateway_rest_api.Main.id
  resource_id      = aws_api_gateway_resource.UserDelete.id
  http_method      = "POST"
  authorization    = "NONE"
  api_key_required = true
}

resource "aws_api_gateway_integration" "UserDeletePost" {
  rest_api_id             = aws_api_gateway_rest_api.Main.id
  resource_id             = aws_api_gateway_resource.UserDelete.id
  http_method             = aws_api_gateway_method.UserDeletePost.http_method
  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = aws_lambda_function.UserDelete.invoke_arn
}
</file>

<file path="test/helpers/electrodb-mock.ts">
import {jest} from '@jest/globals'
/**
 * ElectroDB Entity Mock Structure
 * Provides type-safe mocks for all common ElectroDB operations
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/Jest-ESM-Mocking-Strategy | Jest ESM Mocking Strategy}
 */
interface ElectroDBEntityMock<TData> {
  /** The entity object to pass to jest.unstable_mockModule */
  entity: {
    get: jest.Mock
    scan: {go: jest.Mock; where: jest.Mock}
    query: {
      byUser?: jest.Mock
      byFile?: jest.Mock
      byDevice?: jest.Mock
      byStatus?: jest.Mock
      byStatusRetryAfter?: jest.Mock
      byKey?: jest.Mock
      byEmail?: jest.Mock
      byProvider?: jest.Mock
      byIdentifier?: jest.Mock
      byToken?: jest.Mock
      byAppleDeviceId?: jest.Mock
    }
    create: jest.Mock
    upsert: jest.Mock
    update: jest.Mock
    delete: jest.Mock
  }
  /** Individual mock functions for assertions and setup */
  mocks: {
    get: jest.Mock<() => Promise<{data: TData | TData[] | undefined; unprocessed?: unknown[]} | undefined>>
    scan: {go: jest.Mock<() => Promise<{data: TData[]} | undefined>>; where: jest.Mock}
    query: {
      byUser?: {go: jest.Mock<() => Promise<{data: TData[]} | undefined>>; where: jest.Mock}
      byFile?: {go: jest.Mock<() => Promise<{data: TData[]} | undefined>>; where: jest.Mock}
      byDevice?: {go: jest.Mock<() => Promise<{data: TData[]} | undefined>>; where: jest.Mock}
      byStatus?: {go: jest.Mock<() => Promise<{data: TData[]} | undefined>>; where: jest.Mock}
      byStatusRetryAfter?: {go: jest.Mock<() => Promise<{data: TData[]} | undefined>>; where: jest.Mock}
      byKey?: {go: jest.Mock<() => Promise<{data: TData[]} | undefined>>; where: jest.Mock}
      byEmail?: {go: jest.Mock<() => Promise<{data: TData[]} | undefined>>; where: jest.Mock}
      byProvider?: {go: jest.Mock<() => Promise<{data: TData[]} | undefined>>; where: jest.Mock}
      byIdentifier?: {go: jest.Mock<() => Promise<{data: TData[]} | undefined>>; where: jest.Mock}
      byToken?: {go: jest.Mock<() => Promise<{data: TData[]} | undefined>>; where: jest.Mock}
      byAppleDeviceId?: {go: jest.Mock<() => Promise<{data: TData[]} | undefined>>; where: jest.Mock}
    }
    create: jest.Mock<() => Promise<{data: TData}>>
    upsert: {go: jest.Mock<() => Promise<{data: TData}>>}
    update: {go: jest.Mock<() => Promise<{data: TData} | undefined>>; set: jest.Mock; add: jest.Mock; delete: jest.Mock}
    delete: jest.Mock<() => Promise<{unprocessed?: unknown[]} | void>>
  }
}
‚ãÆ----
/** The entity object to pass to jest.unstable_mockModule */
‚ãÆ----
/** Individual mock functions for assertions and setup */
‚ãÆ----
/**
 * Creates a complete mock for an ElectroDB entity
 *
 * Supports all common ElectroDB operations: get, scan, query, create, update, delete
 *
 * @param options - Configuration options with queryIndexes array
 * @returns Object with entity mock and individual mock functions
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/Jest-ESM-Mocking-Strategy#electrodb-mock-helper-critical | ElectroDB Mock Helper}
 */
export function createElectroDBEntityMock<TData = unknown>(options?: {
  queryIndexes?: Array<
    | 'byUser'
    | 'byFile'
    | 'byDevice'
    | 'byStatus'
    | 'byStatusRetryAfter'
    | 'byKey'
    | 'byEmail'
    | 'byProvider'
    | 'byIdentifier'
    | 'byToken'
    | 'byAppleDeviceId'
  >
}): ElectroDBEntityMock<TData>
‚ãÆ----
// Get operation: Entity.get({key}).go() or Entity.get([...]).go()
// Supports both single and batch operations
‚ãÆ----
// Scan operation: Entity.scan().go() or Entity.scan().where(...).go()
‚ãÆ----
// Query operations: Entity.query.byIndex({key}).go() or Entity.query.byIndex({key}).where(...).go()
type QueryIndexName =
    | 'byUser'
    | 'byFile'
    | 'byDevice'
    | 'byStatus'
    | 'byStatusRetryAfter'
    | 'byKey'
    | 'byEmail'
    | 'byProvider'
    | 'byIdentifier'
    | 'byToken'
    | 'byAppleDeviceId'
‚ãÆ----
// Create operation: Entity.create(item).go()
‚ãÆ----
// Upsert operation: Entity.upsert(item).go()
‚ãÆ----
// Update operation: Entity.update({key}).set/add/delete({...}).go()
‚ãÆ----
// Delete operation: Entity.delete({key}).go() or Entity.delete([...]).go()
// Supports both single (returns void) and batch (returns {unprocessed}) operations
</file>

<file path="test/integration/helpers/dynamodb-helpers.ts">
/**
 * DynamoDB Test Helpers
 *
 * Utilities for inserting and querying test data in LocalStack DynamoDB
 */
import {createTable, deleteTable} from '../lib/vendor/AWS/DynamoDB'
import type {File} from '#types/domain-models'
import {FileStatus} from '#types/enums'
import {createMockFile} from './test-data'
function getMediaDownloaderTable()
function getIdempotencyTable()
/**
 * Create MediaDownloader table in LocalStack with single-table design
 * Matches production Terraform configuration with pk/sk and 5 GSIs
 */
export async function createMediaDownloaderTable(): Promise<void>
/**
 * Delete MediaDownloader table from LocalStack
 */
export async function deleteMediaDownloaderTable(): Promise<void>
‚ãÆ----
// Table might not exist
‚ãÆ----
/**
 * Create Idempotency table in LocalStack for Powertools Idempotency
 * Simple partition key table with TTL for automatic cleanup
 */
export async function createIdempotencyTable(): Promise<void>
/**
 * Delete Idempotency table from LocalStack
 */
export async function deleteIdempotencyTable(): Promise<void>
‚ãÆ----
// Table might not exist
‚ãÆ----
/**
 * Legacy aliases for backward compatibility - these call the new single-table functions
 * @deprecated Use createMediaDownloaderTable instead
 */
‚ãÆ----
/**
 * @deprecated Use deleteMediaDownloaderTable instead
 */
‚ãÆ----
/**
 * Insert a file record into DynamoDB using ElectroDB
 * Uses createMockFile for consistent defaults across all tests
 * This ensures proper entity metadata is added for ElectroDB compatibility
 */
export async function insertFile(file: Partial<File>): Promise<void>
‚ãÆ----
// Get consistent defaults from createMockFile, then apply user overrides
‚ãÆ----
// ElectroDB requires all fields - createMockFile provides them all
‚ãÆ----
/**
 * Get a file record from DynamoDB using ElectroDB
 */
export async function getFile(fileId: string): Promise<Partial<File> | null>
</file>

<file path="src/lambdas/ApiGatewayAuthorizer/test/index.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {APIGatewayRequestAuthorizerEvent} from 'aws-lambda'
‚ãÆ----
import {v4 as uuidv4} from 'uuid'
import {UnexpectedError} from '#util/errors'
import {testContext} from '#util/jest-setup'
import type {SessionPayload} from '#types/util'
‚ãÆ----
getApiKeys: getApiKeysMock, // fmt: multiline
‚ãÆ----
// Setup variations of the getApiKeys response
‚ãÆ----
// if the path supports requires authentication, enforce it
</file>

<file path="src/lambdas/PruneDevices/test/index.test.ts">
import {describe, expect, jest, test} from '@jest/globals'
import type {ScheduledEvent} from 'aws-lambda'
import {fakePrivateKey, testContext} from '#util/jest-setup'
import {v4 as uuidv4} from 'uuid'
import {UnexpectedError} from '#util/errors'
import {createElectroDBEntityMock} from '#test/helpers/electrodb-mock'
// Set APNS env vars for ApnsClient
‚ãÆ----
deleteEndpoint: jest.fn().mockReturnValue({ResponseMetadata: {RequestId: uuidv4()}}), // fmt: multiline
‚ãÆ----
class MockApnsClient
‚ãÆ----
send()
‚ãÆ----
ApnsClient: MockApnsClient, // fmt: multiline
‚ãÆ----
function getExpiredResponseForDevice(arrayIndex: number)
‚ãÆ----
buildApnsOptions()
‚ãÆ----
get priority()
get pushType()
‚ãÆ----
function getSuccessfulResponseForDevice(arrayIndex: number)
</file>

<file path="src/lib/vendor/AWS/DynamoDB.ts">
import {DynamoDBDocument} from '@aws-sdk/lib-dynamodb'
import {createDynamoDBClient} from './clients'
/**
 * DynamoDB DocumentClient for ElectroDB entities.
 *
 * This wrapper provides a shared DocumentClient instance with X-Ray tracing
 * for all ElectroDB entity operations. The DocumentClient handles marshalling
 * and unmarshalling of DynamoDB attribute values automatically.
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/ElectroDB-Testing-Patterns#localstack-setup | DynamoDB DocumentClient Usage}
 */
‚ãÆ----
/**
 * Shared DynamoDB DocumentClient instance.
 * Used by all ElectroDB entities to ensure consistent X-Ray tracing.
 */
</file>

<file path="src/pipeline/infrastructure.environment.test.ts">
import {describe, expect, test} from '@jest/globals'
‚ãÆ----
import type {InfrastructureD} from '#types/infrastructure'
import {logDebug} from '#util/logging'
import path from 'path'
import {fileURLToPath} from 'url'
‚ãÆ----
// IF NEW DEPENDENCIES ARE ADDED, YOU MAY NEED TO ADD MORE EXCLUSIONS HERE
‚ãÆ----
PATH: 1, // System PATH for Lambda runtime and custom binaries
// Library false positives (Zod literals, HTTP headers, etc.)
Exclusive: 1, // Zod validation literal
Connection: 1, // HTTP header
Upgrade: 1, // HTTP header
// better-auth library model/type names (matched by minified function call pattern)
Account: 1, // better-auth Account model
Session: 1, // better-auth Session model
ZodSuccess: 1, // Zod brand/success type
// Web API and library types (esbuild preserves these as string literals)
FormData: 1, // Web API
Headers: 1, // Web API / HTTP
SemVer: 1, // Version handling library type
// Lambda function names referenced as strings (not env vars)
StartFileUpload: 1, // Lambda function name in invoke calls
// Query/ORM library keywords
RightJoin: 1, // ElectroDB/SQL join type
Using: 1, // Query keyword
// CloudWatch metric names (used as string literals, not env vars)
‚ãÆ----
// Patterns that indicate library operation types or domain literals, not environment variables
// These are verb+noun patterns commonly used in ORMs and libraries (e.g., ElectroDB)
‚ãÆ----
/Notification$/, // Type literals like MetadataNotification, DownloadReadyNotification
/Join$/, // SQL join types: InnerJoin, LeftJoin, RightJoin, FullJoin, CrossJoin, etc.
/^Lateral/, // SQL lateral joins: LateralLeftJoin, LateralInnerJoin, etc.
/Apply$/, // SQL apply types: CrossApply, OuterApply
/^Array/, // SQL/ORM array operations: ArrayLocation
/^Member$/ // SQL/ORM member access
‚ãÆ----
function isOperationType(variable: string): boolean
function filterSourceVariables(extractedVariables: string[]): string[]
function preprocessInfrastructurePlan(infrastructurePlan: InfrastructureD)
function getEnvironmentVariablesFromSource(functionName: string, sourceCodeRegex: RegExp, matchSubstring: number, matchSlice = [0])
‚ãÆ----
// You need to use the build version here to see dependent environment variables
‚ãÆ----
// Match direct process.env access patterns
‚ãÆ----
// Also match minified getRequiredEnv/getOptionalEnv patterns
// After minification, these become patterns like:
// - X("EnvVarName") where X is a single-letter minified function name
// - yn("EnvVarName") where yn is a 2-3 letter minified function name
// - }("EnvVarName") for IIFE patterns (immediately invoked function expressions)
// - $("EnvVarName") where $ is used as minified function name ($ is not a word char in regex)
// - bF("EnvVarName","default") for two-argument calls like getOptionalEnv (esbuild preserves both args)
// Match function calls with string arguments that look like env vars (PascalCase, min 3 chars)
‚ãÆ----
// Extract the variable name from patterns like X("VarName") or bF("VarName","default")
‚ãÆ----
}).filter(Boolean) // Exclude ALL_CAPS strings (likely constants, not env vars), crypto terms, and library types
‚ãÆ----
// Deduplicate and filter
‚ãÆ----
// Skip naming convention tests for infrastructure-level variables
‚ãÆ----
// Filter out infrastructure-level variables from Terraform list for comparison
</file>

<file path="src/util/errors.ts">
import {Notification} from 'apns2'
export class CustomLambdaError extends Error
‚ãÆ----
constructor(message: string, options?:
‚ãÆ----
// Called when the client request is invalid (usually via Joi validation)
export class ValidationError extends CustomLambdaError
‚ãÆ----
constructor(message: string, errors?: object, statusCode = 400, cause?: Error)
‚ãÆ----
// Called when the platform hasn't been configured for push
export class ServiceUnavailableError extends CustomLambdaError
‚ãÆ----
constructor(message: string, statusCode = 503, cause?: Error)
‚ãÆ----
// Called when a lambda can't extract UserID from the header
export class UnauthorizedError extends CustomLambdaError
‚ãÆ----
constructor(message: string = 'Invalid Authentication token; login', statusCode = 401, cause?: Error)
‚ãÆ----
// The video, or related metadata needed, doesn't exist or can't be found
export class NotFoundError extends CustomLambdaError
‚ãÆ----
constructor(message: string, statusCode = 404, cause?: Error)
‚ãÆ----
// The "catchall" error message; for anything unexpected
export class UnexpectedError extends CustomLambdaError
‚ãÆ----
constructor(message: string, statusCode = 500, cause?: Error)
‚ãÆ----
// Cookie expiration or bot detection error from YouTube
export class CookieExpirationError extends CustomLambdaError
‚ãÆ----
constructor(message: string, statusCode = 403, cause?: Error)
‚ãÆ----
// The errors thrown by node-apns2 (when sending push health checks)
export class Apns2Error extends Error
‚ãÆ----
constructor(reason: string, statusCode: number, notification: Notification)
</file>

<file path="src/util/github-helpers.ts">
import {Octokit} from '@octokit/rest'
import {logDebug, logError, logInfo} from './logging'
import type {Device} from '#types/domain-models'
import {renderGithubIssueTemplate} from './template-helpers'
import {getRequiredEnv} from './env-validation'
‚ãÆ----
async function getOctokitInstance()
‚ãÆ----
// Constrained to only reading/writing issues
‚ãÆ----
/* c8 ignore next */
‚ãÆ----
export async function createFailedUserDeletionIssue(userId: string, devices: Device[], error: Error, requestId: string)
‚ãÆ----
// Don't fail the Lambda if GitHub issue creation fails
‚ãÆ----
export async function createVideoDownloadFailureIssue(fileId: string, fileUrl: string, error: Error, errorDetails?: string)
‚ãÆ----
// Don't fail the Lambda if GitHub issue creation fails
‚ãÆ----
export async function createCookieExpirationIssue(fileId: string, fileUrl: string, error: Error)
‚ãÆ----
// Don't fail the Lambda if GitHub issue creation fails
</file>

<file path="terraform/api_gateway_authorizer.tf">
resource "aws_iam_role" "ApiGatewayAuthorizer" {
  name               = "ApiGatewayAuthorizer"
  assume_role_policy = data.aws_iam_policy_document.LambdaGatewayAssumeRole.json
}

data "aws_iam_policy_document" "ApiGatewayAuthorizer" {
  statement {
    actions   = ["lambda:InvokeFunction"]
    resources = [aws_lambda_function.ApiGatewayAuthorizer.arn]
  }
}

resource "aws_iam_role_policy" "ApiGatewayAuthorize" {
  name   = "ApiGatewayAuthorizerInvocationPolicy"
  role   = aws_iam_role.ApiGatewayAuthorizer.id
  policy = data.aws_iam_policy_document.ApiGatewayAuthorizer.json
}

resource "aws_cloudwatch_log_group" "ApiGatewayAuthorizer" {
  name              = "/aws/lambda/${aws_lambda_function.ApiGatewayAuthorizer.function_name}"
  retention_in_days = 14
}

resource "aws_iam_role_policy_attachment" "ApiGatewayAuthorizerPolicyLogging" {
  role       = aws_iam_role.ApiGatewayAuthorizer.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "ApiGatewayAuthorizerPolicyXRay" {
  role       = aws_iam_role.ApiGatewayAuthorizer.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

data "aws_iam_policy_document" "ApiGatewayAuthorizerRolePolicy" {
  statement {
    actions = ["apigateway:GET"]
    resources = [
      "arn:aws:apigateway:${data.aws_region.current.id}::/apikeys",
      "arn:aws:apigateway:${data.aws_region.current.id}::/apikeys/*",
      "arn:aws:apigateway:${data.aws_region.current.id}::/usageplans",
      "arn:aws:apigateway:${data.aws_region.current.id}::/usageplans/*/usage"
    ]
  }
  # Better Auth session validation requires DynamoDB access (including UpdateItem for session refresh)
  statement {
    actions = [
      "dynamodb:GetItem",
      "dynamodb:PutItem",
      "dynamodb:UpdateItem",
      "dynamodb:DeleteItem",
      "dynamodb:Query",
      "dynamodb:Scan"
    ]
    resources = [
      aws_dynamodb_table.MediaDownloader.arn,
      "${aws_dynamodb_table.MediaDownloader.arn}/index/*"
    ]
  }
}

resource "aws_iam_policy" "ApiGatewayAuthorizerRolePolicy" {
  name   = "ApiGatewayAuthorizerRolePolicy"
  policy = data.aws_iam_policy_document.ApiGatewayAuthorizerRolePolicy.json
}

resource "aws_iam_role_policy_attachment" "ApiGatewayAuthorizerPolicy" {
  role       = aws_iam_role.ApiGatewayAuthorizer.name
  policy_arn = aws_iam_policy.ApiGatewayAuthorizerRolePolicy.arn
}

resource "aws_lambda_function" "ApiGatewayAuthorizer" {
  description   = "The function that handles authorization for the API Gateway."
  function_name = "ApiGatewayAuthorizer"
  role          = aws_iam_role.ApiGatewayAuthorizer.arn
  handler       = "ApiGatewayAuthorizer.handler"
  runtime       = "nodejs24.x"
  depends_on = [
    aws_iam_role_policy_attachment.ApiGatewayAuthorizerPolicy,
    aws_iam_role_policy_attachment.ApiGatewayAuthorizerPolicyLogging
  ]
  filename         = data.archive_file.ApiGatewayAuthorizer.output_path
  source_code_hash = data.archive_file.ApiGatewayAuthorizer.output_base64sha256

  tracing_config {
    mode = "Active"
  }

  environment {
    variables = {
      DynamoDBTableName = aws_dynamodb_table.MediaDownloader.name
      MultiAuthenticationPathParts = join(",", [
        aws_api_gateway_resource.RegisterDevice.path_part,
        aws_api_gateway_resource.Files.path_part,
        aws_api_gateway_resource.LogEvent.path_part
      ]),
      ReservedClientIp = "104.1.88.244"
    }
  }
}

data "archive_file" "ApiGatewayAuthorizer" {
  type        = "zip"
  source_file = "./../build/lambdas/ApiGatewayAuthorizer.js"
  output_path = "./../build/lambdas/ApiGatewayAuthorizer.zip"
}

resource "aws_lambda_permission" "ApiGatewayAuthorizer" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.ApiGatewayAuthorizer.function_name
  principal     = "apigateway.amazonaws.com"
}

resource "aws_api_gateway_authorizer" "ApiGatewayAuthorizer" {
  name                             = "ApiGatewayAuthorizer"
  rest_api_id                      = aws_api_gateway_rest_api.Main.id
  authorizer_uri                   = aws_lambda_function.ApiGatewayAuthorizer.invoke_arn
  authorizer_result_ttl_in_seconds = 0
  authorizer_credentials           = aws_iam_role.ApiGatewayAuthorizer.arn
  type                             = "REQUEST"
  identity_source                  = "method.request.querystring.ApiKey"
}
</file>

<file path="test/integration/workflows/fileCoordinator.workflow.integration.test.ts">
/**
 * FileCoordinator Workflow Integration Tests
 *
 * Tests the FileCoordinator Lambda against real LocalStack DynamoDB:
 * 1. Query FileDownloads for pending downloads (status='pending')
 * 2. Query FileDownloads for scheduled retries (status='scheduled', retryAfter is past)
 * 3. Fan-out to StartFileUpload for each file
 *
 * Architecture: FileCoordinator now ONLY queries FileDownloads entity.
 * Files entity is for permanent metadata, FileDownloads for orchestration.
 */
// Test configuration
‚ãÆ----
import {afterAll, beforeAll, beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {Context} from 'aws-lambda'
import {DownloadStatus} from '#types/enums'
import {createFilesTable, deleteFilesTable} from '#test/integration/helpers/dynamodb-helpers'
import {createMockContext} from '#test/integration/helpers/lambda-context'
import {createMockScheduledEvent} from '#test/integration/helpers/test-data'
// Type the mock with full signature so mock.calls is properly typed
‚ãÆ----
// Note: No #lambdas/* path alias exists, using relative import for handler
‚ãÆ----
// Helper to insert a FileDownloads record for testing
async function insertFileDownload(fileId: string, status: DownloadStatus, retryAfter?: number)
// Helper to clear FileDownloads table
async function clearFileDownloads()
‚ãÆ----
// Query and delete all - for testing purposes
‚ãÆ----
// Clear FileDownloads between tests
‚ãÆ----
// Create 3 pending download records
‚ãÆ----
// FileCoordinator is a scheduled handler that returns void
‚ãÆ----
// No FileDownloads records - should handle gracefully (returns void)
‚ãÆ----
// Scheduled retry ready to process
‚ãÆ----
// Scheduled retry not yet ready
‚ãÆ----
// Another ready one
‚ãÆ----
// Should process 2 retries (past-retry and now-retry, not future-retry)
‚ãÆ----
// Pending download (new)
‚ãÆ----
// Scheduled retry (ready)
‚ãÆ----
// Both pending and scheduled should be processed
</file>

<file path="test/integration/workflows/startFileUpload.workflow.integration.test.ts">
/**
 * StartFileUpload Workflow Integration Tests
 *
 * Tests the complete video download workflow:
 * - YouTube API: mocked (fetchVideoInfo, streamVideoToS3)
 * - Files entity: LocalStack DynamoDB
 * - FileDownloads entity: mocked (transient orchestration state)
 * - S3 storage: LocalStack S3
 *
 * This tests orchestration logic, not AWS SDK behavior.
 */
// Test configuration
‚ãÆ----
// Set environment variables for Lambda
‚ãÆ----
import {afterAll, beforeAll, beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {Context} from 'aws-lambda'
import {DownloadStatus, FileStatus} from '#types/enums'
// Test helpers
import {createFilesTable, deleteFilesTable, getFile} from '#test/integration/helpers/dynamodb-helpers'
import {createTestBucket, deleteTestBucket} from '#test/integration/helpers/s3-helpers'
import {createMockContext} from '#test/integration/helpers/lambda-context'
import {createMockVideoInfo} from '#test/integration/helpers/mock-youtube'
import {createElectroDBEntityMock} from '#test/helpers/electrodb-mock'
‚ãÆ----
// Mock file size for download response
‚ãÆ----
// FetchVideoInfoResult type for the new safe fetchVideoInfo API
type FetchVideoInfoResult = {success: boolean; info?: typeof mockVideoInfo; error?: Error; isCookieError?: boolean}
// Mock modules using path aliases to match how handler imports them
‚ãÆ----
// fetchVideoInfo now returns a result object {success, info, error}
‚ãÆ----
// downloadVideoToS3 replaces the old streamVideoToS3 - returns fileSize, s3Url, duration
‚ãÆ----
createVideoDownloadFailureIssue: jest.fn<() => Promise<void>>().mockResolvedValue(undefined), // fmt: multiline
‚ãÆ----
// Mock FileDownloads entity (transient orchestration state)
‚ãÆ----
DownloadStatus // Re-export the real enum
‚ãÆ----
// Mock UserFiles entity for MetadataNotification dispatch
‚ãÆ----
// Mock SQS sendMessage for MetadataNotification
‚ãÆ----
// Note: No #lambdas/* path alias exists, using relative import for handler
‚ãÆ----
// Return empty user list for MetadataNotification dispatch (no users waiting)
‚ãÆ----
// Note: S3 upload is mocked in this orchestration test.
// S3 integration is validated in YouTube.ts unit tests.
</file>

<file path="test/integration/workflows/webhookFeedly.workflow.integration.test.ts">
/**
 * WebhookFeedly Workflow Integration Tests
 *
 * Tests the Feedly webhook workflow against LocalStack:
 * 1. Extract video ID from article URL
 * 2. Associate file with user in DynamoDB
 * 3. Check if file already exists (Downloaded ‚Üí send SQS, new ‚Üí add to DynamoDB)
 * 4. Handle duplicate webhooks (idempotency)
 */
‚ãÆ----
import {afterAll, beforeAll, beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {Context} from 'aws-lambda'
import {FileStatus} from '#types/enums'
import type {CustomAPIGatewayRequestAuthorizerEvent} from '#types/infrastructure-types'
import {createFilesTable, createIdempotencyTable, deleteFilesTable, deleteIdempotencyTable, getFile, insertFile} from '../helpers/dynamodb-helpers'
import {createMockContext} from '../helpers/lambda-context'
interface FileInvocationPayload {
  fileId: string
}
type SQSCallArgs = [
  {QueueUrl: string; MessageBody: string; MessageAttributes?: Record<string, {StringValue: string; DataType: string}>}
]
‚ãÆ----
// Use path aliases matching handler imports for proper mock resolution
‚ãÆ----
function createWebhookEvent(articleURL: string, backgroundMode: boolean, userId: string): CustomAPIGatewayRequestAuthorizerEvent
‚ãÆ----
// Recreate tables for clean state each test
‚ãÆ----
// Verify message body is JSON with DownloadReadyNotification format
‚ãÆ----
// FileCoordinator will pick up this file later
‚ãÆ----
// StartFileUpload uses conditional updates for deduplication
</file>

<file path="CLAUDE.md">
@AGENTS.md
</file>

<file path="src/lambdas/CloudfrontMiddleware/test/index.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {CloudFrontRequestEvent} from 'aws-lambda'
import {testContext} from '#util/jest-setup'
</file>

<file path="src/lambdas/RefreshToken/src/index.ts">
/**
 * RefreshToken Lambda
 *
 * Refreshes a user session by extending the expiration time.
 * This allows users to stay logged in without re-authenticating.
 *
 * Request: POST with Authorization Bearer header, empty body
 * Response: 200 with token, expiresAt, sessionId; 401 for invalid session; 500 for errors
 */
import type {APIGatewayProxyEvent, APIGatewayProxyResult} from 'aws-lambda'
import type {ApiHandlerParams} from '#types/lambda-wrappers'
import {buildApiResponse, withPowertools, wrapApiHandler} from '#util/lambda-helpers'
import {logDebug, logError, logInfo} from '#util/logging'
import {refreshSession, validateSessionToken} from '#util/better-auth-helpers'
/**
 * Lambda handler for refreshing session tokens.
 *
 * Validates the current session token from the Authorization header,
 * extends the session expiration, and returns the updated expiration.
 *
 * @param event - API Gateway proxy event
 * @param context - Lambda context
 * @returns API Gateway proxy result with refreshed session info
 */
‚ãÆ----
// Extract and validate Authorization header
‚ãÆ----
// Extract token from Bearer format
‚ãÆ----
// Validate the session token
‚ãÆ----
// Refresh the session (extend expiration)
‚ãÆ----
// Return success with updated session info
‚ãÆ----
token, // Same token, just extended expiration
</file>

<file path="src/lambdas/UserSubscribe/test/index.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import {testContext} from '#util/jest-setup'
import {v4 as uuidv4} from 'uuid'
import type {CustomAPIGatewayRequestAuthorizerEvent} from '#types/infrastructure-types'
‚ãÆ----
deleteEndpoint: jest.fn(), // fmt: multiline
‚ãÆ----
// With Authorization header but unknown principalId = Unauthenticated
‚ãÆ----
// Without Authorization header = Anonymous
</file>

<file path="src/lib/vendor/AWS/S3.ts">
import {HeadObjectCommand} from '@aws-sdk/client-s3'
import type {HeadObjectCommandInput, HeadObjectCommandOutput} from '@aws-sdk/client-s3'
import {Upload} from '@aws-sdk/lib-storage'
import type {Options as UploadOptions} from '@aws-sdk/lib-storage'
import {Readable} from 'stream'
import {createS3Client} from './clients'
‚ãÆ----
/**
 * Get metadata for an S3 object
 * @param bucket - S3 bucket name
 * @param key - S3 object key
 * @returns Object metadata including ContentLength
 */
/* c8 ignore start - Pure AWS SDK wrapper, tested via integration tests */
export async function headObject(bucket: string, key: string): Promise<HeadObjectCommandOutput>
/* c8 ignore stop */
/**
 * Create a multipart upload stream to S3
 * @param bucket - S3 bucket name
 * @param key - S3 object key
 * @param body - Stream or buffer to upload
 * @param contentType - Content type of the object (defaults to 'video/mp4')
 * @param options - Optional upload configuration
 * @returns Upload instance for streaming data to S3
 */
/* c8 ignore start - Thin wrapper with default parameters, tested via integration tests */
export function createS3Upload(
  bucket: string,
  key: string,
  body: Readable | Buffer,
  contentType: string = 'video/mp4',
  options?: Partial<UploadOptions>
): Upload
‚ãÆ----
partSize: options?.partSize || 5 * 1024 * 1024, // 5MB default
‚ãÆ----
/* c8 ignore stop */
</file>

<file path="src/lib/vendor/AWS/SNS.ts">
import {
  CreatePlatformEndpointCommand,
  DeleteEndpointCommand,
  ListSubscriptionsByTopicCommand,
  PublishCommand,
  SubscribeCommand,
  UnsubscribeCommand
} from '@aws-sdk/client-sns'
import type {
  CreateEndpointResponse,
  CreatePlatformEndpointInput,
  DeleteEndpointInput,
  ListSubscriptionsByTopicInput,
  ListSubscriptionsByTopicResponse,
  PublishInput,
  PublishResponse,
  SubscribeInput,
  SubscribeResponse,
  UnsubscribeInput
} from '@aws-sdk/client-sns'
import {createSNSClient} from './clients'
‚ãÆ----
// Re-export types for application code to use
‚ãÆ----
/* c8 ignore start - Pure AWS SDK wrapper, tested via integration tests */
export function publishSnsEvent(params: PublishInput): Promise<PublishResponse>
/* c8 ignore stop */
/* c8 ignore start - Pure AWS SDK wrapper, tested via integration tests */
export function subscribe(params: SubscribeInput): Promise<SubscribeResponse>
/* c8 ignore stop */
/* c8 ignore start - Pure AWS SDK wrapper, tested via integration tests */
export function listSubscriptionsByTopic(params: ListSubscriptionsByTopicInput): Promise<ListSubscriptionsByTopicResponse>
/* c8 ignore stop */
/* c8 ignore start - Pure AWS SDK wrapper, tested via integration tests */
export function createPlatformEndpoint(params: CreatePlatformEndpointInput): Promise<CreateEndpointResponse>
/* c8 ignore stop */
/* c8 ignore start - Pure AWS SDK wrapper, tested via integration tests */
export function unsubscribe(params: UnsubscribeInput): Promise<object>
/* c8 ignore stop */
/* c8 ignore start - Pure AWS SDK wrapper, tested via integration tests */
export function deleteEndpoint(params: DeleteEndpointInput): Promise<object>
/* c8 ignore stop */
</file>

<file path="src/lib/vendor/BetterAuth/electrodb-adapter.ts">
/**
 * Better Auth ElectroDB Adapter
 *
 * Custom database adapter for Better Auth that uses ElectroDB with DynamoDB single-table design.
 * Uses Better Auth's createAdapterFactory for proper integration.
 *
 * @see https://www.better-auth.com/docs/guides/create-a-db-adapter
 */
import {createAdapterFactory} from 'better-auth/adapters'
import {Users} from '#entities/Users'
import {Sessions} from '#entities/Sessions'
import {Accounts} from '#entities/Accounts'
import {VerificationTokens} from '#entities/VerificationTokens'
import {v4 as uuidv4} from 'uuid'
import {logDebug, logError} from '#util/logging'
type ModelName = 'user' | 'session' | 'account' | 'verification'
/**
 * Primary key field for each model
 */
‚ãÆ----
/**
 * Transform input data from Better Auth format to ElectroDB format
 */
function transformInputData(model: string, data: Record<string, unknown>): Record<string, unknown>
‚ãÆ----
// Map 'id' to the appropriate primary key field
‚ãÆ----
// Convert Date objects or ISO strings to timestamps for ElectroDB
‚ãÆ----
// Handle ISO date strings from Better Auth
‚ãÆ----
// Ensure primary key is set
‚ãÆ----
// Handle user-specific fields
‚ãÆ----
// Set flattened appleDeviceId for GSI lookup (denormalized from identityProviders.userId)
‚ãÆ----
// Handle account field mapping
‚ãÆ----
/**
 * Transform output data from ElectroDB format to Better Auth format
 */
function transformOutputData(model: string, data: Record<string, unknown> | null): Record<string, unknown> | null
‚ãÆ----
// Map primary key back to 'id'
‚ãÆ----
// Convert timestamps to Date objects
‚ãÆ----
// Combine firstName/lastName into name for user model
‚ãÆ----
// Map account fields back
‚ãÆ----
// Type for where clause
type WhereClause = Array<{field: string; value: unknown; operator?: string}>
/**
 * Creates a Better Auth adapter for ElectroDB/DynamoDB.
 */
‚ãÆ----
async create<T>(
async findOne<T>(
‚ãÆ----
// Handle by model type
‚ãÆ----
// Better Auth often looks up sessions by token alone
‚ãÆ----
// Filter by other conditions if present
‚ãÆ----
async findMany<T>(
async update<T>(
‚ãÆ----
delete transformedUpdate[pkField] // Remove PK from update data
‚ãÆ----
async updateMany(): Promise<number>
‚ãÆ----
// ElectroDB doesn't support batch updates directly
‚ãÆ----
async delete(
async deleteMany(
async count(
‚ãÆ----
/**
 * Splits a full name into first and last name parts.
 */
export function splitFullName(fullName?: string):
</file>

<file path="src/util/constants.ts">
import type {File} from '#types/domain-models'
import {FileStatus} from '#types/enums'
import {getRequiredEnv, getRequiredEnvNumber} from './env-validation'
/**
 * Cached default file to avoid repeated env var lookups
 */
‚ãÆ----
/**
 * Returns the default file shown to new/anonymous users.
 * Uses lazy evaluation to avoid module-level env validation that breaks tests.
 * @returns The default file with properties from environment variables
 */
export function getDefaultFile(): File
</file>

<file path="terraform/prune_devices.tf">
resource "aws_iam_role" "PruneDevicesRole" {
  name               = "PruneDevicesRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaAssumeRole.json
}

data "aws_iam_policy_document" "PruneDevices" {
  # Query DeviceCollection to find users by device
  # Scan base table for all devices, GetItem/DeleteItem on base table
  statement {
    actions = [
      "dynamodb:Scan",
      "dynamodb:Query",
      "dynamodb:GetItem",
      "dynamodb:DeleteItem"
    ]
    resources = [
      aws_dynamodb_table.MediaDownloader.arn,
      "${aws_dynamodb_table.MediaDownloader.arn}/index/DeviceCollection"
    ]
  }
  dynamic "statement" {
    for_each = length(aws_sns_platform_application.OfflineMediaDownloader) == 1 ? [1] : []
    content {
      actions   = ["sns:DeleteEndpoint"]
      resources = [aws_sns_platform_application.OfflineMediaDownloader[0].arn]
    }
  }
}

resource "aws_iam_policy" "PruneDevicesRolePolicy" {
  name   = "PruneDevicesRolePolicy"
  policy = data.aws_iam_policy_document.PruneDevices.json
}

resource "aws_iam_role_policy_attachment" "PruneDevicesPolicy" {
  role       = aws_iam_role.PruneDevicesRole.name
  policy_arn = aws_iam_policy.PruneDevicesRolePolicy.arn
}

resource "aws_iam_role_policy_attachment" "PruneDevicesPolicyLogging" {
  role       = aws_iam_role.PruneDevicesRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "PruneDevicesPolicyXRay" {
  role       = aws_iam_role.PruneDevicesRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

resource "aws_cloudwatch_event_target" "PruneDevices" {
  rule = aws_cloudwatch_event_rule.PruneDevices.name
  arn  = aws_lambda_function.PruneDevices.arn
}


resource "aws_cloudwatch_event_rule" "PruneDevices" {
  name                = "PruneDevices"
  schedule_expression = "rate(1 day)"
  state               = "ENABLED"
}

resource "aws_lambda_permission" "PruneDevices" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.PruneDevices.function_name
  principal     = "events.amazonaws.com"
  source_arn    = aws_cloudwatch_event_rule.PruneDevices.arn
}

resource "aws_cloudwatch_log_group" "PruneDevices" {
  name              = "/aws/lambda/${aws_lambda_function.PruneDevices.function_name}"
  retention_in_days = 14
}

data "archive_file" "PruneDevices" {
  type        = "zip"
  source_file = "./../build/lambdas/PruneDevices.js"
  output_path = "./../build/lambdas/PruneDevices.zip"
}

resource "aws_lambda_function" "PruneDevices" {
  description      = "Validates iOS devices are still reachable; otherwise removes them."
  function_name    = "PruneDevices"
  role             = aws_iam_role.PruneDevicesRole.arn
  handler          = "PruneDevices.handler"
  runtime          = "nodejs24.x"
  depends_on       = [aws_iam_role_policy_attachment.PruneDevicesPolicy]
  filename         = data.archive_file.PruneDevices.output_path
  source_code_hash = data.archive_file.PruneDevices.output_base64sha256

  tracing_config {
    mode = "Active"
  }
  timeout = 300

  environment {
    variables = {
      DynamoDBTableName = aws_dynamodb_table.MediaDownloader.name
      ApnsSigningKey    = data.sops_file.secrets.data["apns.staging.signingKey"]
      ApnsTeam          = data.sops_file.secrets.data["apns.staging.team"]
      ApnsKeyId         = data.sops_file.secrets.data["apns.staging.keyId"]
      ApnsDefaultTopic  = data.sops_file.secrets.data["apns.staging.defaultTopic"]
    }
  }
}
</file>

<file path="test/integration/workflows/listFiles.workflow.integration.test.ts">
/**
 * ListFiles Workflow Integration Tests
 *
 * Tests the file listing workflow against LocalStack:
 * 1. Extract userId and userStatus from event (custom authorizer)
 * 2. Handle different user statuses (Authenticated, Anonymous, Unauthenticated)
 * 3. Query DynamoDB UserFiles table for user's file IDs
 * 4. BatchGet files from Files table
 * 5. Filter to only Downloaded files
 * 6. Return file list to client
 *
 * This tests YOUR orchestration logic, not AWS SDK behavior.
 */
// Test configuration
‚ãÆ----
// Set environment variables for Lambda
‚ãÆ----
// DefaultFile env vars required by constants.ts (loaded when importing ListFiles handler)
‚ãÆ----
import {afterAll, beforeAll, beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {Context} from 'aws-lambda'
import {FileStatus, UserStatus} from '../../../src/types/enums'
import type {File} from '../../../src/types/domain-models'
// Test helpers
import {createFilesTable, deleteFilesTable} from '../helpers/dynamodb-helpers'
import {createMockContext} from '../helpers/lambda-context'
import {createElectroDBEntityMock} from '../../helpers/electrodb-mock'
import {createMockFile, createMockUserFile} from '../helpers/test-data'
import {fileURLToPath} from 'url'
import {dirname, resolve} from 'path'
import type {CustomAPIGatewayRequestAuthorizerEvent} from '../../../src/types/infrastructure-types'
‚ãÆ----
function createListFilesEvent(userId: string | undefined, userStatus: UserStatus): CustomAPIGatewayRequestAuthorizerEvent
‚ãÆ----
// Create LocalStack infrastructure
‚ãÆ----
// Wait for tables to be ready
‚ãÆ----
// Create mock context
‚ãÆ----
// Clean up LocalStack infrastructure
‚ãÆ----
// Clear all mocks before each test
‚ãÆ----
// Arrange: Mock ElectroDB responses
// UserFiles.query.byUser returns array of individual UserFile records
‚ãÆ----
// Files.get now uses BATCH get - returns array of files with unprocessed
// Note: ListFiles sorts by publishDate descending (most recent first)
// So video-1 needs a later publishDate to appear first in results
‚ãÆ----
// Arrange: Mock ElectroDB responses with mixed file statuses
‚ãÆ----
// Files.get now uses BATCH get - returns array of all files
‚ãÆ----
// Arrange: Mock ElectroDB with 50 files
‚ãÆ----
// Files.get now uses BATCH get - returns array of all 50 files at once
</file>

<file path="eslint.config.mjs">
// Load local ESLint rules plugin (CommonJS module)
‚ãÆ----
// Code quality rules (dprint doesn't handle these)
‚ãÆ----
// Documentation
‚ãÆ----
// NOTE: Formatting rules (quotes, semi, comma-dangle, max-len) removed.
// dprint handles all formatting via dprint.json
‚ãÆ----
// Project-specific rules (mirrors MCP validation for in-editor feedback)
// Phase 1: CRITICAL
‚ãÆ----
// Phase 2: HIGH
</file>

<file path=".github/workflows/unit-tests.yml">
name: Unit Tests
on:
  push:
    branches:
      - '**'
  pull_request:
    branches:
      - master
jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    - name: Setup pnpm
      uses: pnpm/action-setup@v4
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version-file: '.nvmrc'
        cache: 'pnpm'
    - name: Set up Homebrew
      id: set-up-homebrew
      uses: Homebrew/actions/setup-homebrew@master
    - name: Install hcl2json
      id: brew-install-hcl2json
      run: brew install hcl2json
    - name: Install dependencies
      run: pnpm install --frozen-lockfile
    - name: Compile TypeSpec
      run: pnpm run typespec:check
    - name: Setup directories
      id: setup
      run: mkdir build
    - name: Build dependencies
      id: build-dependencies
      run: pnpm run build-dependencies
    - name: Webpack Build
      id: build
      run: pnpm run build
    - name: Validate GraphRAG synchronization
      run: pnpm run validate:graphrag
    - name: Check types
      run: pnpm run check-types
    - name: Lint code
      run: pnpm run lint
    - name: Check formatting
      run: pnpm run format:check
    - name: Test ESLint local rules
      run: pnpm run test:eslint-rules
    - name: Validate documented scripts exist
      run: ./bin/validate-docs.sh
    - name: Validate documentation sync
      run: pnpm run validate:doc-sync
    - name: Validate configuration enforcement
      run: pnpm run validate:config
    - name: Run unit tests
      run: pnpm test --coverage --coverageDirectory=coverage/unit
      env:
        CI: true
    - name: Upload unit test coverage to Codecov
      uses: codecov/codecov-action@v4
      with:
        files: ./coverage/unit/lcov.info
        flags: unit
        name: unit-tests
        token: ${{ secrets.CODECOV_TOKEN }}
        fail_ci_if_error: true
</file>

<file path="src/lambdas/LoginUser/test/index.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import {testContext} from '#util/jest-setup'
import type {CustomAPIGatewayRequestAuthorizerEvent} from '#types/infrastructure-types'
import {createBetterAuthMock} from '#test/helpers/better-auth-mock'
import {v4 as uuidv4} from 'uuid'
‚ãÆ----
// Mock Better Auth API
‚ãÆ----
// Mock Better Auth sign-in response
‚ãÆ----
createdAt: new Date(Date.now() - 86400000).toISOString() // Created yesterday
‚ãÆ----
// Verify Better Auth API was called with correct parameters (using idToken only)
‚ãÆ----
// Mock Better Auth throwing an error for non-existent user
‚ãÆ----
// Mock Better Auth throwing an error for invalid ID token
‚ãÆ----
// Better Auth API should not be called if request validation fails
</file>

<file path="terraform/send_push_notification.tf">
resource "aws_iam_role" "SendPushNotificationRole" {
  name               = "SendPushNotificationRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaAssumeRole.json
}

resource "aws_iam_role_policy_attachment" "SendPushNotificationPolicyLogging" {
  role       = aws_iam_role.SendPushNotificationRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "SendPushNotificationPolicyXRay" {
  role       = aws_iam_role.SendPushNotificationRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

data "aws_iam_policy_document" "SendPushNotification" {
  statement {
    actions = [
      "sqs:ReceiveMessage",
      "sqs:DeleteMessage",
      "sqs:GetQueueAttributes"
    ]
    resources = [
      aws_sqs_queue.SendPushNotification.arn,
      aws_sqs_queue.SendPushNotificationDLQ.arn
    ]
  }
  # Query UserCollection to get user's devices
  # GetItem on base table to retrieve device details
  statement {
    actions = [
      "dynamodb:Query",
      "dynamodb:GetItem"
    ]
    resources = [
      aws_dynamodb_table.MediaDownloader.arn,
      "${aws_dynamodb_table.MediaDownloader.arn}/index/UserCollection"
    ]
  }
  dynamic "statement" {
    for_each = length(aws_sns_platform_application.OfflineMediaDownloader) == 1 ? [1] : []
    content {
      actions   = ["sns:Publish"]
      resources = [aws_sns_platform_application.OfflineMediaDownloader[0].arn]
    }
  }
}

resource "aws_iam_policy" "SendPushNotificationRolePolicy" {
  name   = "SendPushNotificationRolePolicy"
  policy = data.aws_iam_policy_document.SendPushNotification.json
}

resource "aws_iam_role_policy_attachment" "SendPushNotificationRolePolicy" {
  role       = aws_iam_role.SendPushNotificationRole.name
  policy_arn = aws_iam_policy.SendPushNotificationRolePolicy.arn
}

resource "aws_lambda_permission" "SendPushNotification" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.SendPushNotification.function_name
  principal     = "apigateway.amazonaws.com"
}

resource "aws_cloudwatch_log_group" "SendPushNotification" {
  name              = "/aws/lambda/${aws_lambda_function.SendPushNotification.function_name}"
  retention_in_days = 14
}

data "archive_file" "SendPushNotification" {
  type        = "zip"
  source_file = "./../build/lambdas/SendPushNotification.js"
  output_path = "./../build/lambdas/SendPushNotification.zip"
}

resource "aws_lambda_function" "SendPushNotification" {
  description      = "Records an event from a client environment (e.g. App or Web)."
  function_name    = "SendPushNotification"
  role             = aws_iam_role.SendPushNotificationRole.arn
  handler          = "SendPushNotification.handler"
  runtime          = "nodejs24.x"
  depends_on       = [aws_iam_role_policy_attachment.SendPushNotificationPolicyLogging]
  filename         = data.archive_file.SendPushNotification.output_path
  source_code_hash = data.archive_file.SendPushNotification.output_base64sha256

  tracing_config {
    mode = "Active"
  }
  environment {
    variables = {
      DynamoDBTableName = aws_dynamodb_table.MediaDownloader.name
    }
  }
}

# Dead Letter Queue for failed push notifications
resource "aws_sqs_queue" "SendPushNotificationDLQ" {
  name                      = "SendPushNotification-DLQ"
  message_retention_seconds = 1209600 # 14 days for investigation
  tags = {
    Environment = "production"
    Purpose     = "Dead letter queue for failed push notifications"
  }
}

resource "aws_sqs_queue" "SendPushNotification" {
  name                       = "SendPushNotification"
  delay_seconds              = 0
  max_message_size           = 262144
  message_retention_seconds  = 345600
  receive_wait_time_seconds  = 0
  visibility_timeout_seconds = 60 # 6x Lambda timeout

  redrive_policy = jsonencode({
    deadLetterTargetArn = aws_sqs_queue.SendPushNotificationDLQ.arn
    maxReceiveCount     = 3
  })

  tags = {
    Environment = "production"
  }
}

resource "aws_lambda_event_source_mapping" "SendPushNotification" {
  event_source_arn        = aws_sqs_queue.SendPushNotification.arn
  function_name           = aws_lambda_function.SendPushNotification.arn
  function_response_types = ["ReportBatchItemFailures"]
}
</file>

<file path="src/lambdas/ApiGatewayAuthorizer/src/index.ts">
import type {APIGatewayRequestAuthorizerEvent, CustomAuthorizerResult} from 'aws-lambda'
import {withPowertools, wrapAuthorizer} from '#util/lambda-helpers'
import {logDebug, logError, logInfo} from '#util/logging'
import type {ApiKey, UsagePlan} from '#lib/vendor/AWS/ApiGateway'
import {getApiKeys, getUsage, getUsagePlans} from '#lib/vendor/AWS/ApiGateway'
import {providerFailureErrorMessage, UnexpectedError} from '#util/errors'
import {validateSessionToken} from '#util/better-auth-helpers'
import {getOptionalEnv, getRequiredEnv} from '#util/env-validation'
const generatePolicy = (principalId: string, effect: string, resource: string, usageIdentifierKey?: string) =>
export function generateAllow(principalId: string, resource: string, usageIdentifierKey?: string): CustomAuthorizerResult
export function generateDeny(principalId: string, resource: string, usageIdentifierKey?: string): CustomAuthorizerResult
/**
 * Returns an array of ApiKeys for API Gateway
 * @notExported
 */
async function fetchApiKeys(): Promise<ApiKey[]>
/**
 * Returns an array of UsagePlans for a given APIKey
 * @notExported
 */
async function fetchUsagePlans(keyId: string): Promise<UsagePlan[]>
/**
 * Returns an array, by day, of Usage for a given APIKey and UsagePlan
 * @notExported
 */
async function fetchUsageData(keyId: string, usagePlanId: string)
async function getUserIdFromAuthenticationHeader(authorizationHeader: string): Promise<string | undefined>
‚ãÆ----
// Match Bearer token format (session tokens or JWTs during migration)
‚ãÆ----
// Abandon the request, without valid Bearer token, to produce an authorization error (403)
‚ãÆ----
/**
 * If the request is coming from my IP, use a test userId
 * @param event - A APIGatewayRequestAuthorizerEvent
 * @notExported
 */
function isRemoteTestRequest(event: APIGatewayRequestAuthorizerEvent): boolean
/**
 * A custom Lambda Authorizer that handles the Authentication header
 * There are (3) possible outcomes from this method:
 * - Returns a policy with Effect: Allow ... forwards the request
 * - Returns a policy with Effect: Deny ... translated into 403
 * - Returns new Error('Unauthorized') ... translated into 401
 * - Returns callback(Error) ... translated into 500
 * @notExported
 */
‚ãÆ----
// If it's not a multi-authentication path, it needs the Authorization header
</file>

<file path="src/lambdas/RegisterUser/test/index.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {APIGatewayEvent} from 'aws-lambda'
import {testContext} from '#util/jest-setup'
import {createElectroDBEntityMock} from '#test/helpers/electrodb-mock'
import {createBetterAuthMock} from '#test/helpers/better-auth-mock'
import {v4 as uuidv4} from 'uuid'
// Mock Better Auth API
‚ãÆ----
// Mock Users entity for name updates
‚ãÆ----
// Mock Better Auth creating a new user
‚ãÆ----
createdAt: new Date().toISOString() // Just created
‚ãÆ----
// Mock the user name update
‚ãÆ----
// Verify Better Auth API was called with correct parameters (using idToken, not accessToken)
‚ãÆ----
// Verify name was updated for new user
‚ãÆ----
// Mock Better Auth finding and logging in existing user
‚ãÆ----
createdAt: new Date(Date.now() - 7 * 86400000).toISOString() // Created a week ago
‚ãÆ----
// Verify name was NOT updated for existing user
‚ãÆ----
// Better Auth API should not be called if request validation fails
‚ãÆ----
// Mock Better Auth throwing an error
</file>

<file path="src/lambdas/S3ObjectCreated/test/index.test.ts">
import {beforeAll, describe, expect, jest, test} from '@jest/globals'
import type {S3Event} from 'aws-lambda'
import {testContext} from '#util/jest-setup'
import {createElectroDBEntityMock} from '../../../../test/helpers/electrodb-mock'
‚ãÆ----
sendMessage: jest.fn(), // fmt: multiline
‚ãÆ----
// With batch processing, errors are caught and logged rather than thrown
// This allows remaining records to be processed even if one fails
</file>

<file path="src/lambdas/SendPushNotification/test/index.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import type {SQSEvent} from 'aws-lambda'
import {testContext} from '#util/jest-setup'
import {v4 as uuidv4} from 'uuid'
import {createElectroDBEntityMock} from '#test/helpers/electrodb-mock'
</file>

<file path="src/util/constraints.ts">
import {z} from 'zod'
// YouTube URL regex pattern
‚ãÆ----
// Type exports inferred from schemas
export type FeedlyEventInput = z.infer<typeof feedlyEventSchema>
export type RegisterDeviceInput = z.infer<typeof registerDeviceSchema>
export type UserSubscribeInput = z.infer<typeof userSubscribeSchema>
export type RegisterUserInput = z.infer<typeof registerUserSchema>
export type LoginUserInput = z.infer<typeof loginUserSchema>
// Helper function to validate data against a schema
export function validateSchema<T>(schema: z.ZodSchema<T>, data: unknown):
</file>

<file path="terraform/file_coordinator.tf">
resource "aws_iam_role" "FileCoordinatorRole" {
  name               = "FileCoordinatorRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaAssumeRole.json
}

data "aws_iam_policy_document" "FileCoordinator" {
  statement {
    actions   = ["lambda:InvokeFunction"]
    resources = [aws_lambda_function.StartFileUpload.arn]
  }
  # Query StatusIndex for PendingDownload and Scheduled files
  statement {
    actions = ["dynamodb:Query"]
    resources = [
      "${aws_dynamodb_table.MediaDownloader.arn}/index/StatusIndex"
    ]
  }
  # Publish CloudWatch metrics for monitoring
  statement {
    actions   = ["cloudwatch:PutMetricData"]
    resources = ["*"]
  }
}

resource "aws_iam_policy" "FileCoordinatorRolePolicy" {
  name   = "FileCoordinatorRolePolicy"
  policy = data.aws_iam_policy_document.FileCoordinator.json
}

resource "aws_iam_role_policy_attachment" "FileCoordinatorPolicy" {
  role       = aws_iam_role.FileCoordinatorRole.name
  policy_arn = aws_iam_policy.FileCoordinatorRolePolicy.arn
}

resource "aws_iam_role_policy_attachment" "FileCoordinatorPolicyLogging" {
  role       = aws_iam_role.FileCoordinatorRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "FileCoordinatorPolicyXRay" {
  role       = aws_iam_role.FileCoordinatorRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

resource "aws_cloudwatch_event_target" "FileCoordinator" {
  rule = aws_cloudwatch_event_rule.FileCoordinator.name
  arn  = aws_lambda_function.FileCoordinator.arn
}


resource "aws_cloudwatch_event_rule" "FileCoordinator" {
  name                = "FileCoordinator"
  schedule_expression = "rate(4 minutes)"
  state               = "DISABLED"
}

resource "aws_lambda_permission" "FileCoordinator" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.FileCoordinator.function_name
  principal     = "events.amazonaws.com"
  source_arn    = aws_cloudwatch_event_rule.FileCoordinator.arn
}

resource "aws_cloudwatch_log_group" "FileCoordinator" {
  name              = "/aws/lambda/${aws_lambda_function.FileCoordinator.function_name}"
  retention_in_days = 14
}

data "archive_file" "FileCoordinator" {
  type        = "zip"
  source_file = "./../build/lambdas/FileCoordinator.js"
  output_path = "./../build/lambdas/FileCoordinator.zip"
}

resource "aws_lambda_function" "FileCoordinator" {
  description      = "Checks for files to be downloaded and triggers their execution"
  function_name    = "FileCoordinator"
  role             = aws_iam_role.FileCoordinatorRole.arn
  handler          = "FileCoordinator.handler"
  runtime          = "nodejs24.x"
  memory_size      = 1024
  depends_on       = [aws_iam_role_policy_attachment.FileCoordinatorPolicy]
  filename         = data.archive_file.FileCoordinator.output_path
  source_code_hash = data.archive_file.FileCoordinator.output_base64sha256

  tracing_config {
    mode = "Active"
  }

  environment {
    variables = {
      DynamoDBTableName = aws_dynamodb_table.MediaDownloader.name
    }
  }
}
</file>

<file path="AGENTS.md">
# Project Context for AI Agents

## Convention Capture System

**CRITICAL**: This project captures emergent conventions during development. Read `docs/conventions-tracking.md` at session start.

### Detection Signals:
- üö® **CRITICAL**: "NEVER", "FORBIDDEN", "Zero-tolerance"
- ‚ö†Ô∏è **HIGH**: "MUST", "REQUIRED", "ALWAYS"
- üìã **MEDIUM**: "Prefer X over Y", repeated decisions

### When Convention Detected:
1. Update `docs/conventions-tracking.md` with the new convention
2. Document in appropriate wiki page under `docs/wiki/`
3. Mark as documented in tracking file

### Reference:
- **Active Conventions**: `docs/conventions-tracking.md`
- **Documentation Guide**: `docs/wiki/Meta/Convention-Capture-System.md`

**Philosophy**: Current state documented in wiki. History lives in git/PRs. No duplicate documentation.

---

## Project Overview

AWS Serverless media downloader service built with OpenTofu and TypeScript. Downloads media content (primarily YouTube videos) and integrates with a companion iOS app for offline playback. Created as a cost-effective alternative to YouTube Premium's offline playback feature.

### Architecture
- **Infrastructure**: OpenTofu (IaC)
- **Runtime**: AWS Lambda (Node.js 22.x)
- **Language**: TypeScript
- **Storage**: Amazon S3
- **API**: AWS API Gateway with custom authorizer
- **Notifications**: Apple Push Notification Service (APNS)
- **Database**: DynamoDB with ElectroDB ORM (single-table design)
- **Monitoring**: CloudWatch, X-Ray (optional)

### Project Structure
```
.
‚îú‚îÄ‚îÄ terraform/             # AWS Infrastructure definitions (OpenTofu)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ entities/          # ElectroDB entity definitions (single-table design)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Collections.ts # Service combining entities for JOIN-like queries
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Files.ts       # File entity
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FileDownloads.ts # Download tracking entity
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Users.ts       # User entity
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Devices.ts     # Device entity
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ UserFiles.ts   # User-File relationships
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ UserDevices.ts # User-Device relationships
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Sessions.ts    # Better Auth session entity
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Accounts.ts    # Better Auth OAuth account entity
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ VerificationTokens.ts # Better Auth verification tokens
‚îÇ   ‚îú‚îÄ‚îÄ lambdas/           # Lambda functions (each subdirectory = one Lambda)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [lambda-name]/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ src/index.ts         # Lambda handler
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ test/index.test.ts   # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ lib/vendor/        # 3rd party API wrappers & AWS SDK encapsulation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AWS/           # AWS SDK vendor wrappers (src/lib/vendor/AWS/)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BetterAuth/    # Better Auth configuration & ElectroDB adapter
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ElectroDB/     # ElectroDB configuration & service
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ YouTube.ts     # YouTube/yt-dlp wrapper
‚îÇ   ‚îî‚îÄ‚îÄ mcp/               # Model Context Protocol server & validation
‚îÇ       ‚îú‚îÄ‚îÄ server.ts      # MCP server entry point
‚îÇ       ‚îú‚îÄ‚îÄ handlers/      # Query tools (entities, lambda, infrastructure, etc.)
‚îÇ       ‚îî‚îÄ‚îÄ validation/    # AST-based convention enforcement (13 rules)
‚îú‚îÄ‚îÄ test/helpers/          # Test utilities
‚îÇ   ‚îî‚îÄ‚îÄ electrodb-mock.ts  # ElectroDB mock helper for unit tests
‚îú‚îÄ‚îÄ types/                 # TypeScript type definitions
‚îú‚îÄ‚îÄ util/                  # Shared utility functions
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ wiki/              # All documentation and style guides
‚îÇ   ‚îî‚îÄ‚îÄ conventions-tracking.md  # Project-specific conventions
‚îî‚îÄ‚îÄ build/graph.json       # Code graph (ts-morph) - READ THIS
```

## System Architecture

### Lambda Data Flow

```mermaid
graph TD
    %% External Triggers
    API[API Gateway] --> Authorizer[ApiGatewayAuthorizer]
    Authorizer --> ListFiles[ListFiles Lambda]
    Authorizer --> LoginUser[LoginUser Lambda]
    Authorizer --> RegisterDevice[RegisterDevice Lambda]
    Authorizer --> RegisterUser[RegisterUser Lambda]
    Authorizer --> RefreshToken[RefreshToken Lambda]
    Authorizer --> UserDelete[UserDelete Lambda]
    Authorizer --> UserSubscribe[UserSubscribe Lambda]

    Feedly[Feedly Webhook] --> WebhookFeedly[WebhookFeedly Lambda]

    %% Scheduled Tasks
    Schedule[CloudWatch Schedule] --> FileCoordinator[FileCoordinator Lambda]
    Schedule --> PruneDevices[PruneDevices Lambda]

    %% Lambda Invocations
    FileCoordinator --> StartFileUpload[StartFileUpload Lambda]

    %% S3 Triggers
    S3Upload[S3 Upload Event] --> S3ObjectCreated[S3ObjectCreated Lambda]
    S3ObjectCreated --> SQS[SQS Queue]
    SQS --> SendPushNotification[SendPushNotification Lambda]

    %% Data Stores
    ListFiles --> DDB[(DynamoDB)]
    LoginUser --> DDB
    RegisterDevice --> DDB
    RegisterUser --> DDB
    WebhookFeedly --> DDB
    FileCoordinator --> DDB
    UserDelete --> DDB
    PruneDevices --> DDB
    S3ObjectCreated --> DDB

    StartFileUpload --> S3Storage[(S3 Storage)]
    WebhookFeedly --> S3Storage

    SendPushNotification --> APNS[Apple Push Service]
```

### Entity Relationship Model

```mermaid
erDiagram
    USERS ||--o{ USER_FILES : has
    USERS ||--o{ USER_DEVICES : owns
    USERS ||--o{ SESSIONS : has
    USERS ||--o{ ACCOUNTS : has
    FILES ||--o{ USER_FILES : shared_with
    FILES ||--o{ FILE_DOWNLOADS : tracks
    DEVICES ||--o{ USER_DEVICES : registered_to

    USERS {
        string userId PK
        string email
        string status
        timestamp createdAt
    }

    FILES {
        string fileId PK
        string fileName
        string url
        string status
        number size
        timestamp createdAt
    }

    FILE_DOWNLOADS {
        string downloadId PK
        string fileId FK
        string status
        timestamp startedAt
        timestamp completedAt
    }

    DEVICES {
        string deviceId PK
        string deviceToken
        string platform
        timestamp lastActive
    }

    SESSIONS {
        string sessionId PK
        string userId FK
        string token
        timestamp expiresAt
    }

    ACCOUNTS {
        string accountId PK
        string userId FK
        string provider
        string providerAccountId
    }

    VERIFICATION_TOKENS {
        string token PK
        string identifier
        timestamp expiresAt
    }

    USER_FILES {
        string userId FK
        string fileId FK
        timestamp createdAt
    }

    USER_DEVICES {
        string userId FK
        string deviceId FK
        timestamp createdAt
    }
```

### Service Interaction Map

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        API Gateway                          ‚îÇ
‚îÇ                    (Custom Authorizer)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                                    ‚îÇ
             ‚ñº                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Lambda Functions  ‚îÇ              ‚îÇ   External Services ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§              ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ ListFiles         ‚îÇ              ‚îÇ ‚Ä¢ Feedly API        ‚îÇ
‚îÇ ‚Ä¢ LoginUser         ‚îÇ              ‚îÇ ‚Ä¢ YouTube (yt-dlp)  ‚îÇ
‚îÇ ‚Ä¢ RegisterDevice    ‚îÇ              ‚îÇ ‚Ä¢ APNS              ‚îÇ
‚îÇ ‚Ä¢ StartFileUpload   ‚îÇ              ‚îÇ ‚Ä¢ Sign In w/ Apple  ‚îÇ
‚îÇ ‚Ä¢ WebhookFeedly     ‚îÇ              ‚îÇ ‚Ä¢ GitHub API        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     AWS Services Layer                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ    DynamoDB         ‚îÇ      S3       ‚îÇ    CloudWatch        ‚îÇ
‚îÇ  (ElectroDB ORM)    ‚îÇ  (Media Files)‚îÇ   (Logs/Metrics)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Dependency Analysis with graph.json

The `build/graph.json` file contains comprehensive dependency information. Key queries:

```bash
# Get all transitive dependencies for a Lambda function
cat build/graph.json | jq '.transitiveDependencies["src/lambdas/ListFiles/src/index.ts"]'

# Find all files that import a specific module
cat build/graph.json | jq '.files | to_entries[] | select(.value.imports[]? | contains("entities/Files")) | .key'

# List all Lambda entry points
cat build/graph.json | jq '.files | keys[] | select(contains("src/lambdas") and contains("/src/index.ts"))'

# Get import count for complexity analysis
cat build/graph.json | jq '.files | to_entries | map({file: .key, importCount: (.value.imports | length)}) | sort_by(.importCount) | reverse[:10]'
```

### Keeping MCP & GraphRAG in Sync

The MCP server (`src/mcp/`) and GraphRAG (`graphrag/`) use shared data sources for accuracy:

| Data Source | Purpose | Auto-Updated |
|-------------|---------|--------------|
| `src/lambdas/` | Lambda discovery | ‚úì Filesystem scan |
| `src/entities/` | Entity discovery | ‚úì Filesystem scan |
| `build/graph.json` | Dependencies | ‚úì Generated before build |
| `graphrag/metadata.json` | Semantic info | ‚úó Manual updates required |

**When adding/removing Lambdas or Entities:**
1. The MCP handlers and GraphRAG auto-discover from filesystem
2. Update `graphrag/metadata.json` with trigger types and purposes
3. Run `pnpm run graphrag:extract` to regenerate the knowledge graph
4. CI will fail if `knowledge-graph.json` is out of date

**When changing Lambda invocation chains:**
1. Update `graphrag/metadata.json` `lambdaInvocations` array
2. Run `pnpm run graphrag:extract`

### Lambda Trigger Patterns

| Lambda | Trigger Type | Source | Purpose |
|--------|-------------|--------|---------|
| ApiGatewayAuthorizer | API Gateway | All authenticated routes | Authorize API requests via Better Auth |
| CloudfrontMiddleware | CloudFront | Edge requests | Edge processing for CDN |
| FileCoordinator | CloudWatch Events | Scheduled | Orchestrate pending file downloads |
| ListFiles | API Gateway | GET /files | List user's available files |
| LogClientEvent | API Gateway | POST /events | Log client-side events |
| LoginUser | API Gateway | POST /auth/login | Authenticate user |
| PruneDevices | CloudWatch Events | Daily schedule | Clean inactive devices |
| RefreshToken | API Gateway | POST /auth/refresh | Refresh authentication token |
| RegisterDevice | API Gateway | POST /devices | Register iOS device for push |
| RegisterUser | API Gateway | POST /auth/register | Register new user |
| S3ObjectCreated | S3 Event | s3:ObjectCreated | Handle uploaded files, notify users |
| SendPushNotification | SQS | S3ObjectCreated | Send APNS notifications |
| StartFileUpload | Lambda Invoke | FileCoordinator | Initiate file download from YouTube |
| UserDelete | API Gateway | DELETE /users | Delete user and cascade |
| UserSubscribe | API Gateway | POST /subscriptions | Manage user topic subscriptions |
| WebhookFeedly | API Gateway | POST /webhooks/feedly | Process Feedly articles |

### Data Access Patterns

| Pattern | Entity | Access Method | Index Used |
|---------|--------|--------------|------------|
| User's files | UserFiles ‚Üí Files | Query by userId | GSI1 |
| User's devices | UserDevices ‚Üí Devices | Query by userId | GSI1 |
| File's users | UserFiles | Query by fileId | GSI2 |
| Device lookup | Devices | Get by deviceId | Primary |
| User resources | Collections.userResources | Batch query | GSI1 |

## Critical Project-Specific Rules

1. **Use build/graph.json for dependency analysis**:
   - Auto-generated before every build
   - Shows file-level imports and transitive dependencies
   - **CRITICAL for Jest tests**: Use `transitiveDependencies` to find all mocks needed
   - Example: `cat build/graph.json | jq '.transitiveDependencies["src/lambdas/WebhookFeedly/src/index.ts"]'`
2. **pnpm lifecycle script protection** (security hardening):
   - All lifecycle scripts disabled by default in `.npmrc`
   - Protects against AI-targeted typosquatting and supply chain attacks
   - Scripts blocked during installation - must explicitly allowlist packages
   - If package requires install scripts, audit code first then add to `.npmrc`
3. **Feedly webhook** uses query-based authentication (custom authorizer)
4. **APNS certificates** required for iOS push notifications (p12 format)
5. **YouTube downloads** require cookie authentication due to bot detection
6. **LocalStack integration** for local AWS testing via vendor wrappers
7. **Webpack externals** must be updated when adding AWS SDK packages

## ElectroDB Architecture

**CRITICAL**: This project uses ElectroDB as the DynamoDB ORM for type-safe, maintainable database operations.

### Key ElectroDB Features
- **Single-table design**: All entities in one DynamoDB table with optimized GSIs
- **Type-safe queries**: Full TypeScript type inference for all operations
- **Collections**: JOIN-like queries across entity boundaries (see `src/entities/Collections.ts`)
- **Batch operations**: Efficient bulk reads/writes with automatic chunking

### Entity Relationships
- **Users** ‚Üî **Files**: Many-to-many via UserFiles entity
- **Users** ‚Üî **Devices**: Many-to-many via UserDevices entity
- **Users** ‚Üî **Sessions**: One-to-many (Better Auth sessions)
- **Users** ‚Üî **Accounts**: One-to-many (Better Auth OAuth accounts)
- **Files** ‚Üî **FileDownloads**: One-to-many (download tracking)

### Collections (JOIN-like Queries)
- **Collections.userResources**: Query all files & devices for a user in one call
- **Collections.fileUsers**: Get all users associated with a file (for notifications)
- **Collections.deviceUsers**: Get all users associated with a device (for cleanup)
- **Collections.userSessions**: Get all sessions for a user (Better Auth)
- **Collections.userAccounts**: Get all OAuth accounts for a user (Better Auth)

### Testing with ElectroDB
- **ALWAYS** use `test/helpers/electrodb-mock.ts` for mocking entities
- **NEVER** create manual mocks for ElectroDB entities
- See test style guide for detailed mocking patterns

## Wiki Conventions to Follow

**BEFORE WRITING ANY CODE, READ THE APPLICABLE GUIDE:**

### Core Conventions
- **Git Workflow**: [docs/wiki/Conventions/Git-Workflow.md](docs/wiki/Conventions/Git-Workflow.md) - NO AI attribution in commits
- **Naming**: [docs/wiki/Conventions/Naming-Conventions.md](docs/wiki/Conventions/Naming-Conventions.md) - camelCase, PascalCase rules
- **Comments**: [docs/wiki/Conventions/Code-Comments.md](docs/wiki/Conventions/Code-Comments.md) - Git as source of truth

### TypeScript & Testing
- **Lambda Patterns**: [docs/wiki/TypeScript/Lambda-Function-Patterns.md](docs/wiki/TypeScript/Lambda-Function-Patterns.md)
- **Jest Mocking**: [docs/wiki/Testing/Jest-ESM-Mocking-Strategy.md](docs/wiki/Testing/Jest-ESM-Mocking-Strategy.md)
- **Mock Types**: [docs/wiki/Testing/Mock-Type-Annotations.md](docs/wiki/Testing/Mock-Type-Annotations.md)
- **Coverage Philosophy**: [docs/wiki/Testing/Coverage-Philosophy.md](docs/wiki/Testing/Coverage-Philosophy.md)
- **Integration Testing**: [docs/wiki/Integration/LocalStack-Testing.md](docs/wiki/Integration/LocalStack-Testing.md)

### AWS & Infrastructure
- **SDK Encapsulation**: [docs/wiki/AWS/SDK-Encapsulation-Policy.md](docs/wiki/AWS/SDK-Encapsulation-Policy.md) - ZERO tolerance
- **Bash Scripts**: [docs/wiki/Bash/Script-Patterns.md](docs/wiki/Bash/Script-Patterns.md)
- **OpenTofu/Terraform**: [docs/wiki/Infrastructure/OpenTofu-Patterns.md](docs/wiki/Infrastructure/OpenTofu-Patterns.md)

## Anti-Patterns to Avoid

The following patterns have caused issues in this project and should be avoided:

### 1. Direct AWS SDK Imports (CRITICAL)
**Wrong**: `import {DynamoDBClient} from '@aws-sdk/client-dynamodb'`
**Right**: `import {getDynamoDBClient} from '#lib/vendor/AWS/DynamoDB'`
**Why**: Breaks encapsulation, makes testing difficult, loses type safety benefits

### 2. Manual ElectroDB Entity Mocks (CRITICAL)
**Wrong**: Hand-crafted mock objects for entities in tests
**Right**: `const mock = createElectroDBEntityMock({queryIndexes: ['byUser']})`
**Why**: Inconsistent mocking leads to false positives and maintenance burden

### 3. Promise.all for Cascade Deletions (CRITICAL)
**Wrong**: `await Promise.all([deleteUser(), deleteUserFiles()])`
**Right**: `await Promise.allSettled([deleteUserFiles(), deleteUser()])`
**Why**: Partial failures leave orphaned data; children must be deleted before parents

### 4. Try-Catch for Required Environment Variables (CRITICAL)
**Wrong**: `try { config = JSON.parse(process.env.Config) } catch { return fallback }`
**Right**: `const config = getRequiredEnv('Config')` - let it fail fast
**Why**: Silent failures hide configuration errors that should break at cold start

### 5. Underscore-Prefixed Unused Variables (HIGH)
**Wrong**: `handler(event, _context, _callback)` to suppress warnings
**Right**: `handler({body}: APIGatewayProxyEvent)` - destructure only what you need
**Why**: Backwards-compatibility hacks obscure intent and violate project conventions

### 6. AI Attribution in Commits (CRITICAL)
**Wrong**: Commit messages with "Generated with Claude", emojis, "Co-Authored-By: AI"
**Right**: Clean commit messages following commitlint format: `feat: add new feature`
**Why**: Professional commits, code ownership clarity, industry standard

### 7. Module-Level Environment Variable Validation (HIGH)
**Wrong**: `const config = getRequiredEnv('Config')` at top of module
**Right**: Call `getRequiredEnv()` inside functions (lazy evaluation)
**Why**: Module-level calls break tests that need to set up mocks before import

### 8. Raw Response Objects in Lambdas (HIGH)
**Wrong**: `return {statusCode: 200, body: JSON.stringify(data)}`
**Right**: `return response(200, data)`
**Why**: Inconsistent formatting, missing headers, no type safety

## Type Naming Patterns

| Pattern | Usage | Examples |
|---------|-------|----------|
| Simple nouns | Domain entities | `User`, `File`, `Device`, `Session` |
| `*Item` | ElectroDB parsed types | `UserItem`, `FileItem`, `DeviceItem` |
| `*Input` | Request payloads & mutations | `UserLoginInput`, `CreateFileInput` |
| `*Response` | API response wrappers | `FileResponse`, `LoginResponse` |
| `*Error` | Error classes | `AuthorizationError`, `ValidationError` |

### File Organization (`src/types/`)

| File | Contents |
|------|----------|
| `domain-models.d.ts` | User, File, Device, IdentityProvider |
| `request-types.d.ts` | *Input types for API requests |
| `notification-types.d.ts` | Push notification payloads |
| `persistence-types.d.ts` | Relationship types (UserDevice, UserFile) |
| `infrastructure-types.d.ts` | AWS/API Gateway types |
| `enums.ts` | FileStatus, UserStatus, ResponseStatus |

### Enum Values (PascalCase)

```typescript
// FileStatus values (aligned with iOS)
Queued | Downloading | Downloaded | Failed
```

## Development Workflow

### Essential Commands
```bash
pnpm run precheck       # TypeScript type checking and lint (run before commits)
pnpm run build          # Build Lambda functions with esbuild
pnpm run test           # Run unit tests
pnpm run deploy         # Deploy infrastructure with OpenTofu
pnpm run format         # Auto-format with dprint (157 char lines)

# Local CI (run before pushing)
pnpm run ci:local                # Fast CI checks (~2-3 min, no integration)
pnpm run ci:local:full           # Full CI checks (~5-10 min, with integration)

# Integration testing
pnpm run localstack:start        # Start LocalStack
pnpm run test:integration        # Run integration tests (assumes LocalStack running)

# Remote testing
pnpm run test-remote-list        # Test file listing
pnpm run test-remote-hook        # Test Feedly webhook
pnpm run test-remote-registerDevice  # Test device registration

# Documentation
pnpm run document-source         # Generate TSDoc documentation
```

### Pre-Commit Checklist
1. Run `pnpm run precheck` - TypeScript type checking and lint
2. Run `pnpm run format` - Auto-format code
3. Run `pnpm run build` - Compile with esbuild
4. Run `pnpm test` - Ensure all tests pass
5. Verify NO AI references in commit message
6. Stage changes: `git add -A`
7. Commit with clean message: `git commit -m "type: description"`
8. **NEVER push automatically** - Wait for user request

## Integration Points

### External Services
- **Feedly**: Webhook-based article processing (query auth)
- **YouTube**: yt-dlp for video downloads (cookie auth required)
- **APNS**: iOS push notifications (requires certificates)
- **Sign In With Apple**: Authentication for iOS app
- **GitHub API**: Automated issue creation for errors

### AWS Services
- **Lambda**: Event-driven compute (all business logic)
- **S3**: Media storage with transfer acceleration
- **DynamoDB**: Single-table design via ElectroDB ORM for all entities
- **API Gateway**: REST endpoints with custom authorizer
- **SNS**: Push notification delivery
- **CloudWatch**: Logging and metrics
- **X-Ray**: Distributed tracing (optional)

## Common Development Tasks

### Adding New Lambda Function
1. Create `src/lambdas/[name]/` directory structure
2. Implement handler in `src/index.ts` with TypeDoc
3. Write tests in `test/index.test.ts` with fixtures
4. Mock ALL transitive dependencies (see Wiki)
5. Define Lambda resource in OpenTofu
6. Verify esbuild discovers new Lambda entry point
7. Configure appropriate IAM permissions
8. Import utilities from `util/` directory

### Debugging Production Issues
1. Check CloudWatch logs for Lambda
2. Review automated GitHub issues
3. Use AWS X-Ray for tracing (if enabled)
4. Test with production-like data locally
5. Use `test-remote-*` scripts for validation

### Updating API Endpoints
1. Modify API Gateway configuration in OpenTofu
2. Update Lambda handler code
3. Adjust custom authorizer if needed
4. Test with `test-remote-*` scripts
5. Update iOS app if contract changes

## Security & Secrets

- **SOPS**: All secrets managed via SOPS (`secrets.encrypted.yaml`)
- **Environment Variables**: Production secrets via Lambda environment
- **APNS Certificates**: P12 format, separate sandbox/production
- **API Tokens**: Query-based for Feedly compatibility
- **Never commit**: secrets.yaml, certificates, .env files

## Performance Considerations

- Lambda memory allocation: Optimize for cold starts
- S3 transfer acceleration: For large media files
- API Gateway caching: Reduce Lambda invocations
- DynamoDB indexes: Query optimization
- Webpack externals: Reduce bundle size

## Support Resources

- **CI/CD**: GitHub Actions with test pipeline
- **Local Testing**: LocalStack for AWS service emulation
- **Documentation**: TSDoc + terraform-docs
- **Error Tracking**: Automated GitHub issue creation
- **Monitoring**: CloudWatch dashboards and alarms

---

**Remember**: Use TodoWrite tool for complex tasks to track progress and ensure thoroughness.
</file>

<file path="tsconfig.json">
{
  "compilerOptions": {
    "allowSyntheticDefaultImports": true,
    "module": "esnext",
    "target": "ES2024",
    "outDir": "./dist",
    "strict": true,
    "verbatimModuleSyntax": true,
    "types": ["node", "jest"],
    "moduleResolution": "bundler",
    "baseUrl": ".",
    "paths": {
      "#entities/*": ["src/entities/*"],
      "#lib/*": ["src/lib/*"],
      "#util/*": ["src/util/*"],
      "#types/*": ["src/types/*"],
      "#test/*": ["test/*"]
    },
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noImplicitAny": true,
    "noImplicitThis": true,
    "noImplicitReturns": true,
    "noFallthroughCasesInSwitch": true,
    "resolveJsonModule": true,
    "skipLibCheck": true
  },
  "include": ["src/**/*"],
  "exclude": [
    "node_modules",
    "**/*.spec.ts",
    "**/*.fixture.ts",
    "test/**/*",
    "./src/types/terraform.d.ts",
    "src/mcp/test/fixtures/**/*"
  ],
  "ts-node": {
    "compilerOptions": { "module": "esnext" },
    "esm": true
  }
}
</file>

<file path="src/lambdas/FileCoordinator/test/index.test.ts">
import {testContext} from '#util/jest-setup'
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import {DownloadStatus} from '#types/enums'
import {createElectroDBEntityMock} from '#test/helpers/electrodb-mock'
‚ãÆ----
// Mock FileDownloads entity (only entity used by FileCoordinator now)
// FileCoordinator queries:
// - status='pending' for new downloads
// - status='scheduled' for retries
‚ãÆ----
DownloadStatus // Re-export the real enum
‚ãÆ----
// Reset mock to return empty arrays by default
// Note: getPendingFileIds and getScheduledFileIds both use byStatusRetryAfter
// First call = pending, Second call = scheduled
‚ãÆ----
// Mock: first call (pending) returns 1 file, second call (scheduled) returns 0
‚ãÆ----
// Mock: first call (pending) returns 0, second call (scheduled) returns 1
‚ãÆ----
// Mock: first call (pending) returns 1, second call (scheduled) returns 1
‚ãÆ----
// First call (pending) rejects - simulating failure
// Second call (scheduled) succeeds with 1 file
‚ãÆ----
// Handler should complete successfully (not throw)
‚ãÆ----
// Should still process the scheduled download despite pending query failure
‚ãÆ----
// First call (pending) succeeds with 1 file
// Second call (scheduled) rejects - simulating failure
‚ãÆ----
// Handler should complete successfully (not throw)
‚ãÆ----
// Should still process the pending download despite scheduled query failure
‚ãÆ----
// Both queries reject
‚ãÆ----
// Handler should complete successfully (not throw)
‚ãÆ----
// No downloads to process when both queries fail
</file>

<file path="src/lambdas/RegisterDevice/test/index.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import {testContext} from '#util/jest-setup'
import {v4 as uuidv4} from 'uuid'
import type {CustomAPIGatewayRequestAuthorizerEvent} from '#types/infrastructure-types'
import {createElectroDBEntityMock} from '#test/helpers/electrodb-mock'
‚ãÆ----
getUserDevices: getUserDevicesMock, // fmt: multiline
‚ãÆ----
deleteEndpoint: jest.fn().mockReturnValue({ResponseMetadata: {RequestId: uuidv4()}}), // fmt: multiline
‚ãÆ----
// Set up as anonymous user (not unauthenticated) to test APNS config check
‚ãÆ----
// Set up as anonymous user to test validation
‚ãÆ----
// Set up as anonymous user to test AWS failure
</file>

<file path="src/util/apigateway-helpers.ts">
import type {z} from 'zod'
import type {CustomAPIGatewayRequestAuthorizerEvent} from '#types/infrastructure-types'
import {ValidationError} from './errors'
import {logDebug, logError} from './logging'
import type {APIGatewayEvent} from 'aws-lambda'
import {validateSchema} from './constraints'
type PayloadEvent = CustomAPIGatewayRequestAuthorizerEvent | APIGatewayEvent
export function validateRequest<T>(requestBody: unknown, schema: z.ZodSchema<T>): void
export function getPayloadFromEvent(event: PayloadEvent): unknown
</file>

<file path="terraform/register_device.tf">
resource "aws_iam_role" "RegisterDeviceRole" {
  name               = "RegisterDeviceRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaGatewayAssumeRole.json
}

data "aws_iam_policy_document" "RegisterDevice" {
  statement {
    actions = [
      "sns:ListSubscriptionsByTopic",
      "sns:CreatePlatformEndpoint",
      "sns:Subscribe",
      "sns:Unsubscribe"
    ]
    resources = compact([
      aws_sns_topic.PushNotifications.arn,
      length(aws_sns_platform_application.OfflineMediaDownloader) == 1 ? aws_sns_platform_application.OfflineMediaDownloader[0].arn : ""
    ])
  }
  # Query UserCollection to check existing devices
  # PutItem on base table to create UserDevice and Device records
  statement {
    actions = [
      "dynamodb:Query",
      "dynamodb:PutItem",
      "dynamodb:UpdateItem"
    ]
    resources = [
      aws_dynamodb_table.MediaDownloader.arn,
      "${aws_dynamodb_table.MediaDownloader.arn}/index/UserCollection"
    ]
  }
}

resource "aws_iam_policy" "RegisterDeviceRolePolicy" {
  name   = "RegisterDeviceRolePolicy"
  policy = data.aws_iam_policy_document.RegisterDevice.json
}

resource "aws_iam_role_policy_attachment" "RegisterDevicePolicy" {
  role       = aws_iam_role.RegisterDeviceRole.name
  policy_arn = aws_iam_policy.RegisterDeviceRolePolicy.arn
}

resource "aws_iam_role_policy_attachment" "RegisterDevicePolicyLogging" {
  role       = aws_iam_role.RegisterDeviceRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "RegisterDevicePolicyXRay" {
  role       = aws_iam_role.RegisterDeviceRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

resource "aws_lambda_permission" "RegisterDevice" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.RegisterDevice.function_name
  principal     = "apigateway.amazonaws.com"
}

resource "aws_cloudwatch_log_group" "RegisterDevice" {
  name              = "/aws/lambda/${aws_lambda_function.RegisterDevice.function_name}"
  retention_in_days = 14
}

data "archive_file" "RegisterDevice" {
  type        = "zip"
  source_file = "./../build/lambdas/RegisterDevice.js"
  output_path = "./../build/lambdas/RegisterDevice.zip"
}

resource "aws_lambda_function" "RegisterDevice" {
  description      = "Registers an iOS device"
  function_name    = "RegisterDevice"
  role             = aws_iam_role.RegisterDeviceRole.arn
  handler          = "RegisterDevice.handler"
  runtime          = "nodejs24.x"
  depends_on       = [aws_iam_role_policy_attachment.RegisterDevicePolicy]
  filename         = data.archive_file.RegisterDevice.output_path
  source_code_hash = data.archive_file.RegisterDevice.output_base64sha256

  tracing_config {
    mode = "Active"
  }

  environment {
    variables = {
      PlatformApplicationArn   = length(aws_sns_platform_application.OfflineMediaDownloader) == 1 ? aws_sns_platform_application.OfflineMediaDownloader[0].arn : ""
      PushNotificationTopicArn = aws_sns_topic.PushNotifications.arn
      DynamoDBTableName        = aws_dynamodb_table.MediaDownloader.name
    }
  }
}

resource "aws_api_gateway_resource" "RegisterDevice" {
  rest_api_id = aws_api_gateway_rest_api.Main.id
  parent_id   = aws_api_gateway_rest_api.Main.root_resource_id
  path_part   = "registerDevice"
}

resource "aws_api_gateway_method" "RegisterDevicePost" {
  rest_api_id      = aws_api_gateway_rest_api.Main.id
  resource_id      = aws_api_gateway_resource.RegisterDevice.id
  http_method      = "POST"
  authorization    = "CUSTOM"
  authorizer_id    = aws_api_gateway_authorizer.ApiGatewayAuthorizer.id
  api_key_required = true
}

resource "aws_api_gateway_integration" "RegisterDevicePost" {
  rest_api_id             = aws_api_gateway_rest_api.Main.id
  resource_id             = aws_api_gateway_resource.RegisterDevice.id
  http_method             = aws_api_gateway_method.RegisterDevicePost.http_method
  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = aws_lambda_function.RegisterDevice.invoke_arn
}

resource "aws_sns_topic" "PushNotifications" {
  name = "PushNotifications"
}

resource "aws_sns_platform_application" "OfflineMediaDownloader" {
  count                     = 1 # APNS certificate valid until 2027-01-03
  name                      = "OfflineMediaDownloader"
  platform                  = "APNS_SANDBOX"
  platform_credential       = data.sops_file.secrets.data["apns.staging.privateKey"]  # APNS PRIVATE KEY
  platform_principal        = data.sops_file.secrets.data["apns.staging.certificate"] # APNS CERTIFICATE
  success_feedback_role_arn = aws_iam_role.SNSLoggingRole.arn
  failure_feedback_role_arn = aws_iam_role.SNSLoggingRole.arn
}

resource "aws_iam_role" "SNSLoggingRole" {
  name               = "SNSLoggingRole"
  assume_role_policy = data.aws_iam_policy_document.SNSAssumeRole.json
}

resource "aws_iam_role_policy_attachment" "SNSLoggingRolePolicy" {
  role       = aws_iam_role.SNSLoggingRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}
</file>

<file path="secrets.enc.yaml">
signInWithApple:
    config: ENC[AES256_GCM,data:MN/Z/Fy+F3/jbpf4ahc+KuICPIu8u3e9kMDGePUF88CtvApVI0PDJ7ntTOBPfWOPnN5jRc9gIJXcelf1dATsLimv+Or52D2M+PdcYUF3YtdbU3c8ZaDXUDG7A9nfZcaq8DZQHm9LK/W8WyzWmbHgdIrm+2fP6qRQxaI4LFLMNF5VbQ/++A==,iv:/ImMUBa9qhirReu4VPS4pALBhGv0uRDeLjpCXTn+MzE=,tag:/lXH/UiPzOU+cUBoM2SyHw==,type:str]
    authKey: ENC[AES256_GCM,data:CltYXfAk+RiCxI0LP8pVyu2hwDLbhvT7/iZKMSWSntMzh3o1asLg3ZReFiNETjiMF+SCMgNozmVLXMAFuBm/WW4KwP6Hp2fVq4bZ8qAMrvcpYmo5RDLQFN/NfrYOm03Lltbw5OW+EB8282jrePC44JzR2vjZ1aE6pfXfojafpOKXkMSBt/aSpI4Elh8pBT6U8sMTlE6QTLZVZn4NTzNGRMqmT2S8RlnEmzRCjuKi4lEQCvFZ+v/o6LXsQQdSD4XkFRTXVkzHSXdWFy51H+FFSOa7kLBOR+xgPsFByDUozb8B1669/y0E9E88bJByj0uvESKH3h21zD/m2HDUgiftnBKI,iv:DjfiKR5EHaKpdGIgaZ0DSEdEAwNOluIJhoBb0aadGxw=,tag:01YMsXY+3cs/0Z/wazytvg==,type:str]
apns:
    staging:
        team: ENC[AES256_GCM,data:n0cP52d2NIaojQ==,iv:TaWAW4bmoKaq2A102N6M5itS4tWVCra1XiPpqmZtXnE=,tag:nRBTk58RdKNt+kDCa4YXQQ==,type:str]
        keyId: ENC[AES256_GCM,data:wGPuqu5C9T/9uQ==,iv:xx5Pl4Z/nU1pXAj3IQWYCuSyq5mfpOryO3H5qNSAF+4=,tag:l1KABXUNfqZS+nF8MHyO6A==,type:str]
        defaultTopic: ENC[AES256_GCM,data:tHKrwROJToJjvB+dmzvNO/Dw+st+CVAw87q8g05dUPc=,iv:TPNTNhlod5/QWe3F0BZPSavK1kO3KXifYkjCTH41YvA=,tag:gOT4t9/9Q+P15JWD+Q/lAw==,type:str]
        host: ENC[AES256_GCM,data:7Ow/0wcLr/EiDp0zLIMzjtj74CNYpGN0esw=,iv:BoaBxXXrTwEaCb7spAAekwbfuZt5uAW+MAp1v5sbOyA=,tag:dZD/ZJAWeLkafY8tKP9EKA==,type:str]
        signingKey: ENC[AES256_GCM,data:7P0k+8l8P3WmRJ7A8Al/g0oSwVvaYU6sNjxxa2aX5zTMBLNwN7knlGrafbSDTFh/K40cXMJaC+oTFHVekj6tQfdWWat3XCCKVow9CFL9VLnK1JZdvfrUEBA9Ge0NvFYHdeNnfFA5sUu45k1blvCivRbNMwAvzyyVEYJ8lUU1P+Ft4nzax0WUHV6EBo8/No1/4NkbYPAfVr7xE8g9C7x6DhFn14RfqrOk/Ob/WPNgNuOTE8jSVT3HleZNAUVTHvv0JecTBfUZa6JcebHbs0jSklafnIHc7R0N5lzeDuo9QQV3FGJ4R+1MZHUD+ag28hbBRPyGYc5U/uEbanzetiG7HSPh,iv:FzgMDFRkp4NJj84k0bdOIbkWp064hktd8Jmi0+d21Xo=,tag:kIt4KyxBDHnyAyqFk8+s/Q==,type:str]
        privateKey: ENC[AES256_GCM,data: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,iv:WboWQcpn1k2Hiidyq4n8uAI0FZcT/lHdJ25fyAqzPc0=,tag:Qo+/XxSh8+9gcuslBEFEKw==,type:str]
        certificate: ENC[AES256_GCM,data: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,iv:cSL7uUGWh+vGKRYmu9uC66HuLoGb9Zma371O6RHi2Wc=,tag:TPjspGDblpmxn8hPoPAhxQ==,type:str]
github:
    issue:
        token: ENC[AES256_GCM,data:YVRlu7KVpq3dQmjapKKnOKafHE9c7jiqnhdU4wyfa8ZzMowK0uobHkdIV0t/Gh64AnFau4URkJxrgjVZSs3aCiDMYFMsNKu2AAHhEW0Izoh1KHpQ8egogHkG+0vd,iv:X70R83AVXZTutnkFbonc89EEZcg0Ki7wCnEZ+vO0gG0=,tag:JCNf+sko6CavpAF+qVgNyg==,type:str]
platform:
    key: ENC[AES256_GCM,data:wEDNnadshM+weWHVkC66JpIpgLHaUQDV96Kf1OJb2APB7vvL3w76cIpTFBw4nP2GmnQ=,iv:8n3lKBPEBmGTblpJwsi42Yw9/6QuJEjz9ZVML3Pm6vU=,tag:6J8k8xJ0PKVyHhkbRfNJQw==,type:str]
sops:
    age:
        - recipient: age18jlkflydzmmclcds8h5lrn8em6y8kldvuh4wf8zdsvdsyx6uaddsnavfhv
          enc: |
            -----BEGIN AGE ENCRYPTED FILE-----
            YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBTR0pRa0lZbnZFT2orNk1J
            bEJuR09iUWdFNXNzUXdwLzNZZXNveVErclQwCnVoNVRVcStXWHdZTVdsSHh3OVRx
            Wmo1d2dWdUdSd1Z4R3k5ZXVHTW9TM1UKLS0tIGVOYStML1hrd0VSUXdpd0FmSUYr
            WGZvQTdVcmlKaFlZc0l5RWxDb0h4VzAK8uz5967wrxxQZDknN27mAQBK67Ws/XR/
            WwIr455YvVg31BKGh+gCkGcV213xAovhLOemdOKJHdt3QF/oXqVRMw==
            -----END AGE ENCRYPTED FILE-----
    lastmodified: "2025-12-18T21:41:22Z"
    mac: ENC[AES256_GCM,data:okn5HaJISyNFRHHoSrmOPKT99o0Ywgi6MbPz9ps/7Bv94gWSS5AhlR95R0q01ZOOg51w32qlsWM92T7McAO9V+f8OxGtGk1PhPDHLzOpZJucxbYcHWcWtlsb6P60q3zuWUf+MnmxBtr3pDejDvDqXQT2rkr4YeXYS84uOilsU80=,iv:iHJwNhYbSEtFXbb9ZmZ60jLDw+jUGqmoMN3zYtJvZwY=,tag:EBJ8m5JH10TdNMHCrtDssQ==,type:str]
    unencrypted_suffix: _unencrypted
    version: 3.11.0
</file>

<file path="src/lambdas/ListFiles/test/index.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import {testContext} from '#util/jest-setup'
import {v4 as uuidv4} from 'uuid'
import type {CustomAPIGatewayRequestAuthorizerEvent} from '#types/infrastructure-types'
import {createElectroDBEntityMock} from '#test/helpers/electrodb-mock'
// Set DefaultFile env vars BEFORE importing handler (required by constants.ts at module level)
‚ãÆ----
// Default to authenticated user
‚ãÆ----
// Without Authorization header = Anonymous
‚ãÆ----
// With Authorization header but unknown principalId = Unauthenticated
</file>

<file path="terraform/file_bucket.tf">
resource "aws_s3_bucket" "Files" {
  bucket = "lifegames-media-downloader-files"
}

resource "aws_s3_bucket_intelligent_tiering_configuration" "files_tiering" {
  bucket = aws_s3_bucket.Files.id
  name   = "EntireBucket"

  tiering {
    access_tier = "ARCHIVE_ACCESS"
    days        = 90
  }

  tiering {
    access_tier = "DEEP_ARCHIVE_ACCESS"
    days        = 180
  }
}

# Origin Access Control for CloudFront (replaces public-read ACL)
resource "aws_cloudfront_origin_access_control" "media_files_oac" {
  name                              = "media-files-oac"
  description                       = "OAC for media files S3 bucket"
  origin_access_control_origin_type = "s3"
  signing_behavior                  = "always"
  signing_protocol                  = "sigv4"
}

# CloudFront Distribution for S3 media files
resource "aws_cloudfront_distribution" "media_files" {
  enabled             = true
  default_root_object = ""
  price_class         = "PriceClass_100" # US, Canada, Europe - lowest cost

  origin {
    domain_name              = aws_s3_bucket.Files.bucket_regional_domain_name
    origin_id                = "S3-media-files"
    origin_access_control_id = aws_cloudfront_origin_access_control.media_files_oac.id
  }

  default_cache_behavior {
    allowed_methods        = ["GET", "HEAD"]
    cached_methods         = ["GET", "HEAD"]
    target_origin_id       = "S3-media-files"
    viewer_protocol_policy = "redirect-to-https"

    forwarded_values {
      query_string = false
      cookies {
        forward = "none"
      }
    }

    min_ttl     = 0
    default_ttl = 86400    # 1 day
    max_ttl     = 31536000 # 1 year
  }

  restrictions {
    geo_restriction {
      restriction_type = "whitelist"
      locations        = ["US"]
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }

  tags = {
    Name = "MediaFilesDistribution"
  }
}

# S3 Bucket Policy for CloudFront OAC access
resource "aws_s3_bucket_policy" "cloudfront_access" {
  bucket = aws_s3_bucket.Files.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Sid       = "AllowCloudFrontAccess"
      Effect    = "Allow"
      Principal = { Service = "cloudfront.amazonaws.com" }
      Action    = "s3:GetObject"
      Resource  = "${aws_s3_bucket.Files.arn}/*"
      Condition = {
        StringEquals = {
          "AWS:SourceArn" = aws_cloudfront_distribution.media_files.arn
        }
      }
    }]
  })
}

output "cloudfront_media_files_domain" {
  description = "CloudFront domain for media files (use this in iOS app)"
  value       = aws_cloudfront_distribution.media_files.domain_name
}

resource "aws_s3_bucket_notification" "Files" {
  bucket = aws_s3_bucket.Files.bucket
  lambda_function {
    events              = ["s3:ObjectCreated:*"]
    lambda_function_arn = aws_lambda_function.S3ObjectCreated.arn
  }
}

resource "aws_lambda_permission" "S3ObjectCreated" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.S3ObjectCreated.function_name
  principal     = "s3.amazonaws.com"
  source_arn    = aws_s3_bucket.Files.arn
}

resource "aws_cloudwatch_log_group" "S3ObjectCreated" {
  name              = "/aws/lambda/${aws_lambda_function.S3ObjectCreated.function_name}"
  retention_in_days = 14
}

data "archive_file" "S3ObjectCreated" {
  type        = "zip"
  source_file = "./../build/lambdas/S3ObjectCreated.js"
  output_path = "./../build/lambdas/S3ObjectCreated.zip"
}

resource "aws_lambda_function" "S3ObjectCreated" {
  description      = "Dispatches a notification after a file is uploaded to an S3 bucket"
  function_name    = "S3ObjectCreated"
  role             = aws_iam_role.S3ObjectCreatedRole.arn
  handler          = "S3ObjectCreated.handler"
  runtime          = "nodejs24.x"
  depends_on       = [aws_iam_role_policy_attachment.S3ObjectCreatedPolicy]
  filename         = data.archive_file.S3ObjectCreated.output_path
  source_code_hash = data.archive_file.S3ObjectCreated.output_base64sha256

  tracing_config {
    mode = "Active"
  }

  environment {
    variables = {
      DynamoDBTableName = aws_dynamodb_table.MediaDownloader.name
      SNSQueueUrl       = aws_sqs_queue.SendPushNotification.id
    }
  }
}

resource "aws_iam_role" "S3ObjectCreatedRole" {
  name               = "S3ObjectCreatedRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaAssumeRole.json
}

data "aws_iam_policy_document" "S3ObjectCreated" {
  # Query base table and indexes to find files and users
  statement {
    actions = ["dynamodb:Query"]
    resources = [
      aws_dynamodb_table.MediaDownloader.arn,
      "${aws_dynamodb_table.MediaDownloader.arn}/index/*"
    ]
  }
  statement {
    actions   = ["sqs:SendMessage"]
    resources = [aws_sqs_queue.SendPushNotification.arn]
  }
}

resource "aws_iam_policy" "S3ObjectCreatedRolePolicy" {
  name   = "S3ObjectCreatedRolePolicy"
  policy = data.aws_iam_policy_document.S3ObjectCreated.json
}

resource "aws_iam_role_policy_attachment" "S3ObjectCreatedPolicy" {
  role       = aws_iam_role.S3ObjectCreatedRole.name
  policy_arn = aws_iam_policy.S3ObjectCreatedRolePolicy.arn
}

resource "aws_iam_role_policy_attachment" "S3ObjectCreatedPolicyLogging" {
  role       = aws_iam_role.S3ObjectCreatedRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "S3ObjectCreatedPolicyXRay" {
  role       = aws_iam_role.S3ObjectCreatedRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}
</file>

<file path="terraform/list_files.tf">
resource "aws_iam_role" "ListFilesRole" {
  name               = "ListFilesRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaGatewayAssumeRole.json
}

data "aws_iam_policy_document" "ListFiles" {
  # Query UserCollection to get user's file associations
  # Query and BatchGet base table to retrieve file details
  statement {
    actions = [
      "dynamodb:Query",
      "dynamodb:BatchGetItem",
      "dynamodb:GetItem"
    ]
    resources = [
      aws_dynamodb_table.MediaDownloader.arn,
      "${aws_dynamodb_table.MediaDownloader.arn}/index/UserCollection"
    ]
  }
}

resource "aws_iam_policy" "ListFilesRolePolicy" {
  name   = "ListFilesRolePolicy"
  policy = data.aws_iam_policy_document.ListFiles.json
}

resource "aws_iam_role_policy_attachment" "ListFilesPolicy" {
  role       = aws_iam_role.ListFilesRole.name
  policy_arn = aws_iam_policy.ListFilesRolePolicy.arn
}

resource "aws_iam_role_policy_attachment" "ListFilesPolicyLogging" {
  role       = aws_iam_role.ListFilesRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "ListFilesPolicyXRay" {
  role       = aws_iam_role.ListFilesRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

resource "aws_lambda_permission" "ListFiles" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.ListFiles.function_name
  principal     = "apigateway.amazonaws.com"
}

resource "aws_cloudwatch_log_group" "ListFiles" {
  name              = "/aws/lambda/${aws_lambda_function.ListFiles.function_name}"
  retention_in_days = 14
}

# Create a payload zip file from the function source code bundle
data "archive_file" "ListFiles" {
  type        = "zip"
  source_file = "./../build/lambdas/ListFiles.js"
  output_path = "./../build/lambdas/ListFiles.zip"
}

resource "aws_lambda_function" "ListFiles" {
  description      = "A lambda function that lists files in S3."
  function_name    = "ListFiles"
  role             = aws_iam_role.ListFilesRole.arn
  handler          = "ListFiles.handler"
  runtime          = "nodejs24.x"
  memory_size      = 512
  depends_on       = [aws_iam_role_policy_attachment.ListFilesPolicy]
  filename         = data.archive_file.ListFiles.output_path
  source_code_hash = data.archive_file.ListFiles.output_base64sha256

  tracing_config {
    mode = "Active"
  }

  environment {
    variables = {
      DynamoDBTableName      = aws_dynamodb_table.MediaDownloader.name
      DefaultFileSize        = 436743
      DefaultFileName        = aws_s3_object.DefaultFile.key
      DefaultFileUrl         = "https://${aws_s3_object.DefaultFile.bucket}.s3.amazonaws.com/${aws_s3_object.DefaultFile.key}"
      DefaultFileContentType = aws_s3_object.DefaultFile.content_type
    }
  }
}

resource "aws_api_gateway_resource" "Files" {
  rest_api_id = aws_api_gateway_rest_api.Main.id
  parent_id   = aws_api_gateway_rest_api.Main.root_resource_id
  path_part   = "files"
}

resource "aws_api_gateway_method" "ListFilesGet" {
  rest_api_id      = aws_api_gateway_rest_api.Main.id
  resource_id      = aws_api_gateway_resource.Files.id
  http_method      = "GET"
  authorization    = "CUSTOM"
  authorizer_id    = aws_api_gateway_authorizer.ApiGatewayAuthorizer.id
  api_key_required = true
}

resource "aws_api_gateway_integration" "ListFilesGet" {
  rest_api_id             = aws_api_gateway_rest_api.Main.id
  resource_id             = aws_api_gateway_resource.Files.id
  http_method             = aws_api_gateway_method.ListFilesGet.http_method
  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = aws_lambda_function.ListFiles.invoke_arn
}

data "local_file" "DefaultFile" {
  filename = "${path.module}/../static/videos/default-file.mp4"
}

resource "aws_s3_object" "DefaultFile" {
  bucket       = aws_s3_bucket.Files.id
  content_type = "video/mp4"
  key          = "default-file.mp4"
  source       = data.local_file.DefaultFile.filename
  etag         = filemd5(data.local_file.DefaultFile.filename)
  acl          = "public-read"
}
</file>

<file path="terraform/login_user.tf">
resource "aws_iam_role" "LoginUserRole" {
  name               = "LoginUserRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaGatewayAssumeRole.json
}

data "aws_iam_policy_document" "LoginUser" {
  # Better Auth adapter needs full CRUD on base table for user/session/account/verification
  statement {
    actions = [
      "dynamodb:GetItem",
      "dynamodb:PutItem",
      "dynamodb:UpdateItem",
      "dynamodb:DeleteItem",
      "dynamodb:Query",
      "dynamodb:Scan"
    ]
    resources = [
      aws_dynamodb_table.MediaDownloader.arn,
      "${aws_dynamodb_table.MediaDownloader.arn}/index/*"
    ]
  }
}

resource "aws_iam_policy" "LoginUserRolePolicy" {
  name   = "LoginUserRolePolicy"
  policy = data.aws_iam_policy_document.LoginUser.json
}

resource "aws_iam_role_policy_attachment" "LoginUserPolicy" {
  role       = aws_iam_role.LoginUserRole.name
  policy_arn = aws_iam_policy.LoginUserRolePolicy.arn
}

resource "aws_iam_role_policy_attachment" "LoginUserPolicyLogging" {
  role       = aws_iam_role.LoginUserRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "LoginUserPolicyXRay" {
  role       = aws_iam_role.LoginUserRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

resource "aws_lambda_permission" "LoginUser" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.LoginUser.function_name
  principal     = "apigateway.amazonaws.com"
}

resource "aws_cloudwatch_log_group" "LoginUser" {
  name              = "/aws/lambda/${aws_lambda_function.LoginUser.function_name}"
  retention_in_days = 14
}

data "archive_file" "LoginUser" {
  type        = "zip"
  source_file = "./../build/lambdas/LoginUser.js"
  output_path = "./../build/lambdas/LoginUser.zip"
}

resource "aws_lambda_function" "LoginUser" {
  description      = "A lambda function that lists files in S3."
  function_name    = "LoginUser"
  role             = aws_iam_role.LoginUserRole.arn
  handler          = "LoginUser.handler"
  runtime          = "nodejs24.x"
  timeout          = 30
  depends_on       = [aws_iam_role_policy_attachment.LoginUserPolicy]
  filename         = data.archive_file.LoginUser.output_path
  source_code_hash = data.archive_file.LoginUser.output_base64sha256

  tracing_config {
    mode = "Active"
  }

  environment {
    variables = {
      ApplicationUrl        = "https://${aws_api_gateway_rest_api.Main.id}.execute-api.${data.aws_region.current.id}.amazonaws.com/prod"
      DynamoDBTableName     = aws_dynamodb_table.MediaDownloader.name
      SignInWithAppleConfig = data.sops_file.secrets.data["signInWithApple.config"]
      BetterAuthSecret      = data.sops_file.secrets.data["platform.key"]
    }
  }
}

resource "aws_api_gateway_resource" "Login" {
  rest_api_id = aws_api_gateway_rest_api.Main.id
  parent_id   = aws_api_gateway_rest_api.Main.root_resource_id
  path_part   = "login"
}

resource "aws_api_gateway_method" "LoginUserPost" {
  rest_api_id      = aws_api_gateway_rest_api.Main.id
  resource_id      = aws_api_gateway_resource.Login.id
  http_method      = "POST"
  authorization    = "NONE"
  api_key_required = true
}

resource "aws_api_gateway_integration" "LoginUserPost" {
  rest_api_id             = aws_api_gateway_rest_api.Main.id
  resource_id             = aws_api_gateway_resource.Login.id
  http_method             = aws_api_gateway_method.LoginUserPost.http_method
  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = aws_lambda_function.LoginUser.invoke_arn
}
</file>

<file path="docs/conventions-tracking.md">
# Conventions Tracking

This document tracks all conventions, patterns, rules, and methodologies detected during development work. It serves as a central registry ensuring no institutional knowledge is lost to conversation history.

## üìä Enforcement Summary

### Automated Enforcement Methods

| Method | Count | Conventions |
|--------|-------|-------------|
| **MCP Rules** | 15 | aws-sdk-encapsulation, electrodb-mocking, config-enforcement, env-validation, cascade-safety, response-helpers, types-location, batch-retry, scan-pagination, import-order, response-enum, mock-formatting, doc-sync, naming-conventions, authenticated-handler-enforcement |
| **Git Hooks** | 2 | AI attribution (commit-msg), direct master push (pre-push) |
| **ESLint** | 3 | naming conventions, import order, unused vars |
| **CI Workflows** | 2 | script validation, type checking |
| **Build-Time** | 1 | pnpm lifecycle script protection (.npmrc) |
| **Manual Review** | ~10 | code comments, test methodology, architectural patterns |

### MCP Validation Rules by Severity

| Rule | Alias | Severity | What It Checks |
|------|-------|----------|----------------|
| aws-sdk-encapsulation | aws-sdk | CRITICAL | Direct AWS SDK imports |
| electrodb-mocking | electrodb | CRITICAL | Manual entity mocks in tests |
| config-enforcement | config | CRITICAL | ESLint/TSConfig drift |
| env-validation | env | CRITICAL | Raw process.env access |
| cascade-safety | cascade | CRITICAL | Promise.all with deletes |
| response-helpers | response | HIGH | Raw response objects |
| types-location | types | HIGH | Types outside src/types/ |
| batch-retry | batch | HIGH | Unprotected batch ops |
| scan-pagination | scan | HIGH | Unpaginated scans |
| import-order | imports | MEDIUM | Import grouping order |
| response-enum | enum | MEDIUM | Magic status strings |
| mock-formatting | mock | MEDIUM | Chained mock returns |
| doc-sync | docs | HIGH | Documentation drift detection |
| naming-conventions | naming | HIGH | Type and enum naming patterns |
| authenticated-handler-enforcement | auth | HIGH | Manual auth checks in handlers |

---

## üü° Pending Documentation

_No pending conventions - all conventions are documented._

### Detected: 2025-12-20

1. **Branch-First PR Workflow** (Rule)
   - **What**: All feature work must follow strict flow: Create Branch -> Commit -> Push -> Create PR -> Wait for Review.
   - **Why**: Prevents direct commits to main, ensures code review, and maintains a clean history.
   - **Rule**: NEVER commit directly to main. ALWAYS wait for user approval on PRs.
   - **Target**: docs/wiki/Conventions/Git-Workflow.md
   - **Priority**: CRITICAL
   - **Status**: ‚úÖ Documented
   - **Enforcement**: Zero-tolerance (Agent must self-correct).

### Detected: 2025-12-19

1. **Centralized Auth Handler Wrappers** (Security Pattern)
   - **What**: Use `wrapAuthenticatedHandler` for endpoints requiring authentication (rejects Unauthenticated + Anonymous) or `wrapOptionalAuthHandler` for endpoints allowing anonymous access (rejects only Unauthenticated)
   - **Why**: Eliminates boilerplate `getUserDetailsFromEvent()` + `UserStatus` checks; provides type-safe `userId` (guaranteed string in authenticated wrapper); fixes security vulnerabilities from missing auth checks
   - **Detected**: During security audit of Lambda handlers
   - **Target**: docs/wiki/TypeScript/Lambda-Function-Patterns.md
   - **Priority**: HIGH
   - **Status**: ‚úÖ Documented
   - **Enforcement**: MCP `authenticated-handler-enforcement` rule, ESLint `local-rules/authenticated-handler-enforcement`

### Detected: 2025-11-28

1. **Production Fixture Logging** (Testing Pattern)
   - **What**: Use `logIncomingFixture()` / `logOutgoingFixture()` to capture production API requests/responses for test fixture generation
   - **Why**: Transform testing from assumptions to production truth; CloudWatch extraction enables regular fixture updates
   - **Detected**: During fixture automation implementation
   - **Target**: docs/wiki/Testing/Fixture-Extraction.md
   - **Priority**: MEDIUM
   - **Status**: ‚úÖ Documented
   - **Enforcement**: Always enabled (logs to CloudWatch, extract when needed)

2. **ElectroDB Collections Testing** (Testing Pattern)
   - **What**: Test Collections (JOIN-like queries) with LocalStack to validate single-table design
   - **Why**: Ensures GSI queries work correctly across entity boundaries; validates userResources, fileUsers, deviceUsers, userSessions, userAccounts
   - **Detected**: During ElectroDB integration testing implementation
   - **Target**: docs/wiki/Testing/ElectroDB-Testing-Patterns.md
   - **Priority**: HIGH
   - **Status**: ‚úÖ Documented
   - **Enforcement**: Required for Collections changes

### Detected: 2025-11-28 (Script Validation)

7. **Script Documentation Sync** (Rule)
   - **What**: All npm scripts referenced in `AGENTS.md` or `README.md` must exist in `package.json`
   - **Why**: Documentation drift causes confusion and broken developer workflows
   - **Detected**: During comprehensive repository review
   - **Target**: docs/wiki/Infrastructure/Script-Registry.md
   - **Priority**: HIGH
   - **Status**: ‚úÖ Documented
   - **Enforcement**: CI validates on every push (unit-tests.yml)

### Detected: 2025-11-27

3. **No Try-Catch for Required Environment Variables** (Rule)
   - **What**: Never wrap required environment variable access in try-catch blocks with fallback values
   - **Why**: Infrastructure tests enforce that all required environment variables are properly configured; silent failures hide configuration errors
   - **Example**: `const config = JSON.parse(process.env.SignInWithAppleConfig)` NOT `try { const config = ... } catch { return fallback }`
   - **Detected**: During Better Auth configuration cleanup
   - **Documented**: docs/wiki/AWS/Lambda-Environment-Variables.md
   - **Priority**: CRITICAL
   - **Status**: ‚úÖ Documented
   - **Enforcement**: Zero-tolerance (infrastructure tests verify all env vars are present)

### Detected: 2025-11-24

1. **pnpm Lifecycle Script Protection** (Security Rule)
   - **What**: All lifecycle scripts disabled by default in `.npmrc`; packages requiring scripts must be explicitly allowlisted after audit
   - **Why**: Defense against AI-targeted typosquatting and supply chain attacks that exploit LLM-assisted development
   - **Detected**: During npm to pnpm migration for security hardening
   - **Target**: Already documented in docs/wiki/Meta/pnpm-Migration.md
   - **Priority**: CRITICAL
   - **Status**: ‚úÖ Documented
   - **Enforcement**: Zero-tolerance (all scripts blocked by default)

### Detected: 2025-11-22

1. **AGENTS.md Filename Standard** (Convention)
   - **What**: Use `AGENTS.md` (plural) as the filename for AI coding assistant context files
   - **Why**: Industry standard supported by OpenAI Codex, GitHub Copilot, Google Gemini, Cursor, and 20+ AI tools
   - **Detected**: During GitHub Wiki organization planning
   - **Documented**: docs/wiki/Meta/AI-Tool-Context-Files.md
   - **Priority**: HIGH
   - **Status**: ‚úÖ Documented

2. **Passthrough File Pattern** (Pattern)
   - **What**: Tool-specific files (CLAUDE.md, GEMINI.md) contain only a reference to the universal source (AGENTS.md)
   - **Why**: Maintains backwards compatibility while having single source of truth
   - **Detected**: During AI tool compatibility analysis
   - **Documented**: docs/wiki/Meta/AI-Tool-Context-Files.md
   - **Priority**: MEDIUM
   - **Status**: ‚úÖ Documented


4. **Zero AI References in Commits** (Rule)
   - **What**: Absolutely forbidden to include "Generated with Claude Code", "Co-Authored-By: Claude", emojis, or any AI references in commits/PRs
   - **Why**: Professional technical commits only, following commitlint syntax
   - **Detected**: Explicitly stated in CLAUDE.md project instructions
   - **Target**: Already documented in CLAUDE.md
   - **Priority**: CRITICAL
   - **Status**: ‚úÖ Documented
   - **Enforcement**: Zero-tolerance

5. **AWS SDK Encapsulation Policy** (Rule)
   - **What**: NEVER import AWS SDK packages directly in application code; ALL usage must be wrapped in vendor modules (lib/vendor/AWS/)
   - **Why**: Encapsulation, type safety, testability, maintainability, consistency
   - **Detected**: Explicitly stated in CLAUDE.md project instructions
   - **Target**: Already documented in CLAUDE.md
   - **Priority**: CRITICAL
   - **Status**: ‚úÖ Documented
   - **Enforcement**: Zero-tolerance

6. **Comprehensive Jest Mocking Strategy** (Methodology)
   - **What**: When importing ANY function from a module, must mock ALL of that module's transitive dependencies
   - **Why**: ES modules execute all module-level code on import; missing mocks cause obscure test failures
   - **Detected**: Explicitly stated in CLAUDE.md project instructions
   - **Target**: Already documented in CLAUDE.md
   - **Priority**: HIGH
   - **Status**: ‚úÖ Documented

### Detected: 2025-11-25



## ‚úÖ Recently Documented

### Documented: 2025-12-19

1. **Lambda Types Directory Threshold** (Convention)
   - **What**: Create `types/` directory in a lambda only when 3+ types, types are re-exported, or types are complex (5+ properties)
   - **Why**: Reduces directory proliferation while maintaining organization for substantial type collections; small inline types are easier to understand next to their usage
   - **Rule**: Use inline types for 1-2 simple, non-exported types; use types/ directory for 3+ types or re-exported types
   - **Documented**: docs/wiki/TypeScript/Type-Definitions.md (Lambda-Specific Types section)
   - **Priority**: MEDIUM
   - **Status**: ‚úÖ Documented

### Documented: 2025-12-17

1. **Documentation Sync Validation** (Rule)
   - **What**: Automated validation ensures AGENTS.md, wiki, GraphRAG metadata, and MCP rules stay in sync with source code
   - **Why**: Prevents documentation drift that causes confusion for developers and AI assistants
   - **Enforcement**: CI validation via `pnpm run validate:doc-sync` and MCP `doc-sync` rule
   - **Checks**: Entity count, Lambda count, MCP rule count, path existence, stale patterns, GraphRAG metadata, wiki links
   - **Priority**: HIGH
   - **Status**: ‚úÖ Documented
   - **Related Files**: bin/validate-doc-sync.sh, docs/doc-code-mapping.json, src/mcp/validation/rules/doc-sync.ts

### Documented: 2025-12-16

1. **Type Definitions Location** (Rule)
   - **What**: Exported type definitions (type aliases, interfaces, enums) must be in src/types/ directory
   - **Why**: Separation of concerns, discoverability, and maintainability
   - **Exceptions**: Entity-derived types (src/entities/), MCP types (src/mcp/), internal types
   - **Documented**: docs/wiki/TypeScript/Type-Definitions.md
   - **Priority**: HIGH
   - **Enforcement**: MCP `types-location` rule (HIGH severity); CI validates on push

2. **No Underscore-Prefixed Unused Variables** (Rule)
   - **What**: Never use underscore-prefixed variables (`_event`, `_context`, `_metadata`) to suppress unused variable warnings
   - **Why**: Per AGENTS.md: "Avoid backwards-compatibility hacks like renaming unused `_vars`"
   - **Solution**: Use object destructuring in function signatures to extract only needed properties
   - **Documented**: docs/wiki/TypeScript/Lambda-Function-Patterns.md
   - **Priority**: CRITICAL
   - **Enforcement**: MCP `config-enforcement` rule validates ESLint config; CI validates on push

3. **Configuration Drift Detection** (Pattern)
   - **What**: MCP validation rules detect configuration changes that weaken project enforcement
   - **Why**: Configuration files can silently weaken enforcement standards
   - **Detected Patterns**: ESLint underscore ignore patterns, disabled TSConfig strict settings
   - **Documented**: docs/wiki/MCP/Convention-Tools.md
   - **Priority**: HIGH
   - **Enforcement**: MCP `config-enforcement` rule; CI validates on every push

### Documented: 2025-11-29

1. **Multiline Array/Object Formatting Hint** (Convention)
   - **What**: Use `// fmt: multiline` comment after first element to force consistent multiline formatting
   - **Why**: dprint uses "best fit" algorithm that can create ugly mixed inline/multiline arrays; line comments cannot be collapsed to single line
   - **Documented**: docs/wiki/Conventions/Code-Formatting.md
   - **Priority**: MEDIUM
   - **Enforcement**: Optional (use when dprint's default formatting harms readability)

2. **Type Aliases for Line Width Management** (Convention)
   - **What**: Create type aliases for return types or parameter groups when function signatures exceed 157 characters
   - **Why**: Keeps function signatures on single lines for better readability; avoids awkward parameter wrapping
   - **Examples**: `SessionResult`, `RequestPayload`, `MetricInput`
   - **Documented**: docs/wiki/Conventions/Code-Formatting.md
   - **Priority**: MEDIUM
   - **Enforcement**: Optional (use when signatures would otherwise wrap)

3. **Sequential Mock Return Values as Separate Statements** (Convention)
   - **What**: Use separate statements instead of method chaining for `mockResolvedValueOnce` / `mockReturnValueOnce` sequences
   - **Why**: Chained methods exceed line width and wrap mid-chain; separate statements are dprint-stable and more readable
   - **Example**: `mockFn.mockResolvedValueOnce(a)` on line 1, `mockFn.mockResolvedValueOnce(b)` on line 2
   - **Documented**: docs/wiki/Conventions/Code-Formatting.md
   - **Priority**: MEDIUM
   - **Enforcement**: Always for sequences of 2+ mock return values

### Documented: 2025-11-28 (Code Quality Improvements)

1. **ResponseStatus Enum for API Responses** (Convention)
   - **What**: Use `ResponseStatus` enum for all API response status values instead of magic strings
   - **Why**: Type safety, consistency, and easier refactoring
   - **Documented**: src/types/enums.ts
   - **Priority**: MEDIUM
   - **Enforcement**: Prefer enum over string literals

2. **Environment Variable Validation** (Pattern)
   - **What**: Use `getRequiredEnv()` / `getRequiredEnvNumber()` from `util/env-validation.ts` for environment variables
   - **Why**: Fail fast at cold start with clear error messages instead of cryptic runtime failures
   - **Documented**: src/util/env-validation.ts
   - **Priority**: HIGH
   - **Enforcement**: Required for new Lambda functions

3. **Lazy Evaluation for Environment Variables** (Pattern)
   - **What**: Call `getRequiredEnv()` inside functions, not at module level
   - **Why**: Avoids test failures from env validation running at import time before mocks are set up
   - **Exception**: Module-level constants that are directly imported by consumers (e.g., `defaultFile` in constants.ts) should remain module-level to prevent esbuild tree-shaking. For these cases, tests should set env vars BEFORE importing the module rather than mocking env-validation.
   - **Example for functions**: `function getConfig() { return getRequiredEnv('Config') }` (lazy)
   - **Example for constants**: Set `process.env.DefaultFileUrl = 'value'` before import, NOT mock env-validation
   - **Documented**: src/util/constants.ts, src/lib/vendor/YouTube.ts
   - **Priority**: HIGH
   - **Enforcement**: Prefer lazy evaluation; use env vars in tests for module-level constants

4. **Batch Operation Retry Logic** (Pattern)
   - **What**: Use `retryUnprocessed()` / `retryUnprocessedDelete()` from `util/retry.ts` for DynamoDB batch operations
   - **Why**: DynamoDB batch operations may return unprocessed items; retry with exponential backoff prevents data loss
   - **Documented**: src/util/retry.ts
   - **Priority**: HIGH
   - **Enforcement**: Required for batch get/delete operations

5. **Paginated Scan Operations** (Pattern)
   - **What**: Use `scanAllPages()` from `util/pagination.ts` for DynamoDB scan operations
   - **Why**: DynamoDB scans are limited to 1MB per request; pagination prevents silent data truncation
   - **Documented**: src/util/pagination.ts
   - **Priority**: HIGH
   - **Enforcement**: Required for all scan operations

6. **Promise.allSettled for Cascade Operations** (Pattern)
   - **What**: Use `Promise.allSettled()` instead of `Promise.all()` for cascade deletion and multi-resource operations
   - **Why**: Prevents partial state from orphaning data; allows handling individual failures gracefully
   - **Documented**: src/lambdas/UserDelete/src/index.ts, src/lambdas/PruneDevices/src/index.ts
   - **Priority**: HIGH
   - **Enforcement**: Required for cascade operations

7. **Cascade Deletion Order** (Rule)
   - **What**: Delete child entities BEFORE parent entities in cascade operations
   - **Why**: Prevents orphaned references if parent deletion succeeds but child deletion fails
   - **Documented**: src/lambdas/UserDelete/src/index.ts
   - **Priority**: CRITICAL
   - **Enforcement**: Zero-tolerance for incorrect cascade order

### Documented: 2025-11-28

1. **ElectroDB Test Mocking Standard** (Rule)
   - **What**: ALWAYS use the `createElectroDBEntityMock()` helper for mocking ElectroDB entities
   - **Why**: Ensures consistent mocking patterns and proper type safety
   - **Documented**: docs/wiki/Testing/Jest-ESM-Mocking-Strategy.md
   - **Priority**: CRITICAL
   - **Enforcement**: Zero-tolerance

3. **Lambda Response Helper Usage** (Convention)
   - **What**: Always use the `response` function from lambda-helpers for Lambda responses
   - **Why**: Ensures consistent response formatting across all Lambda functions
   - **Documented**: docs/wiki/TypeScript/Lambda-Function-Patterns.md
   - **Priority**: HIGH

4. **GitHub Wiki Sync Automation** (Methodology)
   - **What**: Automated GitHub Actions workflow syncs docs/wiki/ to GitHub Wiki
   - **Why**: Git-tracked source with beautiful web UI, zero manual maintenance
   - **Documented**: docs/wiki/Meta/GitHub-Wiki-Sync.md
   - **Priority**: HIGH

5. **Dependency Graph Analysis** (Methodology)
   - **What**: Use build/graph.json to identify all transitive dependencies for Jest mocking
   - **Why**: ES modules execute all module-level code, requiring comprehensive mocking
   - **Documented**: docs/wiki/Testing/Dependency-Graph-Analysis.md (NEW)
   - **Priority**: HIGH

6. **Lambda Directory Naming** (Convention)
   - **What**: Lambda function directories use PascalCase to match AWS resource naming
   - **Why**: Easy correlation between code and infrastructure
   - **Documented**: docs/wiki/Conventions/Naming-Conventions.md
   - **Priority**: MEDIUM

## üí≠ Proposed Conventions

### Device ID Tracking in Auth Flows

**What**: Login and Registration requests should include deviceId in request payload
**Why**: Better Auth session tracking includes deviceId for device-specific session management
**Current Status**: deviceId is always `undefined` in LoginUser and RegisterUser Lambdas
**Implementation Note**: iOS app needs to be updated to send deviceId in auth requests
**Code References**:
- LoginUser: `src/lambdas/LoginUser/src/index.ts:77`
- RegisterUser: `src/lambdas/RegisterUser/src/index.ts:161`

_Note: This is not blocking functionality but would improve session tracking capabilities._

## üóÑÔ∏è Archived Conventions

_No archived conventions yet - superseded or deprecated conventions will be moved here._

---

## Usage Guidelines

### For AI Assistants

When working on this project:
1. **Start of Session**: Review this file to understand current conventions
2. **During Session**: Flag new conventions immediately when detected
3. **End of Session**: Update this file with newly detected conventions
4. **Before Documenting**: Move convention from "Pending" to "Recently Documented"

### For Developers

When contributing:
1. Review pending conventions to understand emerging patterns
2. Provide feedback on proposed conventions
3. Help document conventions in the wiki
4. Update this file when conventions are officially documented

### Convention Lifecycle

```
Detected ‚Üí Pending Documentation ‚Üí Documented in Wiki ‚Üí Recently Documented ‚Üí (after 30 days) ‚Üí Archived
                                                                                              ‚Üì
                                                                                      Superseded/Deprecated
```

## Metadata

- **Created**: 2025-11-22
- **Last Updated**: 2025-12-17
- **Total Conventions**: 31 detected, 31 documented, 0 pending
- **Convention Capture System**: Active
</file>

<file path="src/lambdas/CloudfrontMiddleware/src/index.ts">
import type {CloudFrontRequestEvent, Context} from 'aws-lambda'
import type {CloudFrontHeaders, CloudFrontRequest} from 'aws-lambda/common/cloudfront'
import {logDebug, logInfo} from '#util/logging'
import type {CloudFrontHandlerResult, CustomCloudFrontRequest} from '../types'
// Note: Lambda@Edge does not support externalized modules (no layers) and has strict size limits
// X-Ray wrapper removed to avoid bundling aws-xray-sdk-core (~1MB) into the deployment package
/**
 * For **every request** to the system:
 * - Extract the API key as a header if sent via querystring (a limitation of API Gateway)
 */
/**
 * Transforms the API key to a header via the querystring (if not already present)
 * @param request - A **reference** to the CloudFrontRequest (modified in place)
 * @notExported
 */
async function handleQueryString(request: CloudFrontRequest)
export const handler = async (event: CloudFrontRequestEvent, context: Context): Promise<CloudFrontHandlerResult> =>
</file>

<file path="src/lambdas/PruneDevices/src/index.ts">
import {Devices} from '#entities/Devices'
import {UserDevices} from '#entities/UserDevices'
import {withPowertools, wrapScheduledHandler} from '#util/lambda-helpers'
import {logDebug, logError, logInfo} from '#util/logging'
import {UnexpectedError} from '#util/errors'
import type {Device} from '#types/domain-models'
import {deleteDevice} from '#util/device-helpers'
import {ApnsClient, Notification, Priority, PushType} from 'apns2'
import {Apns2Error} from '#util/errors'
import {scanAllPages} from '#util/pagination'
import {retryUnprocessedDelete} from '#util/retry'
import {getOptionalEnv, getRequiredEnv} from '#util/env-validation'
import type {ApplePushNotificationResponse, PruneDevicesResult} from '../types'
// Re-export types for external consumers
‚ãÆ----
/**
 * Returns an array of all devices using paginated scan
 * @notExported
 */
async function getDevices(): Promise<Device[]>
async function isDeviceDisabled(token: string): Promise<boolean>
async function dispatchHealthCheckNotificationToDeviceToken(token: string): Promise<ApplePushNotificationResponse>
async function getUserIdsByDeviceId(deviceId: string): Promise<string[]>
/**
 * Removes Devices and related data if the device is no longer active.
 * Activity is determined by directly querying the APNS.
 * - If the device is disabled, remove the platform endpoint and device data
 * - If the device is associated with a user, remove it from UserDevices
 * {@label PRUNE_DEVICES_HANDLER}
 * @param event - An AWS ScheduledEvent; happening daily
 * @param context - An AWS Context object
 * @returns PruneDevicesResult with counts of devices checked, pruned, and any errors
 * @notExported
 */
‚ãÆ----
// Unbelievably, all these methods are idempotent
‚ãÆ----
// TODO: Trigger severe alarm with device details and requestId so it can be manually deleted later
</file>

<file path="src/lambdas/UserDelete/src/index.ts">
import {Users} from '#entities/Users'
import {UserFiles} from '#entities/UserFiles'
import {UserDevices} from '#entities/UserDevices'
import {Devices} from '#entities/Devices'
import {buildApiResponse, withPowertools, wrapAuthenticatedHandler} from '#util/lambda-helpers'
import {logDebug, logError} from '#util/logging'
import {deleteDevice, getUserDevices} from '#util/device-helpers'
import {providerFailureErrorMessage, UnexpectedError} from '#util/errors'
import type {Device} from '#types/domain-models'
import {createFailedUserDeletionIssue} from '#util/github-helpers'
import {retryUnprocessedDelete} from '#util/retry'
import {retryUnprocessed} from '#util/retry'
async function deleteUserFiles(userId: string): Promise<void>
async function deleteUser(userId: string): Promise<void>
async function deleteUserDevices(userId: string): Promise<void>
/**
 * Deletes a User and all associated data.
 * It does NOT delete the files themselves; this happens through a separate process.
 * @param event - An AWS ScheduledEvent; happening daily
 * @param context - An AWS Context object
 */
‚ãÆ----
/* c8 ignore else */
‚ãÆ----
// Delete children FIRST (correct cascade order), then parent LAST
‚ãÆ----
// Check for failures before deleting parent
‚ãÆ----
// Don't delete parent if children failed - prevents orphaned records
‚ãÆ----
// Delete parent LAST - only if all children succeeded
</file>

<file path="src/lambdas/WebhookFeedly/test/index.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import {testContext} from '#util/jest-setup'
import {v4 as uuidv4} from 'uuid'
import type {CustomAPIGatewayRequestAuthorizerEvent} from '#types/infrastructure-types'
import {createElectroDBEntityMock} from '#test/helpers/electrodb-mock'
‚ãÆ----
}), // fmt: multiline
‚ãÆ----
// Mock yt-dlp-wrap to prevent YouTube module from failing
class MockYTDlpWrap
‚ãÆ----
constructor(public binaryPath: string)
‚ãÆ----
// Mock child_process for YouTube spawn operations
‚ãÆ----
// Mock fs for YouTube operations (createReadStream for S3 upload, promises for cookie/cleanup)
‚ãÆ----
// Mock S3 vendor wrapper for YouTube
‚ãÆ----
headObject: jest.fn(), // fmt: multiline
‚ãÆ----
// With Authorization header but unknown principalId = Unauthenticated
‚ãÆ----
// Without Authorization header = Anonymous
</file>

<file path="src/lib/vendor/YouTube.ts">
import YTDlpWrap from 'yt-dlp-wrap'
import {spawn} from 'child_process'
import {createReadStream} from 'fs'
import {copyFile, stat, unlink} from 'fs/promises'
import type {YtDlpVideoInfo} from '#types/youtube'
import {putMetrics} from '#util/lambda-helpers'
import {logDebug, logError} from '#util/logging'
import {CookieExpirationError, UnexpectedError} from '#util/errors'
import {createS3Upload} from '../vendor/AWS/S3'
import {getRequiredEnv} from '#util/env-validation'
/**
 * yt-dlp configuration constants
 */
‚ãÆ----
/** Cookies source path (read-only in Lambda) */
‚ãÆ----
/** Cookies destination path (writable in Lambda) */
‚ãÆ----
/** Common extractor args to work around YouTube restrictions */
‚ãÆ----
/** Format selection: best mp4 video + m4a audio, fallback to best mp4 or best available */
‚ãÆ----
/** Output container format */
‚ãÆ----
/** Number of concurrent fragment downloads for speed */
‚ãÆ----
/**
 * Check if an error message indicates cookie expiration or bot detection
 * @param errorMessage - Error message from yt-dlp
 * @returns true if error is related to cookie expiration
 */
export function isCookieExpirationError(errorMessage: string): boolean
import type {FetchVideoInfoResult} from '#types/video'
/**
 * Safely fetch video metadata using yt-dlp.
 *
 * This function is designed to be "safe" - it never throws, instead returning
 * a result object with success/failure status and optional error details.
 * This enables callers to handle errors gracefully (e.g., scheduling retries).
 *
 * @param uri - YouTube video URL
 * @returns Result object with video info (if successful) or error details
 */
export async function fetchVideoInfo(uri: string): Promise<FetchVideoInfoResult>
‚ãÆ----
// Copy cookies from read-only /opt to writable /tmp (yt-dlp needs write access)
‚ãÆ----
// Configure yt-dlp with flags to work around YouTube restrictions
‚ãÆ----
// Get video info in JSON format
‚ãÆ----
/**
 * Extract video ID from YouTube URL
 * @param url - YouTube video URL
 * @returns Video ID
 */
export function getVideoID(url: string): string
/**
 * Parse yt-dlp progress line and extract useful info.
 * Progress lines look like: "[download]  45.2% of ~151.23MiB at 2.50MiB/s ETA 00:35"
 */
function parseProgressLine(line: string):
‚ãÆ----
// Match download progress line
‚ãÆ----
// Match merger line
‚ãÆ----
return {percent: 100} // Merging means download is complete
‚ãÆ----
/**
 * Execute yt-dlp command and wait for completion.
 * Logs progress periodically during download.
 * @param ytdlpBinaryPath - Path to yt-dlp binary
 * @param args - Command line arguments
 * @returns Promise that resolves on success, rejects with error details on failure
 */
function execYtDlp(ytdlpBinaryPath: string, args: string[]): Promise<void>
‚ãÆ----
let lastLoggedPercent = -10 // Log every 10% progress
‚ãÆ----
const LOG_INTERVAL_MS = 30000 // Also log at least every 30 seconds
‚ãÆ----
// Parse and log progress
‚ãÆ----
// Log if: 10% progress milestone, 30s elapsed, or merging started
‚ãÆ----
// Also capture stdout for any output (yt-dlp mostly uses stderr)
‚ãÆ----
/**
 * Download video to temp file then stream to S3.
 *
 * Two-phase approach:
 * 1. yt-dlp downloads to /tmp with proper video+audio merging (uses ffmpeg internally)
 * 2. Stream completed file to S3
 *
 * This solves the stdout merge bug where yt-dlp concatenates instead of muxing streams.
 *
 * @param uri - YouTube video URL
 * @param bucket - Target S3 bucket name
 * @param key - Target S3 object key (e.g., "dQw4w9WgXcQ.mp4")
 * @returns Upload results including file size, S3 URL, and duration
 */
export async function downloadVideoToS3(uri: string, bucket: string, key: string): Promise<
‚ãÆ----
// Copy cookies from read-only /opt to writable /tmp
‚ãÆ----
// Phase 1: Download to temp file with proper merging (yt-dlp uses ffmpeg internally)
‚ãÆ----
// Phase 2: Stream file to S3
‚ãÆ----
partSize: 10 * 1024 * 1024 // 10MB parts for larger files
‚ãÆ----
// Get file size before cleanup
‚ãÆ----
// Cleanup temp file
‚ãÆ----
// Publish CloudWatch metrics
‚ãÆ----
// Always try to clean up temp file
‚ãÆ----
// File may not exist if download failed early
‚ãÆ----
// Publish failure metric
‚ãÆ----
// Re-throw CookieExpirationError without wrapping
‚ãÆ----
// Check if error message contains cookie expiration indicators
</file>

<file path="terraform/cloudfront_middleware.tf">
resource "aws_iam_role" "CloudfrontMiddlewareRole" {
  name               = "CloudfrontMiddlewareRole"
  assume_role_policy = data.aws_iam_policy_document.LamdbaEdgeAssumeRole.json
}

resource "aws_iam_role_policy_attachment" "CloudfrontMiddlewarePolicyLogging" {
  role       = aws_iam_role.CloudfrontMiddlewareRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "CloudfrontMiddlewarePolicyXRay" {
  role       = aws_iam_role.CloudfrontMiddlewareRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

provider "aws" {
  alias  = "us_east_1"
  region = "us-east-1"
}

data "archive_file" "CloudfrontMiddleware" {
  type        = "zip"
  source_file = "./../build/lambdas/CloudfrontMiddleware.js"
  output_path = "./../build/lambdas/CloudfrontMiddleware.zip"
}

resource "aws_lambda_function" "CloudfrontMiddleware" {
  description      = "A lambda that acts as middleware before hitting the API."
  function_name    = "CloudfrontMiddleware"
  role             = aws_iam_role.CloudfrontMiddlewareRole.arn
  handler          = "CloudfrontMiddleware.handler"
  runtime          = "nodejs24.x"
  publish          = true
  provider         = aws.us_east_1
  filename         = data.archive_file.CloudfrontMiddleware.output_path
  source_code_hash = data.archive_file.CloudfrontMiddleware.output_base64sha256

  tracing_config {
    mode = "Active"
  }
}

resource "aws_cloudfront_distribution" "Production" {
  // This comment needs to match the associated lambda function
  comment = aws_lambda_function.CloudfrontMiddleware.function_name
  origin {
    domain_name = "${aws_api_gateway_rest_api.Main.id}.execute-api.${data.aws_region.current.id}.amazonaws.com"
    origin_path = "/${aws_api_gateway_stage.Production.stage_name}"
    origin_id   = "CloudfrontMiddleware"
    custom_origin_config {
      origin_protocol_policy = "https-only"
      origin_ssl_protocols   = ["TLSv1.2"]
      http_port              = 80
      https_port             = 443
    }
  }
  enabled = true
  default_cache_behavior {
    lambda_function_association {
      event_type = "origin-request"
      lambda_arn = aws_lambda_function.CloudfrontMiddleware.qualified_arn
    }
    viewer_protocol_policy = "https-only"
    allowed_methods        = ["DELETE", "GET", "HEAD", "OPTIONS", "PATCH", "POST", "PUT"]
    cached_methods         = ["GET", "HEAD"]
    target_origin_id       = "CloudfrontMiddleware"
    forwarded_values {
      query_string = true
      headers      = ["X-API-Key", "Authorization", "User-Agent"]
      cookies {
        forward = "none"
      }
    }
    // Intentionally set these values to not cache
    default_ttl = 0
    min_ttl     = 0
    max_ttl     = 0
  }
  restrictions {
    geo_restriction {
      restriction_type = "whitelist"
      locations        = ["US"]
    }
  }
  viewer_certificate {
    cloudfront_default_certificate = true
  }
}

output "cloudfront_distribution_domain" {
  description = "The CloudFront distribution domain. The URL to make requests (e.g. d3q75k9ayjjukw.cloudfront.net)"
  value       = aws_cloudfront_distribution.Production.domain_name
}
</file>

<file path="terraform/register_user.tf">
resource "aws_iam_role" "RegisterUserRole" {
  name               = "RegisterUserRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaGatewayAssumeRole.json
}

data "aws_iam_policy_document" "RegisterUser" {
  # Better Auth adapter needs full CRUD on base table for user/session/account/verification
  statement {
    actions = [
      "dynamodb:GetItem",
      "dynamodb:PutItem",
      "dynamodb:UpdateItem",
      "dynamodb:DeleteItem",
      "dynamodb:Query",
      "dynamodb:Scan"
    ]
    resources = [
      aws_dynamodb_table.MediaDownloader.arn,
      "${aws_dynamodb_table.MediaDownloader.arn}/index/*"
    ]
  }
}

resource "aws_iam_policy" "RegisterUserRolePolicy" {
  name   = "RegisterUserRolePolicy"
  policy = data.aws_iam_policy_document.RegisterUser.json
}

resource "aws_iam_role_policy_attachment" "RegisterUserPolicy" {
  role       = aws_iam_role.RegisterUserRole.name
  policy_arn = aws_iam_policy.RegisterUserRolePolicy.arn
}

resource "aws_iam_role_policy_attachment" "RegisterUserPolicyLogging" {
  role       = aws_iam_role.RegisterUserRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "RegisterUserPolicyXRay" {
  role       = aws_iam_role.RegisterUserRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

resource "aws_lambda_permission" "RegisterUser" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.RegisterUser.function_name
  principal     = "apigateway.amazonaws.com"
}

resource "aws_cloudwatch_log_group" "RegisterUser" {
  name              = "/aws/lambda/${aws_lambda_function.RegisterUser.function_name}"
  retention_in_days = 14
}

data "archive_file" "RegisterUser" {
  type        = "zip"
  source_file = "./../build/lambdas/RegisterUser.js"
  output_path = "./../build/lambdas/RegisterUser.zip"
}

resource "aws_lambda_function" "RegisterUser" {
  description      = "Registers a new user"
  function_name    = "RegisterUser"
  role             = aws_iam_role.RegisterUserRole.arn
  handler          = "RegisterUser.handler"
  runtime          = "nodejs24.x"
  timeout          = 10
  depends_on       = [aws_iam_role_policy_attachment.RegisterUserPolicy]
  filename         = data.archive_file.RegisterUser.output_path
  source_code_hash = data.archive_file.RegisterUser.output_base64sha256

  tracing_config {
    mode = "Active"
  }

  environment {
    variables = {
      ApplicationUrl        = "https://${aws_api_gateway_rest_api.Main.id}.execute-api.${data.aws_region.current.id}.amazonaws.com/prod"
      DynamoDBTableName     = aws_dynamodb_table.MediaDownloader.name
      SignInWithAppleConfig = data.sops_file.secrets.data["signInWithApple.config"]
      BetterAuthSecret      = data.sops_file.secrets.data["platform.key"]
    }
  }
}

resource "aws_api_gateway_resource" "RegisterUser" {
  rest_api_id = aws_api_gateway_rest_api.Main.id
  parent_id   = aws_api_gateway_rest_api.Main.root_resource_id
  path_part   = "registerUser"
}

resource "aws_api_gateway_method" "RegisterUserPost" {
  rest_api_id      = aws_api_gateway_rest_api.Main.id
  resource_id      = aws_api_gateway_resource.RegisterUser.id
  http_method      = "POST"
  authorization    = "NONE"
  api_key_required = true
}

resource "aws_api_gateway_integration" "RegisterUserPost" {
  rest_api_id             = aws_api_gateway_rest_api.Main.id
  resource_id             = aws_api_gateway_resource.RegisterUser.id
  http_method             = aws_api_gateway_method.RegisterUserPost.http_method
  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = aws_lambda_function.RegisterUser.invoke_arn
}
</file>

<file path="terraform/main.tf">
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 6.19.0"
    }
    http = {
      source  = "hashicorp/http"
      version = "3.5.0"
    }
    sops = {
      source  = "carlpett/sops"
      version = "1.2.1"
    }
  }
}

provider "aws" {
  profile = "default"
  region  = "us-west-2"
}

data "aws_region" "current" {}
data "aws_caller_identity" "current" {}

# Read encrypted secrets from YAML
data "sops_file" "secrets" {
  source_file = "../secrets.enc.yaml"
}

data "aws_iam_policy_document" "CommonLambdaLogging" {
  statement {
    actions = [
      "logs:CreateLogGroup",
      "logs:CreateLogStream",
      "logs:PutLogEvents"
    ]
    resources = ["arn:aws:logs:*:*:*"]
  }
}

resource "aws_iam_policy" "CommonLambdaLogging" {
  name        = "CommonLambdaLogging"
  description = "Allows Lambda functions to write to ALL CloudWatch logs"
  policy      = data.aws_iam_policy_document.CommonLambdaLogging.json
}

data "aws_iam_policy_document" "CommonLambdaXRay" {
  statement {
    actions = [
      "xray:PutTraceSegments",
      "xray:PutTelemetryRecords"
    ]
    resources = ["*"]
  }
}

resource "aws_iam_policy" "CommonLambdaXRay" {
  name        = "CommonLambdaXRay"
  description = "Allows Lambda functions to write X-Ray traces"
  policy      = data.aws_iam_policy_document.CommonLambdaXRay.json
}

data "aws_iam_policy_document" "LambdaGatewayAssumeRole" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["apigateway.amazonaws.com", "lambda.amazonaws.com"]
    }
  }
}

data "aws_iam_policy_document" "LamdbaEdgeAssumeRole" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["lambda.amazonaws.com", "edgelambda.amazonaws.com"]
    }
  }
}

data "aws_iam_policy_document" "LambdaAssumeRole" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["lambda.amazonaws.com"]
    }
  }
}

data "aws_iam_policy_document" "StatesAssumeRole" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["states.amazonaws.com"]
    }
  }
}

data "aws_iam_policy_document" "SNSAssumeRole" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["sns.amazonaws.com"]
    }
  }
}

# Single-table DynamoDB design for all entities
# ElectroDB manages entity discrimination via pk/sk composite keys
resource "aws_dynamodb_table" "MediaDownloader" {
  name         = "MediaDownloader"
  billing_mode = "PAY_PER_REQUEST" # On-demand billing - best for low/variable traffic
  hash_key     = "pk"
  range_key    = "sk"

  attribute {
    name = "pk"
    type = "S"
  }

  attribute {
    name = "sk"
    type = "S"
  }

  # UserCollection: Query all resources by userId (files, devices)
  # Access pattern: "Get all files and devices for a user"
  # Used by: ListFiles, UserDelete, RegisterDevice
  attribute {
    name = "gsi1pk"
    type = "S"
  }

  attribute {
    name = "gsi1sk"
    type = "S"
  }

  global_secondary_index {
    name            = "UserCollection"
    hash_key        = "gsi1pk"
    range_key       = "gsi1sk"
    projection_type = "ALL"
  }

  # FileCollection: Query all users by fileId (reverse lookup)
  # Access pattern: "Which users need notification for this file?"
  # Used by: S3ObjectCreated
  attribute {
    name = "gsi2pk"
    type = "S"
  }

  attribute {
    name = "gsi2sk"
    type = "S"
  }

  global_secondary_index {
    name            = "FileCollection"
    hash_key        = "gsi2pk"
    range_key       = "gsi2sk"
    projection_type = "ALL"
  }

  # DeviceCollection: Query all users by deviceId (reverse lookup)
  # Access pattern: "Which users are affected by this device?"
  # Used by: PruneDevices
  attribute {
    name = "gsi3pk"
    type = "S"
  }

  attribute {
    name = "gsi3sk"
    type = "S"
  }

  global_secondary_index {
    name            = "DeviceCollection"
    hash_key        = "gsi3pk"
    range_key       = "gsi3sk"
    projection_type = "ALL"
  }

  # StatusIndex: Query files by status, sorted by availableAt
  # Access pattern: "Find files ready to download"
  # Used by: FileCoordinator
  attribute {
    name = "gsi4pk"
    type = "S"
  }

  attribute {
    name = "gsi4sk"
    type = "N"
  }

  global_secondary_index {
    name            = "StatusIndex"
    hash_key        = "gsi4pk"
    range_key       = "gsi4sk"
    projection_type = "ALL"
  }

  # KeyIndex: Query files by S3 object key
  # Access pattern: "Find file by S3 event key"
  # Used by: S3ObjectCreated
  attribute {
    name = "gsi5pk"
    type = "S"
  }

  global_secondary_index {
    name            = "KeyIndex"
    hash_key        = "gsi5pk"
    projection_type = "ALL"
  }

  # GSI6: Query FileDownloads by status and retryAfter
  # Access pattern: "Find downloads ready for retry"
  # Used by: FileCoordinator
  attribute {
    name = "gsi6pk"
    type = "S"
  }

  attribute {
    name = "gsi6sk"
    type = "N"
  }

  global_secondary_index {
    name            = "GSI6"
    hash_key        = "gsi6pk"
    range_key       = "gsi6sk"
    projection_type = "ALL"
  }

  # AppleDeviceIndex: Query users by Apple device ID
  # Access pattern: "Find user by Apple Sign-In device identifier"
  # Used by: LoginUser, RegisterUser
  attribute {
    name = "gsi7pk"
    type = "S"
  }

  attribute {
    name = "gsi7sk"
    type = "S"
  }

  global_secondary_index {
    name            = "AppleDeviceIndex"
    hash_key        = "gsi7pk"
    range_key       = "gsi7sk"
    projection_type = "ALL"
  }

  # TTL for automatic cleanup of completed/failed FileDownloads
  ttl {
    attribute_name = "ttl"
    enabled        = true
  }

  tags = {
    Name        = "MediaDownloader"
    Description = "Single-table design for all entities"
  }
}

data "http" "icanhazip" {
  url = "https://ipv4.icanhazip.com/"
}

output "public_ip" {
  description = "Your public IP address (used for local development/testing)"
  value       = chomp(data.http.icanhazip.response_body)
}
</file>

<file path="src/lambdas/RegisterUser/src/index.ts">
/**
 * RegisterUser Lambda (Better Auth Version)
 *
 * Registers a new user or logs in existing user via Sign in with Apple using Better Auth OAuth.
 * Fully delegates OAuth verification, user creation, and session creation to Better Auth.
 *
 * Flow:
 * 1. Receive ID token directly from iOS app (Apple SDK provides this)
 * 2. Use Better Auth to verify and sign in/register with ID token
 * 3. Better Auth handles user creation, OAuth account linking, and session creation
 * 4. Update user with first/last name from iOS app (Apple doesn't include name in ID token)
 *
 * Note: Apple's ID token doesn't contain first/last name for privacy reasons.
 * The iOS app sends this separately from ASAuthorizationAppleIDCredential.fullName.
 * This is only populated on first sign-in, so we cache it for new user registration.
 */
import type {APIGatewayEvent, APIGatewayProxyResult} from 'aws-lambda'
import {Users} from '#entities/Users'
import {auth} from '#lib/vendor/BetterAuth/config'
import type {ApiHandlerParams} from '#types/lambda-wrappers'
import {getPayloadFromEvent, validateRequest} from '#util/apigateway-helpers'
import {registerUserSchema} from '#util/constraints'
import {buildApiResponse, withPowertools, wrapApiHandler} from '#util/lambda-helpers'
import {logInfo} from '#util/logging'
interface UserRegistrationInput {
  idToken: string
  email: string
  firstName?: string
  lastName?: string
}
/**
 * Registers a User or logs in existing User via Sign in with Apple using Better Auth.
 *
 * Flow:
 * 1. Receive ID token directly from iOS app
 * 2. Use Better Auth's OAuth sign-in/registration with ID token
 * 3. Better Auth verifies token, creates/finds user, links account, creates session
 * 4. Update user with first/last name if this is a new registration
 * 5. Return session token with expiration
 *
 * Error cases:
 * - 401: Invalid ID token
 * - 500: Other errors
 *
 * @notExported
 */
‚ãÆ----
// 1. Validate request
‚ãÆ----
// 2. Sign in/Register using Better Auth with ID token from iOS app
// Better Auth handles:
// - ID token verification (signature, expiration, issuer using Apple's public JWKS)
// - User lookup by Apple ID (or creation if new)
// - OAuth account linking (Accounts entity)
// - Session creation with device tracking
// - Email verification status from Apple
‚ãÆ----
// No accessToken needed - we only have the ID token from iOS
‚ãÆ----
// Better Auth returns a redirect response for OAuth flows or a token response for ID token flows
// Since we're using ID token authentication, we expect a token response
‚ãÆ----
// Type narrow to token response
‚ãÆ----
// 3. Check if this is a new user and update with name from iOS app
// Apple's ID token doesn't include first/last name for privacy reasons
// The iOS app provides this from ASAuthorizationAppleIDCredential.fullName
// (only populated on first sign-in)
‚ãÆ----
// 4. Return session token (Better Auth format)
</file>

<file path="src/lambdas/StartFileUpload/test/index.test.ts">
import {beforeEach, describe, expect, jest, test} from '@jest/globals'
import {createElectroDBEntityMock} from '#test/helpers/electrodb-mock'
import {DownloadStatus} from '#types/enums'
import type {FetchVideoInfoResult} from '#types/video'
import type {YtDlpVideoInfo} from '#types/youtube'
import {CookieExpirationError, UnexpectedError} from '#util/errors'
import {testContext} from '#util/jest-setup'
interface StartFileUploadParams {
  fileId: string
  correlationId?: string
}
// Mock YouTube functions
‚ãÆ----
// Mock ElectroDB Files entity (for permanent metadata)
‚ãÆ----
// Mock ElectroDB FileDownloads entity (for transient download state)
‚ãÆ----
DownloadStatus // Re-export the real enum
‚ãÆ----
// Mock ElectroDB UserFiles entity (for querying users waiting for a file)
‚ãÆ----
// Mock SQS sendMessage for MetadataNotification dispatch
‚ãÆ----
// Re-export helpers used by transformers.ts
‚ãÆ----
// Helper to create a successful video info result
const createSuccessResult = (info: Partial<YtDlpVideoInfo>): FetchVideoInfoResult => (
// Helper to create a failure result
const createFailureResult = (error: Error, isCookieError = false): FetchVideoInfoResult => (
‚ãÆ----
// Deep clone event to prevent test interference
‚ãÆ----
// Reset mocks
‚ãÆ----
// Set default mock return values
‚ãÆ----
// Mock UserFiles.query.byFile for MetadataNotification dispatch
‚ãÆ----
// Mock SQS sendMessage
‚ãÆ----
// Set environment variables
‚ãÆ----
// Verify Files.upsert was called (for permanent metadata)
‚ãÆ----
// Verify FileDownloads was updated (status changes: in_progress -> completed)
‚ãÆ----
// Verify downloadVideoToS3 was called with correct parameters (no format ID - yt-dlp handles selection)
‚ãÆ----
// Test that large files (e.g., 100MB) are handled correctly
‚ãÆ----
// Verify Files.upsert was called with success
‚ãÆ----
// Transient errors get scheduled for retry (200)
‚ãÆ----
// Verify FileDownloads was updated with scheduled status
‚ãÆ----
// Permanent errors return error status codes
‚ãÆ----
// Error responses have structure: {error: {code, message: <body>}}
‚ãÆ----
// Verify FileDownloads was updated with failed status
‚ãÆ----
// Unknown errors are treated as transient (retryable) by default
‚ãÆ----
// Unknown errors get scheduled for retry
‚ãÆ----
// Verify FileDownloads was updated with scheduled status
‚ãÆ----
// Fetch fails but we have partial video info (e.g., scheduled video)
‚ãÆ----
release_timestamp: Math.floor(Date.now() / 1000) + 3600, // 1 hour from now
‚ãÆ----
// Should be scheduled for retry at release time
‚ãÆ----
// FileDownloads operations fail - but the handler should handle this gracefully
// The actual behavior depends on the implementation - just verify we get some response
‚ãÆ----
// The handler may throw or return an error response - either is acceptable
‚ãÆ----
// Throwing is also acceptable if DynamoDB operations fail completely
‚ãÆ----
// First the handler reads existing download state with maxed retries
‚ãÆ----
// Then the fetch fails (doesn't matter what error, retries are exhausted)
‚ãÆ----
// Max retries exceeded should result in error
‚ãÆ----
// Error responses have structure: {error: {code, message: <body>}}
‚ãÆ----
// CookieExpirationError is recognized by the classifier
‚ãÆ----
// Cookie errors are permanent (require manual intervention)
‚ãÆ----
// Error responses have structure: {error: {code, message: <body>}}
‚ãÆ----
// Mock multiple users waiting for the file
‚ãÆ----
// Verify UserFiles.query.byFile was called
‚ãÆ----
// Verify sendMessage was called 3 times (once per user)
‚ãÆ----
// Verify sendMessage was called with correct parameters
‚ãÆ----
// No users waiting for the file
‚ãÆ----
// Verify sendMessage was NOT called (no users to notify)
</file>

<file path="src/lambdas/UserSubscribe/src/index.ts">
import {getPayloadFromEvent, validateRequest} from '#util/apigateway-helpers'
import {userSubscribeSchema} from '#util/constraints'
import {buildApiResponse, verifyPlatformConfiguration, withPowertools, wrapAuthenticatedHandler} from '#util/lambda-helpers'
import {subscribeEndpointToTopic} from '#util/device-helpers'
interface UserSubscribeInput {
  endpointArn: string
  topicArn: string
}
/**
 * Subscribes an endpoint (a client device) to an SNS topic
 *
 * - Requires authentication (rejects Unauthenticated and Anonymous users)
 * - Requires that the platformApplicationArn environment variable is set
 * - Requires the endpointArn and topicArn are in the payload
 *
 * @notExported
 */
</file>

<file path="terraform/feedly_webhook.tf">
resource "aws_iam_role" "WebhookFeedlyRole" {
  name               = "WebhookFeedlyRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaGatewayAssumeRole.json
}

data "aws_iam_policy_document" "WebhookFeedlyRole" {
  statement {
    actions   = ["sqs:SendMessage"]
    resources = [aws_sqs_queue.SendPushNotification.arn]
  }
  statement {
    actions   = ["lambda:InvokeFunction"]
    resources = [aws_lambda_function.StartFileUpload.arn]
  }
  # PutItem/UpdateItem on base table for Files and UserFiles
  # GetItem to check existing files
  statement {
    actions = [
      "dynamodb:PutItem",
      "dynamodb:UpdateItem",
      "dynamodb:GetItem"
    ]
    resources = [aws_dynamodb_table.MediaDownloader.arn]
  }
  # Powertools Idempotency - read/write to idempotency table
  statement {
    actions = [
      "dynamodb:GetItem",
      "dynamodb:PutItem",
      "dynamodb:UpdateItem",
      "dynamodb:DeleteItem"
    ]
    resources = [aws_dynamodb_table.IdempotencyTable.arn]
  }
}

resource "aws_iam_policy" "WebhookFeedlyRolePolicy" {
  name   = "WebhookFeedlyRolePolicy"
  policy = data.aws_iam_policy_document.WebhookFeedlyRole.json
}

resource "aws_iam_role_policy_attachment" "WebhookFeedlyPolicy" {
  role       = aws_iam_role.WebhookFeedlyRole.name
  policy_arn = aws_iam_policy.WebhookFeedlyRolePolicy.arn
}

resource "aws_iam_role_policy_attachment" "WebhookFeedlyPolicyLogging" {
  role       = aws_iam_role.WebhookFeedlyRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "WebhookFeedlyPolicyXRay" {
  role       = aws_iam_role.WebhookFeedlyRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

resource "aws_lambda_permission" "WebhookFeedly" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.WebhookFeedly.function_name
  principal     = "apigateway.amazonaws.com"
}

resource "aws_cloudwatch_log_group" "WebhookFeedly" {
  name              = "/aws/lambda/${aws_lambda_function.WebhookFeedly.function_name}"
  retention_in_days = 14
}

data "archive_file" "WebhookFeedly" {
  type        = "zip"
  source_file = "./../build/lambdas/WebhookFeedly.js"
  output_path = "./../build/lambdas/WebhookFeedly.zip"
}

resource "aws_lambda_function" "WebhookFeedly" {
  description      = "A webhook from Feedly via IFTTT"
  function_name    = "WebhookFeedly"
  role             = aws_iam_role.WebhookFeedlyRole.arn
  handler          = "WebhookFeedly.handler"
  runtime          = "nodejs24.x"
  memory_size      = 512
  depends_on       = [aws_iam_role_policy_attachment.WebhookFeedlyPolicy]
  filename         = data.archive_file.WebhookFeedly.output_path
  source_code_hash = data.archive_file.WebhookFeedly.output_base64sha256

  tracing_config {
    mode = "Active"
  }

  environment {
    variables = {
      DynamoDBTableName    = aws_dynamodb_table.MediaDownloader.name
      SNSQueueUrl          = aws_sqs_queue.SendPushNotification.id
      IdempotencyTableName = aws_dynamodb_table.IdempotencyTable.name
    }
  }
}

resource "aws_api_gateway_resource" "Feedly" {
  rest_api_id = aws_api_gateway_rest_api.Main.id
  parent_id   = aws_api_gateway_rest_api.Main.root_resource_id
  path_part   = "feedly"
}

resource "aws_api_gateway_method" "WebhookFeedlyPost" {
  rest_api_id      = aws_api_gateway_rest_api.Main.id
  resource_id      = aws_api_gateway_resource.Feedly.id
  http_method      = "POST"
  authorization    = "CUSTOM"
  authorizer_id    = aws_api_gateway_authorizer.ApiGatewayAuthorizer.id
  api_key_required = true
}

resource "aws_api_gateway_integration" "WebhookFeedlyPost" {
  rest_api_id             = aws_api_gateway_rest_api.Main.id
  resource_id             = aws_api_gateway_resource.Feedly.id
  http_method             = aws_api_gateway_method.WebhookFeedlyPost.http_method
  integration_http_method = "POST"
  type                    = "AWS_PROXY"
  uri                     = aws_lambda_function.WebhookFeedly.invoke_arn
}

data "aws_iam_policy_document" "MultipartUpload" {
  # UpdateItem on base table to update File metadata during upload
  # GetItem to retrieve existing file for retry count
  statement {
    actions   = ["dynamodb:UpdateItem", "dynamodb:GetItem"]
    resources = [aws_dynamodb_table.MediaDownloader.arn]
  }
  # Query FileCollection GSI to find users waiting for file (for MetadataNotification)
  statement {
    actions   = ["dynamodb:Query"]
    resources = ["${aws_dynamodb_table.MediaDownloader.arn}/index/FileCollection"]
  }
  # Send MetadataNotification to push notification queue
  statement {
    actions   = ["sqs:SendMessage"]
    resources = [aws_sqs_queue.SendPushNotification.arn]
  }
  statement {
    actions = [
      "s3:PutObject",
      "s3:PutObjectAcl",
      "s3:GetObject",
    ]
    resources = ["${aws_s3_bucket.Files.arn}/*"]
  }
  statement {
    actions = [
      "s3:ListBucket",
      "s3:AbortMultipartUpload",
      "s3:ListMultipartUploadParts",
      "s3:ListBucketMultipartUploads"
    ]
    resources = [aws_s3_bucket.Files.arn]
  }
  statement {
    actions   = ["cloudwatch:PutMetricData"]
    resources = ["*"]
  }
}

resource "aws_iam_role" "MultipartUploadRole" {
  name               = "MultipartUploadRole"
  assume_role_policy = data.aws_iam_policy_document.LambdaAssumeRole.json
}

resource "aws_iam_policy" "MultipartUploadRolePolicy" {
  name   = "MultipartUploadRolePolicy"
  policy = data.aws_iam_policy_document.MultipartUpload.json
}

resource "aws_iam_role_policy_attachment" "MultipartUploadPolicy" {
  role       = aws_iam_role.MultipartUploadRole.name
  policy_arn = aws_iam_policy.MultipartUploadRolePolicy.arn
}

resource "aws_iam_role_policy_attachment" "MultipartUploadPolicyLogging" {
  role       = aws_iam_role.MultipartUploadRole.name
  policy_arn = aws_iam_policy.CommonLambdaLogging.arn
}

resource "aws_iam_role_policy_attachment" "MultipartUploadPolicyXRay" {
  role       = aws_iam_role.MultipartUploadRole.name
  policy_arn = aws_iam_policy.CommonLambdaXRay.arn
}

data "archive_file" "StartFileUpload" {
  type        = "zip"
  source_file = "./../build/lambdas/StartFileUpload.js"
  output_path = "./../build/lambdas/StartFileUpload.zip"
}

resource "null_resource" "DownloadYtDlpBinary" {
  triggers = {
    version = fileexists("${path.module}/../layers/yt-dlp/VERSION") ? trimspace(file("${path.module}/../layers/yt-dlp/VERSION")) : "none"
  }

  provisioner "local-exec" {
    command = <<-EOT
      set -e

      VERSION="${trimspace(file("${path.module}/../layers/yt-dlp/VERSION"))}"
      LAYER_BIN_DIR="${path.module}/../layers/yt-dlp/bin"
      BINARY_NAME="yt-dlp_linux"

      echo "Downloading yt-dlp $${VERSION}..."
      mkdir -p "$${LAYER_BIN_DIR}"

      wget -q "https://github.com/yt-dlp/yt-dlp/releases/download/$${VERSION}/$${BINARY_NAME}" -O "$${LAYER_BIN_DIR}/$${BINARY_NAME}"
      wget -q "https://github.com/yt-dlp/yt-dlp/releases/download/$${VERSION}/SHA2-256SUMS" -O /tmp/yt-dlp-SHA2-256SUMS

      echo "Verifying checksum..."
      cd "$${LAYER_BIN_DIR}"
      if command -v shasum >/dev/null 2>&1; then
        grep "$${BINARY_NAME}$" /tmp/yt-dlp-SHA2-256SUMS | shasum -a 256 -c -s
      elif sha256sum --version 2>&1 | grep -q GNU; then
        grep "$${BINARY_NAME}$" /tmp/yt-dlp-SHA2-256SUMS | sha256sum --check --status
      else
        echo "ERROR: No compatible checksum utility found (shasum or GNU sha256sum)"
        exit 1
      fi

      echo "Making binary executable..."
      chmod +x "$${BINARY_NAME}"

      if [ "$(uname -s)" = "Linux" ]; then
        echo "Testing binary..."
        BINARY_VERSION=$(./"$${BINARY_NAME}" --version)
        if [ "$${BINARY_VERSION}" != "$${VERSION}" ]; then
          echo "ERROR: Binary version mismatch (expected: $${VERSION}, got: $${BINARY_VERSION})"
          exit 1
        fi
        echo "‚úÖ Binary version verified: $${BINARY_VERSION}"
      else
        echo "‚è≠Ô∏è  Skipping binary test (Linux binary, non-Linux host)"
      fi

      rm -f /tmp/yt-dlp-SHA2-256SUMS
      echo "‚úÖ yt-dlp $${VERSION} downloaded and verified successfully"
    EOT
  }
}

resource "null_resource" "DownloadFfmpegBinary" {
  triggers = {
    # Re-download if ffmpeg binary doesn't exist
    ffmpeg_exists = fileexists("${path.module}/../layers/ffmpeg/bin/ffmpeg") ? "exists" : "missing"
  }

  provisioner "local-exec" {
    command = <<-EOT
      set -e

      # ffmpeg now in separate layer directory
      LAYER_BIN_DIR="${path.module}/../layers/ffmpeg/bin"
      FFMPEG_URL="https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-amd64-static.tar.xz"

      if [ -f "$${LAYER_BIN_DIR}/ffmpeg" ]; then
        echo "‚úÖ ffmpeg binary already exists, skipping download"
        exit 0
      fi

      echo "Downloading ffmpeg static build from John Van Sickle..."
      mkdir -p "$${LAYER_BIN_DIR}"
      cd "$${LAYER_BIN_DIR}"

      wget -q "$${FFMPEG_URL}" -O ffmpeg-release-amd64-static.tar.xz
      tar xf ffmpeg-release-amd64-static.tar.xz
      mv ffmpeg-*-amd64-static/ffmpeg .
      rm -rf ffmpeg-*-amd64-static* ffmpeg-release-amd64-static.tar.xz

      chmod +x ffmpeg
      echo "‚úÖ ffmpeg downloaded successfully"
    EOT
  }
}

# yt-dlp layer (binary + cookies only, ~34MB compressed - direct upload)
data "archive_file" "YtDlpLayer" {
  type        = "zip"
  source_dir  = "./../layers/yt-dlp"
  output_path = "./../build/layers/yt-dlp.zip"

  depends_on = [null_resource.DownloadYtDlpBinary]
}

resource "aws_lambda_layer_version" "YtDlp" {
  filename            = data.archive_file.YtDlpLayer.output_path
  layer_name          = "yt-dlp"
  source_code_hash    = data.archive_file.YtDlpLayer.output_base64sha256
  compatible_runtimes = ["nodejs24.x"]

  description = "yt-dlp binary and YouTube cookies for video downloading"
}

# ffmpeg layer (binary only, ~29MB compressed - direct upload)
# Source: John Van Sickle's static builds (https://johnvansickle.com/ffmpeg/)
data "archive_file" "FfmpegLayer" {
  type        = "zip"
  source_dir  = "./../layers/ffmpeg"
  output_path = "./../build/layers/ffmpeg.zip"

  depends_on = [null_resource.DownloadFfmpegBinary]
}

resource "aws_lambda_layer_version" "Ffmpeg" {
  filename            = data.archive_file.FfmpegLayer.output_path
  layer_name          = "ffmpeg"
  source_code_hash    = data.archive_file.FfmpegLayer.output_base64sha256
  compatible_runtimes = ["nodejs24.x"]

  description = "ffmpeg binary (John Van Sickle static build) for video merging"
}

resource "aws_lambda_function" "StartFileUpload" {
  description                    = "Downloads videos to temp file then streams to S3 using yt-dlp"
  function_name                  = "StartFileUpload"
  role                           = aws_iam_role.MultipartUploadRole.arn
  handler                        = "StartFileUpload.handler"
  runtime                        = "nodejs24.x"
  depends_on                     = [aws_iam_role_policy_attachment.MultipartUploadPolicy]
  timeout                        = 900
  memory_size                    = 2048
  reserved_concurrent_executions = 10 # Prevent YouTube rate limiting
  filename                       = data.archive_file.StartFileUpload.output_path
  source_code_hash               = data.archive_file.StartFileUpload.output_base64sha256
  layers = [
    aws_lambda_layer_version.YtDlp.arn,
    aws_lambda_layer_version.Ffmpeg.arn
  ]

  # 10GB ephemeral storage for temp file downloads (handles 1+ hour 1080p videos)
  ephemeral_storage {
    size = 10240
  }

  tracing_config {
    mode = "Active"
  }

  environment {
    variables = {
      Bucket              = aws_s3_bucket.Files.id
      DynamoDBTableName   = aws_dynamodb_table.MediaDownloader.name
      CloudfrontDomain    = aws_cloudfront_distribution.media_files.domain_name
      SNSQueueUrl         = aws_sqs_queue.SendPushNotification.id
      YtdlpBinaryPath     = "/opt/bin/yt-dlp_linux"
      PATH                = "/var/lang/bin:/usr/local/bin:/usr/bin/:/bin:/opt/bin"
      GithubPersonalToken = data.sops_file.secrets.data["github.issue.token"]
    }
  }
}

resource "aws_lambda_permission" "StartFileUpload" {
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.StartFileUpload.function_name
  principal     = "events.amazonaws.com"
}

resource "aws_cloudwatch_log_group" "StartFileUpload" {
  name              = "/aws/lambda/${aws_lambda_function.StartFileUpload.function_name}"
  retention_in_days = 14
}
</file>

<file path=".gitignore">
.DS_Store
.env
.idea
.nyc_output
build
coverage
dist
docs/source
docs/terraform.md

# Dependency cruiser generated files
dependency-graph.svg
dependency-report.html
secure
local
temp
node_modules
secrets.yaml
youtube-cookies.txt
cloudformation.json
src/types/infrastructure.d.ts
terraform/terraform.tfstate*
terraform/.terraform.tfstate.lock.info
terraform/.terraform.lock.hcl
terraform/.terraform
.webpackCache
.python-version
layers/yt-dlp/bin/ffmpeg
layers/ffmpeg/bin/ffmpeg

# pnpm
.pnpm-store

# Fixture extraction (raw fixtures excluded, processed fixtures committed)
test/fixtures/raw

# Claude Code (keep commands, exclude user-specific settings)
.claude/settings.local.json
</file>

<file path="src/lambdas/LoginUser/src/index.ts">
/**
 * LoginUser Lambda (Better Auth Version)
 *
 * Logs in an existing user via Sign in with Apple using Better Auth OAuth.
 * Fully delegates OAuth verification and session creation to Better Auth.
 *
 * Flow:
 * 1. Receive ID token directly from iOS app (Apple SDK provides this)
 * 2. Use Better Auth to verify and sign in with ID token
 * 3. Better Auth handles user lookup, session creation, and account linking
 */
import {getPayloadFromEvent, validateRequest} from '#util/apigateway-helpers'
import {loginUserSchema} from '#util/constraints'
import {buildApiResponse, withPowertools, wrapApiHandler} from '#util/lambda-helpers'
import {logInfo} from '#util/logging'
import {auth} from '#lib/vendor/BetterAuth/config'
interface UserLoginInput {
  idToken: string
}
/**
 * Logs in a User via Sign in with Apple using Better Auth.
 *
 * Flow:
 * 1. Receive ID token directly from iOS app
 * 2. Use Better Auth's OAuth sign-in with ID token
 * 3. Better Auth verifies token, finds user, creates session
 * 4. Return session token with expiration
 *
 * Error cases:
 * - 401: Invalid ID token
 * - 404: User not found (need to register first)
 * - 500: Other errors
 *
 * @notExported
 */
‚ãÆ----
// 1. Validate request body
‚ãÆ----
// 2. Sign in using Better Auth with ID token from iOS app
// Better Auth handles:
// - ID token verification (signature, expiration, issuer using Apple's public JWKS)
// - User lookup by Apple ID
// - Session creation with device tracking
// - Account linking if needed
‚ãÆ----
// No accessToken needed - we only have the ID token from iOS
‚ãÆ----
// Better Auth returns a redirect response for OAuth flows or a token response for ID token flows
// Since we're using ID token authentication, we expect a token response
‚ãÆ----
// Type narrow to token response
‚ãÆ----
// 3. Return session token (Better Auth format)
</file>

<file path="src/lambdas/FileCoordinator/src/index.ts">
import {DownloadStatus, FileDownloads} from '#entities/FileDownloads'
import {putMetrics, withPowertools, wrapScheduledHandler} from '#util/lambda-helpers'
import {logDebug, logError, logInfo} from '#util/logging'
import {providerFailureErrorMessage, UnexpectedError} from '#util/errors'
import {initiateFileDownload} from '#util/lambda-invoke-helpers'
import {getOptionalEnvNumber} from '#util/env-validation'
/** Minimal download info needed for processing */
interface DownloadInfo {
  fileId: string
  correlationId?: string
}
/** Maximum number of files to process concurrently per batch (configurable via FILE_COORDINATOR_BATCH_SIZE) */
‚ãÆ----
/** Delay between batches in milliseconds (configurable via FILE_COORDINATOR_BATCH_DELAY_MS) */
‚ãÆ----
/**
 * Returns download info for FileDownloads with status='pending'.
 * These are new downloads (from WebhookFeedly) that haven't been attempted yet.
 * Uses FileDownloads.byStatusRetryAfter GSI to efficiently query.
 */
async function getPendingDownloads(): Promise<DownloadInfo[]>
‚ãÆ----
// Query FileDownloads with status='pending' - these are new downloads
// Note: pending downloads don't have retryAfter set, so we just query by status
‚ãÆ----
/**
 * Returns download info for FileDownloads scheduled for retry.
 * These are downloads that failed but are retryable (scheduled videos, transient errors).
 * Uses FileDownloads.byStatusRetryAfter GSI to efficiently query.
 */
async function getScheduledDownloads(): Promise<DownloadInfo[]>
‚ãÆ----
// Query FileDownloads with status='scheduled' and retryAfter <= now
‚ãÆ----
/**
 * Process downloads in batches with delays between batches
 * Prevents overwhelming YouTube/yt-dlp with concurrent requests
 */
async function processDownloadsInBatches(downloads: DownloadInfo[]): Promise<void>
‚ãÆ----
// Process batch concurrently, passing correlationId for tracing
// Use Promise.allSettled so one bad file doesn't prevent initiating 4 others
‚ãÆ----
// Add delay between batches (except for the last batch)
‚ãÆ----
/**
 * A scheduled event lambda that checks for files to be downloaded
 * Processes both new pending files and scheduled files ready for retry
 * @param event - An AWS ScheduledEvent; happening every X minutes
 * @param context - An AWS Context object
 */
‚ãÆ----
// Query both pending and scheduled downloads in parallel
// Use Promise.allSettled to process available downloads even if one query fails
‚ãÆ----
// Combine downloads, deduplicate by fileId
‚ãÆ----
// Publish metrics for monitoring
‚ãÆ----
// Process downloads in batches to avoid overwhelming yt-dlp
</file>

<file path="src/lambdas/S3ObjectCreated/src/index.ts">
import type {S3EventRecord} from 'aws-lambda'
import {Files} from '#entities/Files'
import {UserFiles} from '#entities/UserFiles'
import {sendMessage} from '#lib/vendor/AWS/SQS'
import type {SendMessageRequest} from '#lib/vendor/AWS/SQS'
import type {File} from '#types/domain-models'
import type {EventHandlerParams} from '#types/lambda-wrappers'
import {s3Records, withPowertools, wrapEventHandler} from '#util/lambda-helpers'
import {logDebug} from '#util/logging'
import {createDownloadReadyNotification} from '#util/transformers'
import {UnexpectedError} from '#util/errors'
import {getRequiredEnv} from '#util/env-validation'
/**
 * Returns the DynamoDBFile by S3 object key using KeyIndex GSI
 * @param fileName - The S3 object key to search for
 * @notExported
 */
async function getFileByFilename(fileName: string): Promise<File>
/**
 * Returns an array of user IDs who have requested a given file
 * Uses FileCollection GSI for efficient reverse lookup (eliminates full table scan)
 * @param file - The DynamoDBFile you want to search for
 * @notExported
 */
async function getUsersOfFile(file: File): Promise<string[]>
/**
 * Dispatches DownloadReadyNotification to a user via SQS
 * @param file - The DynamoDBFile that is now ready to download
 * @param userId - The UUID of the user
 * @notExported
 */
function dispatchFileNotificationToUser(file: File, userId: string)
/**
 * Process a single S3 record - dispatch notifications to all users of the file
 * @notExported
 */
async function processS3Record(
‚ãÆ----
// Use allSettled to continue processing even if some notifications fail
‚ãÆ----
/**
 * After a File is downloaded, dispatch a notification to all UserDevices
 * @notExported
 */
</file>

<file path="src/util/transformers.ts">
import type {File} from '#types/domain-models'
import type {DownloadReadyNotification, MetadataNotification} from '#types/notification-types'
import type {YtDlpVideoInfo} from '#types/youtube'
import type {PublishInput} from '#lib/vendor/AWS/SNS'
import {stringAttribute} from '#lib/vendor/AWS/SQS'
import type {MessageAttributeValue} from '#lib/vendor/AWS/SQS'
‚ãÆ----
/**
 * Truncates description to MAX_DESCRIPTION_LENGTH to fit within APNS payload limits
 */
export function truncateDescription(description: string): string
/**
 * Creates MetadataNotification - full details, sent after fetchVideoInfo succeeds
 * @param fileId - The video ID
 * @param videoInfo - Video metadata from yt-dlp
 * @param userId - User ID to send notification to
 * @returns SQS message body and attributes for routing
 */
export function createMetadataNotification(
  fileId: string,
  videoInfo: YtDlpVideoInfo,
  userId: string
):
/**
 * Creates DownloadReadyNotification - minimal, sent after S3 upload completes
 * @param dbFile - File record from DynamoDB
 * @param userId - User ID to send notification to
 * @returns SQS message body and attributes for routing
 */
export function createDownloadReadyNotification(
  dbFile: File,
  userId: string
):
/**
 * Transform SQS message body (JSON) to APNS push notification
 * Supports both MetadataNotification and DownloadReadyNotification types
 * @param messageBody - JSON string containing file and notificationType
 * @param targetArn - SNS endpoint ARN for the device
 * @returns SNS PublishInput for APNS
 */
export function transformToAPNSNotification(messageBody: string, targetArn: string): PublishInput
</file>

<file path="src/lambdas/SendPushNotification/src/index.ts">
import type {SQSBatchResponse, SQSEvent, SQSRecord} from 'aws-lambda'
import {UserDevices} from '#entities/UserDevices'
import {Devices} from '#entities/Devices'
import {publishSnsEvent} from '#lib/vendor/AWS/SNS'
import type {PublishInput} from '#lib/vendor/AWS/SNS'
import type {Device} from '#types/domain-models'
import type {FileNotificationType} from '#types/notification-types'
import {withPowertools} from '#util/lambda-helpers'
import {logDebug, logError, logInfo} from '#util/logging'
import {providerFailureErrorMessage, UnexpectedError} from '#util/errors'
import {transformToAPNSNotification} from '#util/transformers'
‚ãÆ----
/**
 * Returns device IDs for a user
 * @param userId - The UUID of the user
 * @notExported
 */
async function getUserDevicesByUserId(userId: string): Promise<string[]>
/**
 * Retrieves a Device from DynamoDB (if it exists)
 * @param deviceId - The unique Device identifier
 * @notExported
 */
async function getDevice(deviceId: string): Promise<Device>
/**
 * Process a single SQS record - send push notifications to all user devices.
 * Throws on failure to enable batch item failure reporting.
 * @notExported
 */
async function processSQSRecord(record: SQSRecord): Promise<void>
/**
 * Dispatches push notifications to all user devices.
 * Supports MetadataNotification and DownloadReadyNotification types.
 *
 * Returns SQSBatchResponse with failed message IDs for partial batch failure handling.
 * Failed messages will be retried by SQS and eventually sent to DLQ after maxReceiveCount.
 *
 * @notExported
 */
</file>

<file path="README.md">
# Media Downloader

[![codecov](https://codecov.io/gh/j0nathan-ll0yd/aws-cloudformation-media-downloader/branch/master/graph/badge.svg)](https://codecov.io/gh/j0nathan-ll0yd/aws-cloudformation-media-downloader)

A media downloader designed to integrate with [it's companion iOS App](https://github.com/j0nathan-ll0yd/ios-OfflineMediaDownloader). It is [serverless](https://aws.amazon.com/serverless/), deployed with [OpenTofu](https://opentofu.org/), and built with [TypeScript](https://www.typescriptlang.org/).

## Architecture

View the [AWS Architecture Diagram](https://gitdiagram.com/repo/j0nathan-ll0yd/aws-cloudformation-media-downloader) (via GitDiagram)

## Technologies

### Core Stack
- **Runtime**: Node.js 22.x (AWS Lambda)
- **Language**: TypeScript with strict type checking
- **Infrastructure as Code**: OpenTofu (Terraform fork)
- **Package Manager**: pnpm (with security hardening)

### Authentication & Authorization
- **Better Auth 1.4.3**: Modern authentication framework with session management
- **First-in-class ElectroDB Adapter**: Custom DynamoDB adapter for Better Auth
- **Apple Sign In**: OAuth provider with ID token flow (eliminates 200-500ms latency)
- **Session-based Auth**: 30-day sessions with automatic refresh
- **Custom Authorizer**: Query-based API tokens for Feedly integration

### Database & ORM
- **DynamoDB**: AWS NoSQL database with single-table design
- **ElectroDB**: Type-safe ORM with optimized GSI queries
- **Entities**: Users, Sessions, Accounts, VerificationTokens, Files, Devices
- **Collections**: JOIN-like queries across entity boundaries

### AWS Services
- **Lambda**: Serverless compute (all business logic)
- **S3**: Media storage with transfer acceleration
- **API Gateway**: REST endpoints with custom authorizer
- **SNS**: Apple Push Notification Service (APNS) delivery
- **CloudWatch**: Logging, metrics, and fixture extraction
- **X-Ray**: Distributed tracing (optional)

### Development & Testing
- **Jest**: Unit testing with ESM support
- **LocalStack**: Local AWS service emulation for integration tests
- **Webpack**: Lambda function bundling with externals optimization
- **TSDoc**: API documentation generation
- **Fixture Logging**: Production request/response capture for test generation

### Media Processing
- **yt-dlp**: YouTube video downloading (auto-updated weekly)
- **FFmpeg**: Video processing and conversion
- **Cookie Authentication**: Bypass YouTube bot detection

### Notable Features
- **First ElectroDB adapter for Better Auth** (potential npm package)
- **Automated fixture extraction from CloudWatch** for test generation
- **pnpm lifecycle script protection** against supply chain attacks
- **Convention capture system** for institutional knowledge preservation

## Background

When [YouTube Premium](https://en.wikipedia.org/wiki/YouTube_Premium) was released they announced "exclusive original content, access to audio-only versions of videos and offline playback on your mobile device." I wasn't interested in the content, but I was excited about offline playback due to poor connectivity when commuting via the [MUNI](https://www.sfmta.com/). _Buuuuuuut_, there was a monthly fee of $11.99.

So, [as an engineer](https://www.linkedin.com/in/lifegames), I used this opportunity to build my own media downloader service, experiment with the latest AWS features, along with a [companion iOS App](https://github.com/j0nathan-ll0yd/ios-OfflineMediaDownloader) using SwiftUI and Combine.

The end result is a generic backend infrastructure that could support any number of features or Apps. This repository is the source code, OpenTofu templates, deployment scripts, documentation and tests that power the App's backend. This includes:

* **Authentication**: Better Auth integration with Apple Sign In (ID token flow) and session management
* **Media Downloads**: Download videos and store them to an S3 bucket
* **API Access**: View downloaded videos via authenticated REST API
* **Push Notifications**: Register for and dispatch push notifications to the iOS App
* **Custom Authorization**: Custom authorizer Lambda supporting query-based API tokens (Feedly integration)
* **Database**: DynamoDB single-table design with ElectroDB ORM for type-safe queries

I share this for any engineer to be able to build a basic backend and iOS App for a future pet project.

## Project Tenants

* The costs per month should be less than $12.
* Minimize external dependencies.
* [Convention over configuration](https://en.wikipedia.org/wiki/Convention_over_configuration). Minimize code, leverage AWS services.

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.

```bash
# Ensure the correct version of NodeJS (via NVM)
nvm use lts/hydrogen

# Install pnpm globally
npm install -g pnpm

# Install dependencies
pnpm install

# Build the AWS Lambda functions (using esbuild)
pnpm run build

# Run the tests to ensure everything is working
pnpm run test

# Use OpenTofu to deploy the infrastructure
cd terraform
tofu init
tofu apply

# Once complete, verify the application works remotely
pnpm run test-remote-list
pnpm run test-remote-hook
```

## Quick Start

```bash
# Install system dependencies and configure
brew install awscli graphviz jq nvm quicktype opentofu terraform-docs
nvm install lts/hydrogen
nvm use lts/hydrogen
npm install -g pnpm
aws configure

# Install Node dependencies and deploy project
pnpm install
pnpm run build-dependencies
pnpm run build
pnpm run test
pnpm run deploy

# Confirm everything is working as expected
pnpm run test-remote-list
```


## Installation

* Install the [Node Version Manager](https://github.com/creationix/nvm). This will allow you to download the specific version of NodeJS supported by AWS Lambda (8.10).

```bash
brew install nvm
nvm install lts/hydrogen
nvm use lts/hydrogen
```

* Install the [Official Amazon AWS command-line interface](https://aws.amazon.com/cli/). [Configure](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) for your AWS account.

```bash
brew install awscli
aws configure
```

* Install [OpenTofu](https://opentofu.org/) (used for deployment scripts)

```bash
brew install opentofu
```

* Install [sops](https://github.com/getsops/sops) (used for secret management)

```bash
brew install sops age

# Generate a local encryption key (AGE format - modern and simple)
mkdir -p ~/.config/sops/age
age-keygen -o ~/.config/sops/age/keys.txt

# Get the public key for SOPS config
PUBLIC_KEY=$(age-keygen -y ~/.config/sops/age/keys.txt)
echo "Your public key: $PUBLIC_KEY"

# Create SOPS config in your project root
cat > .sops.yaml << EOF
creation_rules:
  # YAML and JSON files
  - path_regex: secrets\.yaml
    age: $PUBLIC_KEY
EOF

# Create secrets.yaml template
cat > secrets.yaml << 'EOF'
signInWithApple:
  config: >
    {"client_id":"your.bundle.id","team_id":"YOUR_TEAM_ID","redirect_uri":"","key_id":"YOUR_KEY_ID","scope":"email name"}
  authKey: |
    -----BEGIN PRIVATE KEY-----
    YOUR_SIGN_IN_WITH_APPLE_PRIVATE_KEY_HERE
    -----END PRIVATE KEY-----

apns:
  staging:
    team: YOUR_TEAM_ID
    keyId: YOUR_APNS_KEY_ID
    defaultTopic: your.bundle.id
    host: 'api.sandbox.push.apple.com'
    signingKey: |
      -----BEGIN PRIVATE KEY-----
      YOUR_APNS_SIGNING_KEY_HERE
      -----END PRIVATE KEY-----
    privateKey: |
      -----BEGIN PRIVATE KEY-----
      YOUR_APNS_PRIVATE_KEY_HERE
      -----END PRIVATE KEY-----
    certificate: |
      -----BEGIN CERTIFICATE-----
      YOUR_APNS_CERTIFICATE_HERE
      -----END CERTIFICATE-----

github:
  issue:
    token: YOUR_GITHUB_PERSONAL_ACCESS_TOKEN

platform:
  key: 'YOUR_RANDOM_ENCRYPTION_KEY_HERE'
EOF

echo "Setup complete! Your private key is in ~/.config/sops/age/keys.txt"
echo "Public key added to .sops.yaml"
echo "Created secrets.yaml template - update with your actual values"
echo "Keep your private key secure and share the public key with team members"

# Encrypt secrets (after updating with real values)
# sops --encrypt --output secrets.yaml.encrypted secrets.yaml
```

* Install [quicktype](https://quicktype.io/) (used for generating TypeScript types from OpenTofu)

```bash
brew install quicktype
```

* Install [terraform-docs](https://github.com/terraform-docs/terraform-docs) (used for infrastructure documentation)

```bash
brew install terraform-docs
```

* Install [jq](https://stedolan.github.io/jq/) (used for JSON parsing)

```bash
brew install jq
```

* Install [Graphviz](https://graphviz.org/) (used for dependency graph visualization)

```bash
brew install graphviz
```

* Install [gh](https://cli.github.com/) (for Github usage by Claude Code)

```bash
brew install gh
```

## Migration from Terraform to OpenTofu

This project migrated from Terraform to OpenTofu in [PR #95](https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/pull/95). OpenTofu is a drop-in replacement for Terraform with 100% HCL compatibility.

### Why OpenTofu?

- **Open Source Fork**: Based on Terraform 1.5 with MPL v2 license (no relicensing risk)
- **Community Governance**: Steering committee prevents single-vendor control
- **Enhanced Features**: State encryption, provider iteration, resource exclusion, early variable evaluation
- **Provider Compatibility**: Uses identical provider source code as Terraform with OpenTofu's own registry at registry.opentofu.org

Read more: [Make the Switch to OpenTofu](https://gruntwork.io/blog/make-the-switch-to-opentofu)

### For Existing Deployments

If you have an existing Terraform deployment, migration is straightforward:

```bash
# 1. Install OpenTofu (if not already installed)
brew install opentofu

# 2. No changes needed to .terraform/ directory or state files
# OpenTofu is fully compatible with Terraform state

# 3. Clean and reinitialize to use OpenTofu registry
cd terraform
rm -rf .terraform .terraform.lock.hcl
tofu init

# 4. Verify configuration
tofu validate

# 5. Review planned changes (should show no infrastructure changes)
tofu plan

# 6. Continue using npm scripts as before
cd ..
npm run deploy
```

**Note**: The `.terraform/` directory name and `.tf` file extensions remain unchanged - OpenTofu maintains backward compatibility with these conventions.

## Configuring Push Notifications

In order for this project to work out-of-the-box, you will need to do some additional configuration in order to support push notifications. This includes generating a certificate to use the Apple Push Notification Service (APNS) and a subsequent p12 file. Instructions can be found [here](https://calvium.com/how-to-make-a-p12-file/).

Once created, you will extract the certificate and the private key in to separate files and move them in to the `secure/APNS_SANDBOX` directory at the root of the project:

```bash
# Extract the private key
openssl pkcs12 -in certificate.p12 -nodes -nocerts -legacy | sed -ne '/-BEGIN PRIVATE KEY-/,/-END PRIVATE KEY-/p' > privateKey.txt

# Extract the certificate file
openssl pkcs12 -in certificate.p12 -clcerts -nokeys -legacy  | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > certificate.txt

# Create the directories
mkdir -p secure/APNS_SANDBOX

# Move the files in to the directory
mv privateKey.txt certificate.txt secure/APNS_SANDBOX
```

Once complete, run `tofu apply` and a new platform application will be created so you can register your device to receive push notifications.

## Configuring Github Issue Creation

As an engineer, I appreciate actionable alerting. If something went wrong, I'd like to be able to know about it, have the relevant data to address the situation, and then mark it as completed. To do this, errors that are correctable will be automatically submitted as Github issues to the repository. To support this functionality, you need to generate a [Github Personal Token](https://github.com/settings/tokens?type=beta) that has access to creating issues.

Once generated, store it as `githubPersonalToken.txt` in the `secure` directory so that it isn't tracked by version control.

## Deployment

* Deploy Code - To deploy code changes only, this command will build the distribution files and trigger an OpenTofu **auto approval**.

```bash
npm run build
npm run deploy
```

### Production Testing

In order to test your endpoint in production, you can use the npm commands below.

Remotely test the listing of files

```bash
npm run test-remote-list
```

Remotely test the feedly webhook

```bash
npm run test-remote-hook
```

Remotely test the register device method for registering for push notifications on iOS

```bash
npm run test-remote-registerDevice
```

### Integration Testing with LocalStack

This project includes integration tests that run against [LocalStack](https://localstack.cloud/), a local AWS cloud emulator. Integration tests verify that AWS service interactions work correctly without mocking, providing higher confidence in production deployments.

#### Prerequisites

- **Docker**: Required to run LocalStack container (includes Compose plugin)
- **jq**: Optional, for pretty-printing health check results

```bash
brew install docker jq
```

#### Running Integration Tests

**Quick Start:**

```bash
# Full CI with integration tests (handles LocalStack lifecycle)
pnpm run ci:local:full

# Or manually manage LocalStack:
pnpm run localstack:start
pnpm run test:integration
pnpm run localstack:stop
```

**Available Commands:**

- `pnpm run ci:local` - Fast local CI checks (~2-3 min, no integration tests)
- `pnpm run ci:local:full` - Full local CI (~5-10 min, manages LocalStack lifecycle)
- `pnpm run test:integration` - Run integration tests only (~30s, for fast iteration when developing tests)
- `pnpm run localstack:start` - Start LocalStack container in detached mode
- `pnpm run localstack:stop` - Stop and remove LocalStack container
- `pnpm run localstack:logs` - Stream LocalStack logs
- `pnpm run localstack:health` - Check LocalStack service health

> **Note:** Use `ci:local:full` for comprehensive pre-push validation. Use `test:integration` with manually-started LocalStack for rapid iteration when developing integration tests.

**Test Organization:**

Integration tests are located in `test/integration/` and organized by AWS service:

```
test/integration/
‚îú‚îÄ‚îÄ s3/                   # S3 integration tests
‚îú‚îÄ‚îÄ dynamodb/             # DynamoDB integration tests
‚îú‚îÄ‚îÄ lambda/               # Lambda integration tests
‚îú‚îÄ‚îÄ sns/                  # SNS integration tests
‚îú‚îÄ‚îÄ sqs/                  # SQS integration tests
‚îú‚îÄ‚îÄ cloudwatch/           # CloudWatch integration tests
‚îî‚îÄ‚îÄ apigateway/           # API Gateway integration tests
```

**LocalStack Configuration:**

LocalStack runs on `http://localhost:4566` with ephemeral storage (fresh state each run) and provides the following AWS services:

- S3 (Simple Storage Service)
- DynamoDB (NoSQL Database)
- SNS (Simple Notification Service)
- SQS (Simple Queue Service)
- Lambda (Serverless Functions)
- CloudWatch (Monitoring)
- API Gateway (REST APIs)

**Architecture:**

Integration tests use the same vendor wrappers (`lib/vendor/AWS/*`) as production code. When `USE_LOCALSTACK=true` environment variable is set, the vendor wrappers automatically configure AWS SDK clients to connect to LocalStack instead of production AWS.

See `test/integration/README.md` for detailed integration testing documentation.

## Maintenance

### Automated yt-dlp Updates

The project uses an **OpenTofu-based binary management system** with GitHub Actions automation to keep yt-dlp up-to-date without bloating the git repository.

#### Architecture

**Version Tracking**: The `layers/yt-dlp/VERSION` file contains the current yt-dlp version (e.g., `2025.11.12`)

**OpenTofu Download**: The `null_resource.DownloadYtDlpBinary` resource in `terraform/feedly_webhook.tf`:
- Triggers whenever the VERSION file changes
- Downloads the yt-dlp binary from GitHub releases
- Verifies SHA256 checksum against official release checksums
- Tests binary execution (`--version` check)
- Makes binary executable and places it in `layers/yt-dlp/bin/`

**Git Exclusion**: The binary (`layers/yt-dlp/bin/yt-dlp_linux`) is excluded from git via `.gitignore`, preventing repository bloat

#### Automated GitHub Actions Workflow

The workflow (`.github/workflows/update-yt-dlp.yml`):
- **Runs**: Weekly on Sunday at 2am UTC
- **Trigger**: Can be manually triggered via workflow_dispatch
- **Process**:
  1. Fetches latest stable release (excludes pre-releases)
  2. Compares with current VERSION file
  3. Downloads and verifies binary for testing
  4. Tests format listing capability
  5. Updates VERSION file only (not the binary)
  6. Creates PR with release notes and deployment instructions

#### Manual Update Process

To manually check for and apply yt-dlp updates:

```bash
# Check for updates
npm run update-yt-dlp check

# Update VERSION file (if update available)
npm run update-yt-dlp update

# Review the change
git diff layers/yt-dlp/VERSION

# Test locally with OpenTofu
npm run plan  # Should show null_resource.DownloadYtDlpBinary will run

# Commit and push
git add layers/yt-dlp/VERSION
git commit -m "chore(deps): update yt-dlp to <VERSION>"
git push
```

#### Deployment Process

When a VERSION update is merged:

1. **Deploy**: Run `npm run deploy` to apply infrastructure changes
2. **Binary Download**: OpenTofu's `null_resource.DownloadYtDlpBinary` executes:
   - Downloads binary from yt-dlp GitHub releases
   - Verifies SHA256 checksum
   - Tests binary execution
3. **Layer Update**: `data.archive_file.YtDlpLayer` creates new layer zip with updated binary
4. **Lambda Update**: `aws_lambda_layer_version.YtDlp` deploys new layer version
5. **Monitoring**: Check CloudWatch logs for any download issues

#### Rollback Procedure

If a yt-dlp update causes issues in production:

```bash
# Find the previous working version
git log layers/yt-dlp/VERSION

# Revert to previous version (example: 2025.11.10)
echo "2025.11.10" > layers/yt-dlp/VERSION

# Commit and redeploy
git commit -am "chore(deps): revert yt-dlp to 2025.11.10"
git push

# Deploy with OpenTofu
npm run deploy  # Downloads and deploys the previous version
```

Alternatively, use git revert:

```bash
# Revert the problematic commit
git revert <commit-hash>
git push

# Redeploy
npm run deploy
```

#### Monitoring yt-dlp Updates

**CloudWatch Logs**: Monitor `/aws/lambda/StartFileUpload` for download failures or errors

**GitHub Issues**: The workflow automatically creates a GitHub issue if the update process fails

**Version Tracking**: The deployed version can be verified by:
```bash
# Check VERSION file
cat layers/yt-dlp/VERSION

# Check deployed Lambda layer description
aws lambda list-layer-versions --layer-name yt-dlp --region us-west-2 | jq '.LayerVersions[0]'
```

#### Benefits of This Approach

- **No repository bloat**: 35MB binary not committed to git history
- **Single source of truth**: VERSION file tracks desired version
- **Automated verification**: Checksum validation on every download
- **Easy rollback**: Change VERSION file and redeploy
- **OpenTofu integration**: Binary download is part of infrastructure deployment
- **Testing before commit**: GitHub Actions verifies binary works before creating PR

## Documentation

This project uses multiple documentation approaches:

### API Documentation with TypeSpec

The API is documented using [TypeSpec](https://typespec.io/), a language for defining APIs that generates OpenAPI specifications. To generate and view API documentation:

```bash
npm run document-api
```

This command will:
1. Automatically discover and sync API fixtures (`apiRequest-*.json` and `apiResponse-*.json`) from lambda test directories
2. Compile TypeSpec definitions to OpenAPI 3.0 specification (`docs/api/openapi.yaml`)
3. Generate a Redoc HTML documentation file (`docs/api/index.html`)
4. Automatically open the documentation in your default browser

You can also view the documentation by opening `docs/api/index.html` directly in any browser.

See `tsp/README.md` for more details about the TypeSpec definitions.

### Source Code Documentation with TSDoc

This project uses [TSDoc](https://tsdoc.org) for documenting the source code. To generate this documentation:

```bash
npm run document-source
```

The resulting output is located in `docs/source` and can open viewed by running:

```bash
open docs/source/index.html
```
</file>

<file path="src/lambdas/StartFileUpload/src/index.ts">
import type {APIGatewayProxyResult, Context} from 'aws-lambda'
import {DownloadStatus, FileDownloads} from '#entities/FileDownloads'
import {UserFiles} from '#entities/UserFiles'
import {sendMessage} from '#lib/vendor/AWS/SQS'
import {getSegment} from '#lib/vendor/AWS/XRay'
import {downloadVideoToS3, fetchVideoInfo} from '#lib/vendor/YouTube'
import type {File} from '#types/domain-models'
import {FileStatus, ResponseStatus} from '#types/enums'
import type {FetchVideoInfoResult, VideoErrorClassification} from '#types/video'
import type {YtDlpVideoInfo} from '#types/youtube'
import {getRequiredEnv} from '#util/env-validation'
import {UnexpectedError} from '#util/errors'
import {createCookieExpirationIssue, createVideoDownloadFailureIssue} from '#util/github-helpers'
import {buildApiResponse, putMetric, putMetrics, withPowertools, wrapLambdaInvokeHandler} from '#util/lambda-helpers'
import {logDebug, logInfo} from '#util/logging'
import {createMetadataNotification} from '#util/transformers'
import {classifyVideoError, isRetryExhausted} from '#util/video-error-classifier'
import {upsertFile} from './file-helpers'
interface StartFileUploadParams {
  fileId: string
  /** Correlation ID for end-to-end request tracing */
  correlationId?: string
}
‚ãÆ----
/** Correlation ID for end-to-end request tracing */
‚ãÆ----
/**
 * Fetch video info with X-Ray tracing.
 * Wraps fetchVideoInfo and handles subsegment lifecycle.
 */
async function fetchVideoInfoTraced(fileUrl: string, fileId: string): Promise<FetchVideoInfoResult>
/**
 * Download video to S3 with X-Ray tracing.
 * Wraps downloadVideoToS3 and handles subsegment lifecycle including error capture.
 */
async function downloadVideoToS3Traced(fileUrl: string, bucket: string, fileName: string): Promise<
/**
 * Update FileDownload entity with current download state.
 * This is the transient state that tracks retry attempts, errors, and scheduling.
 * TTL is automatically set for completed/failed statuses.
 */
async function updateDownloadState(fileId: string, status: DownloadStatus, classification?: VideoErrorClassification, retryCount = 0): Promise<void>
‚ãÆ----
// Set TTL for completed/failed downloads (auto-cleanup after 7 days)
‚ãÆ----
// Try to update existing record first
‚ãÆ----
// If record doesn't exist, create it
‚ãÆ----
/**
 * Dispatch MetadataNotification to all users waiting for this file.
 * Sends notifications via SQS to the push notification queue.
 * @param fileId - The video ID
 * @param videoInfo - Video metadata from yt-dlp
 */
async function dispatchMetadataNotifications(fileId: string, videoInfo: YtDlpVideoInfo): Promise<void>
‚ãÆ----
// Get all users waiting for this file
‚ãÆ----
// Send MetadataNotification to each user
// Use Promise.allSettled so one SQS error doesn't stop other user notifications
‚ãÆ----
/**
 * Handle download failure: classify error, update state, and determine next action.
 * Returns appropriate response based on whether download should be scheduled for retry.
 */
async function handleDownloadFailure(
  fileId: string,
  fileUrl: string,
  error: Error,
  videoInfoResult: FetchVideoInfoResult,
  existingRetryCount: number,
  existingMaxRetries: number,
  context: Context
): Promise<
‚ãÆ----
// Classify the error to determine retry strategy
‚ãÆ----
// Handle retryable errors with scheduled retry
‚ãÆ----
// Return success - scheduling a retry is expected behavior, not a failure
‚ãÆ----
// Handle permanent failures or retry exhaustion
‚ãÆ----
// Also update File entity to reflect permanent failure
‚ãÆ----
// Create GitHub issues for actionable failures
‚ãÆ----
/**
 * Downloads a YouTube video and uploads it to S3.
 *
 * Architecture:
 * - FileDownloads entity: Tracks transient download state (retries, scheduling, errors)
 * - Files entity: Stores permanent media metadata (only updated on success)
 *
 * Flow:
 * 1. Mark download as in_progress
 * 2. Fetch video info (safe - never throws)
 * 3. If fetch failed ‚Üí classify ‚Üí schedule retry or mark failed
 * 4. If fetch succeeded ‚Üí stream to S3
 * 5. If stream failed ‚Üí classify ‚Üí schedule retry or mark failed
 * 6. If stream succeeded ‚Üí update Files entity with metadata
 *
 * @param event - Contains the fileId to download
 * @param context - AWS Lambda context
 * @notExported
 */
‚ãÆ----
// Get existing download state for retry counting
‚ãÆ----
// Mark download as in_progress
‚ãÆ----
// Step 1: Fetch video info (safe - never throws)
‚ãÆ----
// Dispatch MetadataNotification to all users waiting for this file
‚ãÆ----
// Step 2: Prepare for download
// Always use .mp4 extension - yt-dlp will merge to mp4 container
‚ãÆ----
// Step 3: Download video to S3 (two-phase: temp file -> S3 stream)
// yt-dlp handles format selection internally (best video + best audio, merged)
‚ãÆ----
// Step 4: Update permanent File entity with metadata (only on success)
‚ãÆ----
// Step 5: Mark download as completed
</file>

<file path="src/lambdas/RegisterDevice/src/index.ts">
import {Devices} from '#entities/Devices'
import {UserDevices} from '#entities/UserDevices'
import {createPlatformEndpoint, listSubscriptionsByTopic} from '#lib/vendor/AWS/SNS'
import type {Device} from '#types/domain-models'
import {UserStatus} from '#types/enums'
import {getPayloadFromEvent, validateRequest} from '#util/apigateway-helpers'
import {registerDeviceSchema} from '#util/constraints'
import {getUserDevices, subscribeEndpointToTopic, unsubscribeEndpointToTopic} from '#util/device-helpers'
import {getRequiredEnv} from '#util/env-validation'
import {providerFailureErrorMessage, UnexpectedError} from '#util/errors'
import {buildApiResponse, verifyPlatformConfiguration, withPowertools, wrapOptionalAuthHandler} from '#util/lambda-helpers'
import {logDebug} from '#util/logging'
interface DeviceRegistrationRequest {
  name: string
  token: string
  systemVersion: string
  deviceId: string
  systemName: string
}
/**
 * An idempotent operation that creates an endpoint for a device on one of the supported services (e.g. GCP, APNS)
 * @param token - The client device token
 * @notExported
 */
async function createPlatformEndpointFromToken(token: string)
‚ãÆ----
// An idempotent option that creates an endpoint for a device on one of the supported services (e.g. GCP, APNS)
‚ãÆ----
/**
 * Store the device details associated with the user by creating a UserDevice record
 * Creates individual record for the user-device relationship
 * @param userId - The userId
 * @param deviceId - The UUID of the device (either iOS or Android)
 * @notExported
 */
async function upsertUserDevices(userId: string, deviceId: string)
/**
 * Store the device details independent of the user (e.g. iPhone, Android) and stores it to DynamoDB
 * @param device - The Device details (e.g. endpointArn)
 * @notExported
 */
async function upsertDevice(device: Device)
/**
 * Store the device details associated with the user (e.g. iPhone, Android) and stores it to DynamoDB
 * @param endpointArn - The userId
 * @param topicArn - The Device details (e.g. endpointArn)
 * @notExported
 */
async function getSubscriptionArnFromEndpointAndTopic(endpointArn: string, topicArn: string): Promise<string>
/**
 * Registers a Device (e.g. iPhone) to receive push notifications via AWS SNS
 * Unauthenticated users (invalid token) are rejected with 401 by wrapOptionalAuthHandler
 * @notExported
 */
‚ãÆ----
// wrapOptionalAuthHandler already rejected Unauthenticated users with 401
‚ãÆ----
// Store the device details, regardless of user status
‚ãÆ----
/* c8 ignore else */
‚ãÆ----
// Extract the userId and associate them
// Store the device details associated with the user
‚ãÆ----
// Determine if the user already exists
‚ãÆ----
// Confirm the subscription, and unsubscribe
‚ãÆ----
// If the user hasn't registered; add them to the unregistered topic
</file>

<file path="src/util/lambda-helpers.ts">
import type {
  APIGatewayProxyEventHeaders,
  APIGatewayProxyResult,
  APIGatewayRequestAuthorizerEvent,
  Context,
  CustomAuthorizerResult,
  S3Event,
  S3EventRecord,
  ScheduledEvent,
  SQSEvent,
  SQSRecord
} from 'aws-lambda'
import {getStandardUnit, putMetricData} from '#lib/vendor/AWS/CloudWatch'
import {CustomLambdaError, ServiceUnavailableError, UnauthorizedError} from './errors'
import type {CustomAPIGatewayRequestAuthorizerEvent} from '#types/infrastructure-types'
import type {MetricInput, UserEventDetails} from '#types/util'
import {UserStatus} from '#types/enums'
import {getOptionalEnv} from './env-validation'
import {logDebug, logError, logInfo} from './logging'
// Re-export logging functions for backwards compatibility
‚ãÆ----
/**
 * Extracts a human-readable message from an unknown error value.
 * Used as fallback when error is not an Error instance.
 */
function getErrorMessage(error: unknown): string
/** @deprecated Use getErrorMessage instead. Kept for backwards compatibility. */
export function formatUnknownError(unknownVariable: unknown): string
/**
 * Internal function to format API Gateway responses.
 * Automatically detects error vs success based on status code.
 */
function formatResponse(context: Context, statusCode: number, body?: string | object, headers?: APIGatewayProxyEventHeaders): APIGatewayProxyResult
‚ãÆ----
// Note: 3xx responses are treated as success (not wrapped in error format)
‚ãÆ----
/**
 * Build an API Gateway response from either a status code + body or an Error object.
 *
 * @example
 * // Success response
 * return buildApiResponse(context, 200, \{data: files\})
 *
 * // Error response with status code
 * return buildApiResponse(context, 404, 'File not found')
 *
 * // Error response from Error object (extracts status and message)
 * return buildApiResponse(context, new ValidationError('Invalid input'))
 */
export function buildApiResponse(context: Context, statusCodeOrError: number | Error, body?: string | object): APIGatewayProxyResult
‚ãÆ----
// If first arg is Error, extract status and message
‚ãÆ----
// Otherwise use status code directly
‚ãÆ----
/*#__PURE__*/
export function verifyPlatformConfiguration(): void
/** @deprecated Use buildApiResponse(context, error) instead. Kept for backwards compatibility. */
export function lambdaErrorResponse(context: Context, error: unknown): APIGatewayProxyResult
‚ãÆ----
// Delegate to buildApiResponse which now handles Error objects directly
‚ãÆ----
// For non-Error types, convert to message and return 500
‚ãÆ----
export function getUserDetailsFromEvent(event: CustomAPIGatewayRequestAuthorizerEvent): UserEventDetails
‚ãÆ----
// This should always be present, via the API Gateway
/* c8 ignore else */
‚ãÆ----
/**
 * Publish a custom CloudWatch metric
 * @param metricName - Name of the metric
 * @param value - Numeric value
 * @param unit - Unit of measurement (Seconds, Bytes, Count, etc.)
 * @param dimensions - Optional dimensions for filtering/grouping
 */
export async function putMetric(metricName: string, value: number, unit?: string, dimensions:
‚ãÆ----
// Don't fail Lambda execution if metrics fail
‚ãÆ----
/**
 * Publish multiple metrics in a single API call for efficiency
 * @param metrics - Array of metrics to publish
 */
export async function putMetrics(metrics: MetricInput[]): Promise<void>
‚ãÆ----
// Don't fail Lambda execution if metrics fail
‚ãÆ----
/**
 * Sanitize data for test fixtures by removing sensitive fields
 * Recursively processes objects and arrays to redact PII and credentials
 * @param data - Data to sanitize
 * @returns Sanitized copy of data with sensitive fields redacted
 */
function sanitizeForTest(data: unknown): unknown
‚ãÆ----
// Remove sensitive fields - case-insensitive patterns for comprehensive PII protection
‚ãÆ----
/^authorization$/i, // fmt: multiline
‚ãÆ----
/**
 * Log incoming request for fixture extraction from CloudWatch
 * Marks production requests for automated fixture generation
 *
 * Automatically detects the Lambda function name from AWS_LAMBDA_FUNCTION_NAME
 * environment variable (set by AWS Lambda runtime).
 *
 * @param event - Lambda event (API Gateway request)
 * @param fixtureType - Optional type identifier (auto-detected from Lambda name if not provided)
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/Fixture-Extraction#fixture-logging-implementation | Fixture Logging Implementation}
 */
export function logIncomingFixture(event: unknown, fixtureType?: string): void
/**
 * Log outgoing response for fixture extraction from CloudWatch
 * Marks production responses for automated fixture generation
 *
 * Automatically detects the Lambda function name from AWS_LAMBDA_FUNCTION_NAME
 * environment variable (set by AWS Lambda runtime).
 *
 * @param response - Lambda response
 * @param fixtureType - Optional type identifier (auto-detected from Lambda name if not provided)
 *
 * @see {@link https://github.com/j0nathan-ll0yd/aws-cloudformation-media-downloader/wiki/Fixture-Extraction#fixture-logging-implementation | Fixture Logging Implementation}
 */
export function logOutgoingFixture(response: unknown, fixtureType?: string): void
// ============================================================================
// Lambda Handler Wrappers
// ============================================================================
import type {
  ApiHandlerParams,
  AuthenticatedApiParams,
  AuthorizerParams,
  EventHandlerParams,
  LambdaInvokeHandlerParams,
  OptionalAuthApiParams,
  ScheduledHandlerParams,
  WrapperMetadata
} from '#types/lambda-wrappers'
/**
 * Wraps an API Gateway handler with automatic error handling and fixture logging.
 * Eliminates try-catch boilerplate and ensures consistent error responses.
 *
 * @param handler - Business logic that returns APIGatewayProxyResult or throws
 * @returns Wrapped handler with error handling and fixture logging
 *
 * @example
 * ```typescript
 * export const handler = withXRay(wrapApiHandler(async ({event, context}) => {
 *   // Business logic - just throw on error
 *   if (!valid) throw new UnauthorizedError('Invalid')
 *   return response(context, 200, data)
 * }))
 * ```
 */
export function wrapApiHandler<TEvent = CustomAPIGatewayRequestAuthorizerEvent>(
  handler: (params: ApiHandlerParams<TEvent>) => Promise<APIGatewayProxyResult>
): (event: TEvent, context: Context, metadata?: WrapperMetadata) => Promise<APIGatewayProxyResult>
/**
 * Wraps an API Gateway handler that REQUIRES authentication.
 * Rejects both Unauthenticated AND Anonymous users with 401.
 * Guarantees userId is available (non-optional string) in the handler.
 *
 * @param handler - Business logic with guaranteed userId
 * @returns Wrapped handler with authentication enforcement
 *
 * @example
 * ```typescript
 * export const handler = withXRay(wrapAuthenticatedHandler(
 *   async ({event, context, userId}) => {
 *     // userId is guaranteed to be a string - no null checks needed
 *     const files = await getFilesByUser(userId)
 *     return response(context, 200, files)
 *   }
 * ))
 * ```
 */
export function wrapAuthenticatedHandler<TEvent = CustomAPIGatewayRequestAuthorizerEvent>(
  handler: (params: AuthenticatedApiParams<TEvent>) => Promise<APIGatewayProxyResult>
): (event: TEvent, context: Context, metadata?: WrapperMetadata) => Promise<APIGatewayProxyResult>
‚ãÆ----
// Reject Unauthenticated (invalid token)
‚ãÆ----
// Reject Anonymous (no token at all)
‚ãÆ----
// At this point, userStatus is Authenticated, so userId is guaranteed
‚ãÆ----
/**
 * Wraps an API Gateway handler that allows Anonymous OR Authenticated users.
 * Rejects only Unauthenticated users (invalid token) with 401.
 * Provides userId and userStatus for handler to differentiate behavior.
 *
 * @param handler - Business logic with userId and userStatus
 * @returns Wrapped handler with optional authentication support
 *
 * @example
 * ```typescript
 * export const handler = withPowertools(wrapOptionalAuthHandler(
 *   async ({context, userId, userStatus}) => {
 *     if (userStatus === UserStatus.Anonymous) {
 *       return buildApiResponse(context, 200, [getDefaultFile()])
 *     }
 *     // userId is available for authenticated users
 *     const files = await getFilesByUser(userId as string)
 *     return buildApiResponse(context, 200, files)
 *   }
 * ))
 * ```
 */
export function wrapOptionalAuthHandler<TEvent = CustomAPIGatewayRequestAuthorizerEvent>(
  handler: (params: OptionalAuthApiParams<TEvent>) => Promise<APIGatewayProxyResult>
): (event: TEvent, context: Context, metadata?: WrapperMetadata) => Promise<APIGatewayProxyResult>
‚ãÆ----
// Reject only Unauthenticated (invalid token)
‚ãÆ----
// Allow Anonymous and Authenticated through
‚ãÆ----
/**
 * Wraps an API Gateway custom authorizer with proper error propagation.
 * Lets `Error('Unauthorized')` propagate (‚Üí401), logs unexpected errors.
 *
 * @param handler - Authorizer business logic
 * @returns Wrapped authorizer handler
 *
 * @example
 * ```typescript
 * export const handler = withXRay(wrapAuthorizer(async ({event}) => {
 *   if (!valid) throw new Error('Unauthorized')  // ‚Üí 401
 *   return generateAllow(userId, event.methodArn)
 * }))
 * ```
 */
export function wrapAuthorizer(
  handler: (params: AuthorizerParams) => Promise<CustomAuthorizerResult>
): (event: APIGatewayRequestAuthorizerEvent, context: Context, metadata?: WrapperMetadata) => Promise<CustomAuthorizerResult>
‚ãÆ----
// Let 'Unauthorized' errors propagate (API Gateway returns 401)
‚ãÆ----
// Log unexpected errors and rethrow
‚ãÆ----
/**
 * Wraps an S3/SQS event handler with per-record error handling.
 * Processes all records even if some fail, logs errors per record.
 *
 * @param handler - Handler for individual records
 * @param options - Configuration with getRecords extractor function
 * @returns Wrapped handler that processes all records
 *
 * @example
 * ```typescript
 * export const handler = withXRay(wrapEventHandler(
 *   async ({record}) => {
 *     // Process single S3 record
 *     await processFile(record.s3.object.key)
 *   },
 *   {getRecords: s3Records}
 * ))
 * ```
 */
export function wrapEventHandler<TEvent, TRecord>(
  handler: (params: EventHandlerParams<TRecord>) => Promise<void>,
  options: {getRecords: (event: TEvent) => TRecord[]}
): (event: TEvent, context: Context, metadata?: WrapperMetadata) => Promise<void>
/**
 * Wraps a CloudWatch scheduled event handler with logging.
 * Logs event and result, rethrows errors for CloudWatch visibility.
 *
 * @param handler - Scheduled event business logic
 * @returns Wrapped handler with logging
 *
 * @example
 * ```typescript
 * export const handler = withXRay(wrapScheduledHandler(async () => {
 *   // Scheduled task logic
 *   await pruneOldRecords()
 * }))
 * ```
 */
export function wrapScheduledHandler<TResult = void>(
  handler: (params: ScheduledHandlerParams) => Promise<TResult>
): (event: ScheduledEvent, context: Context, metadata?: WrapperMetadata) => Promise<TResult>
/**
 * Wraps a Lambda-to-Lambda invoke handler with logging and error handling.
 * Used for handlers invoked asynchronously by other Lambdas (e.g., StartFileUpload).
 * Provides consistent logging, fixture extraction, and error propagation.
 *
 * @param handler - Lambda invoke handler business logic
 * @returns Wrapped handler with logging and error handling
 *
 * @example
 * ```typescript
 * export const handler = withPowertools(wrapLambdaInvokeHandler(
 *   async ({event, context}) => {
 *     // Process the Lambda invocation event
 *     await processFile(event.fileId)
 *     return buildApiResponse(context, 200, {status: 'success'})
 *   }
 * ))
 * ```
 */
export function wrapLambdaInvokeHandler<TEvent, TResult>(
  handler: (params: LambdaInvokeHandlerParams<TEvent>) => Promise<TResult>
): (event: TEvent, context: Context, metadata?: WrapperMetadata) => Promise<TResult>
/**
 * Convenience extractor for S3 event records
 */
export const s3Records = (event: S3Event): S3EventRecord[]
/**
 * Convenience extractor for SQS event records
 */
export const sqsRecords = (event: SQSEvent): SQSRecord[]
// ============================================================================
// Powertools Middleware Wrapper
// ============================================================================
import middy from '@middy/core'
import {captureLambdaHandler, injectLambdaContext, logger, logMetrics, metrics, tracer} from '#lib/vendor/Powertools'
/**
 * Wraps a Lambda handler with AWS Powertools middleware stack.
 * Provides enhanced observability with structured logging, tracing, and metrics.
 *
 * Features:
 * - Structured JSON logging with automatic context enrichment
 * - X-Ray tracing with enhanced annotations
 * - Automatic cold start metric tracking
 * - Correlation IDs through all logs
 *
 * Use this as a replacement for `withXRay()` for enhanced observability.
 *
 * @param handler - Lambda handler function
 * @returns Wrapped handler with Powertools middleware
 *
 * @example
 * ```typescript
 * // Replace withXRay with withPowertools for enhanced observability
 * export const handler = withPowertools(wrapAuthenticatedHandler(
 *   async ({event, context, userId}) => {
 *     const files = await getFilesByUser(userId)
 *     return response(context, 200, files)
 *   }
 * ))
 * ```
 */
export function withPowertools<TEvent, TResult>(
  handler: (event: TEvent, context: Context) => Promise<TResult>
): (event: TEvent, context: Context) => Promise<TResult>
// Re-export Powertools utilities for direct access
</file>

<file path="src/lambdas/ListFiles/src/index.ts">
import {Files} from '#entities/Files'
import {UserFiles} from '#entities/UserFiles'
import {buildApiResponse, withPowertools, wrapOptionalAuthHandler} from '#util/lambda-helpers'
import {logDebug, logError} from '#util/logging'
import type {File} from '#types/domain-models'
import {FileStatus, UserStatus} from '#types/enums'
import {getDefaultFile} from '#util/constants'
import {retryUnprocessed} from '#util/retry'
/**
 * Returns an array of Files for a user using ElectroDB batch get
 * Eliminates N+1 query pattern by using batch operations
 * @param userId - The User ID
 * @notExported
 */
async function getFilesByUser(userId: string): Promise<File[]>
/**
 * Returns a list of files available to the user.
 *
 * - In an authenticated state, returns the files the user has available
 * - In an anonymous state, returns a single demo file (for training purposes)
 * - Unauthenticated users (invalid token) are rejected with 401 by wrapOptionalAuthHandler
 *
 * @notExported
 */
‚ãÆ----
// wrapOptionalAuthHandler already rejected Unauthenticated users with 401
</file>

<file path="src/lambdas/WebhookFeedly/src/index.ts">
import {randomUUID} from 'node:crypto'
import {Files} from '#entities/Files'
import {DownloadStatus, FileDownloads} from '#entities/FileDownloads'
import {sendMessage} from '#lib/vendor/AWS/SQS'
import type {SendMessageRequest} from '#lib/vendor/AWS/SQS'
import {getVideoID} from '#lib/vendor/YouTube'
import type {File} from '#types/domain-models'
import type {Webhook} from '#types/vendor/IFTTT/Feedly/Webhook'
import {getPayloadFromEvent, validateRequest} from '#util/apigateway-helpers'
import {feedlyEventSchema} from '#util/constraints'
import {buildApiResponse, withPowertools, wrapAuthenticatedHandler} from '#util/lambda-helpers'
import {logDebug, logError, logInfo} from '#util/logging'
import {createDownloadReadyNotification} from '#util/transformers'
import {FileStatus, ResponseStatus} from '#types/enums'
import {initiateFileDownload} from '#util/lambda-invoke-helpers'
import {associateFileToUser} from '#util/user-file-helpers'
import {getRequiredEnv} from '#util/env-validation'
import {createPersistenceStore, defaultIdempotencyConfig, makeIdempotent} from '#lib/vendor/Powertools/idempotency'
/**
 * Adds a base File record to DynamoDB with placeholder metadata.
 * Also creates a FileDownloads record to track the download orchestration.
 *
 * Files = permanent metadata (populated when download succeeds)
 * FileDownloads = transient orchestration state (retries, scheduling)
 *
 * @param fileId - The unique file identifier (YouTube video ID)
 * @param sourceUrl - The original YouTube URL for the video
 * @param correlationId - Correlation ID for end-to-end request tracing
 */
async function addFile(fileId: string, sourceUrl?: string, correlationId?: string)
‚ãÆ----
// Create placeholder Files record (will be updated with real metadata on successful download)
‚ãÆ----
key: fileId, // Will be updated to include extension
‚ãÆ----
// Create FileDownloads record to track download orchestration
‚ãÆ----
/**
 * Retrieves a File from DynamoDB (if it exists)
 * @param fileId - The unique file identifier
 * @notExported
 */
async function getFile(fileId: string): Promise<File | undefined>
/**
 * Sends a DownloadReadyNotification to the user
 * @param file - A DynamoDB File object
 * @param userId - The UUID of the user
 * @notExported
 */
async function sendFileNotification(file: File, userId: string)
interface WebhookProcessingInput {
  fileId: string
  userId: string
  articleURL: string
  backgroundMode?: boolean
  correlationId: string
}
interface WebhookProcessingResult {
  statusCode: number
  status: ResponseStatus
}
/**
 * Core webhook processing logic - wrapped with idempotency
 * This function handles the actual file processing and returns the result
 * Idempotency ensures duplicate webhook calls return the same response
 */
async function processWebhookRequest(input: WebhookProcessingInput): Promise<WebhookProcessingResult>
‚ãÆ----
// Parallelize independent operations for ~60% latency reduction
// Use Promise.allSettled to handle partial failures gracefully
‚ãÆ----
// File already downloaded - send notification to user
‚ãÆ----
// New file - create Files and FileDownloads records with correlationId
‚ãÆ----
// Foreground mode - initiate download immediately with correlationId
‚ãÆ----
// Background mode - FileCoordinator will pick it up
‚ãÆ----
// Idempotent wrapper for webhook processing
// Lazy initialization to ensure environment variables are available
‚ãÆ----
function getIdempotentProcessor()
/**
 * Receives a webhook to download a file from Feedly.
 *
 * - If the file already exists: it is associated with the requesting user and a push notification is dispatched.
 * - If the file doesn't exist: it is associated with the requesting user and queued for download.
 *
 * Uses Powertools Idempotency to prevent duplicate processing of the same webhook request.
 *
 * @notExported
 */
‚ãÆ----
// Generate correlation ID for end-to-end request tracing
‚ãÆ----
// Process webhook with idempotency protection
</file>

<file path="package.json">
{
  "name": "aws-cloudformation-media-downloader",
  "version": "1.0.0",
  "engines": {
    "node": ">=22.0.0",
    "pnpm": ">=10.0.0"
  },
  "packageManager": "pnpm@10.0.0",
  "type": "module",
  "scripts": {
    "preinstall": "npx only-allow pnpm",
    "precheck": "pnpm run check-types && pnpm run lint",
    "check-types": "tsc --project ./tsconfig.json",
    "generate-graph": "node --experimental-strip-types scripts/generateDependencyGraph.ts",
    "document-source": "./bin/document-source.sh",
    "document-terraform": "terraform-docs markdown terraform/ > docs/terraform.md",
    "document-api": "./bin/document-api.sh",
    "build": "pnpm run generate-graph && node --disable-warning=DEP0180 --import tsx config/esbuild.config.ts",
    "analyze": "ANALYZE=true pnpm run build && node --disable-warning=DEP0180 --import tsx scripts/visualize-bundles.ts && open build/reports/bundle-analysis.html",
    "build-dependencies": "./bin/build-dependencies.sh",
    "install-prod": "pnpm install --prod",
    "format": "dprint fmt",
    "format:check": "dprint check",
    "format:bash": "shfmt -i 2 -ci -sr -w bin/*.sh",
    "format:bash:check": "shfmt -i 2 -ci -sr -d bin/*.sh",
    "lint": "eslint -c ./eslint.config.mjs .",
    "lint-fix": "eslint -c ./eslint.config.mjs . --fix",
    "deps:check": "dependency-cruiser src --config .dependency-cruiser.cjs",
    "deps:graph": "dependency-cruiser src --config .dependency-cruiser.cjs --output-type dot | dot -T svg > dependency-graph.svg",
    "deps:report": "dependency-cruiser src --config .dependency-cruiser.cjs --output-type html --output-to dependency-report.html",
    "test": "pnpm run generate-graph && node --no-warnings --experimental-vm-modules node_modules/jest/bin/jest.js --silent --config config/jest.config.mjs",
    "test:eslint-rules": "node eslint-local-rules/test/no-direct-aws-sdk-import.test.cjs && node eslint-local-rules/test/cascade-delete-order.test.cjs && node eslint-local-rules/test/use-electrodb-mock-helper.test.cjs && node eslint-local-rules/test/response-helpers.test.cjs && node eslint-local-rules/test/env-validation.test.cjs",
    "test:integration": "pnpm run generate-graph && USE_LOCALSTACK=true node --no-warnings --experimental-vm-modules node_modules/jest/bin/jest.js --silent --config config/jest.integration.config.mjs",
    "ci:local": "./bin/ci-local.sh",
    "ci:local:full": "./bin/ci-local-full.sh",
    "validate:docs": "./bin/validate-docs.sh",
    "validate:doc-sync": "./bin/validate-doc-sync.sh",
    "validate:graphrag": "./bin/validate-graphrag.sh",
    "validate:config": "node --disable-warning=DEP0180 --import tsx scripts/validateConfig.ts",
    "typespec:compile": "tsp compile tsp/",
    "typespec:check": "tsp compile tsp/ --no-emit",
    "lint:workflows": "actionlint",
    "test-remote-list": "./bin/test-list.sh",
    "test-remote-hook": "./bin/test-hook.sh",
    "test-remote-registerDevice": "./bin/test-registerDevice.sh",
    "localstack:start": "docker compose -f docker-compose.localstack.yml up -d",
    "localstack:stop": "docker compose -f docker-compose.localstack.yml down",
    "localstack:logs": "docker compose -f docker-compose.localstack.yml logs -f localstack",
    "localstack:health": "curl -s http://localhost:4566/_localstack/health | jq",
    "update-cookies": "./bin/update-youtube-cookies.sh",
    "update-yt-dlp": "./bin/update-yt-dlp.sh",
    "extract-fixtures": "./bin/extract-fixtures.sh",
    "extract-fixtures:production": "./bin/extract-production-fixtures.sh",
    "process-fixtures": "node bin/process-fixtures.js",
    "mcp:server": "node --disable-warning=DEP0180 --import tsx src/mcp/server.ts",
    "mcp:inspect": "pnpm exec mcp-inspector src/mcp/server.ts",
    "graphrag:extract": "node --disable-warning=DEP0180 --import tsx graphrag/extract.ts",
    "graphrag:query": "pnpm dlx tsx graphrag/query.ts",
    "update:agents-prs": "./bin/update-agents-prs.sh",
    "plan": "eval export $(cat .env) && cd terraform && tofu plan",
    "deploy": "eval export $(cat .env) && cd terraform && tofu apply -auto-approve",
    "pack:context": "repomix",
    "pack:light": "repomix --include \"src/**/*.ts,docs/**/*.md,AGENTS.md\" --ignore \"**/*.test.ts,**/*.spec.ts\"",
    "prepare": "husky"
  },
  "author": "Jonathan Lloyd",
  "dependencies": {
    "@aws-lambda-powertools/idempotency": "^2.30.0",
    "@aws-lambda-powertools/logger": "^2.30.0",
    "@aws-lambda-powertools/metrics": "^2.30.0",
    "@aws-lambda-powertools/parser": "^2.30.0",
    "@aws-lambda-powertools/tracer": "^2.30.0",
    "@aws-sdk/client-api-gateway": "3.953.0",
    "@aws-sdk/client-cloudwatch": "3.953.0",
    "@aws-sdk/client-dynamodb": "3.953.0",
    "@aws-sdk/client-lambda": "3.953.0",
    "@aws-sdk/client-s3": "3.953.0",
    "@aws-sdk/client-sns": "3.953.0",
    "@aws-sdk/client-sqs": "3.953.0",
    "@aws-sdk/lib-dynamodb": "3.953.0",
    "@aws-sdk/lib-storage": "3.953.0",
    "@aws-sdk/util-dynamodb": "3.953.0",
    "@middy/core": "^6.4.5",
    "@modelcontextprotocol/sdk": "^1.25.1",
    "@octokit/rest": "^22.0.1",
    "apns2": "12.2.0",
    "aws-xray-sdk-core": "3.12.0",
    "axios": "^1.13.2",
    "better-auth": "1.4.7",
    "electrodb": "3.5.0",
    "jose": "^6.1.3",
    "uuid": "^13.0.0",
    "yt-dlp-wrap": "^2.3.12",
    "ytdl-core": "^4.11.5",
    "zod": "^4.2.1"
  },
  "sideEffects": false,
  "imports": {
    "#entities/*": "./src/entities/*",
    "#lib/*": "./src/lib/*",
    "#util/*": "./src/util/*",
    "#types/*": "./src/types/*",
    "#test/*": "./test/*"
  },
  "devDependencies": {
    "@eslint/eslintrc": "^3.3.3",
    "@eslint/js": "^9.39.2",
    "@jest/globals": "^30.2.0",
    "@modelcontextprotocol/inspector": "^0.18.0",
    "@redocly/cli": "^2.12.7",
    "@types/aws-lambda": "^8.10.159",
    "@types/debug": "^4.1.12",
    "@types/eslint__js": "^9.14.0",
    "@types/node": "^25.0.3",
    "@types/uuid": "^11.0.0",
    "@typescript-eslint/eslint-plugin": "^8.50.0",
    "@typescript-eslint/parser": "^8.50.0",
    "@typespec/compiler": "^1.7.1",
    "@typespec/http": "^1.7.0",
    "@typespec/json-schema": "^1.7.0",
    "@typespec/openapi": "^1.7.0",
    "@typespec/openapi3": "^1.7.0",
    "dependency-cruiser": "^17.3.4",
    "dprint": "^0.50.2",
    "esbuild": "^0.27.2",
    "esbuild-visualizer": "^0.7.0",
    "eslint": "^9.39.2",
    "eslint-plugin-tsdoc": "^0.5.0",
    "glob": "^13.0.0",
    "husky": "^9.1.7",
    "jest": "^30.2.0",
    "quicktype": "^23.2.6",
    "redoc": "^2.5.2",
    "repomix": "^1.10.2",
    "source-map-support": "^0.5.21",
    "ts-jest": "^29.4.6",
    "ts-morph": "^27.0.2",
    "ts-node": "^10.9.2",
    "tsx": "^4.21.0",
    "typedoc": "^0.28.15",
    "typescript": "^5.9.3"
  }
}
</file>

</files>
